{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\newcommand{\\calf}{{\\cal F}}\n",
    "\\newcommand{\\dnu}{d \\nu}\n",
    "\\newcommand{\\mf}{{\\bf F}}\n",
    "\\newcommand{\\md}{{\\bf D}}\n",
    "\\newcommand{\\mP}{{\\bf P}}\n",
    "\\newcommand{\\mU}{{\\bf U}}\n",
    "\\newcommand{\\vu}{{\\bf u}}\n",
    "\\newcommand{\\vx}{{\\bf x}}\n",
    "\\newcommand{\\vw}{{\\bf w}}\n",
    "\\newcommand{\\vy}{{\\bf y}}\n",
    "\\newcommand{\\vf}{{\\bf f}}\n",
    "\\newcommand{\\vs}{{\\bf s}}\n",
    "\\newcommand{\\ve}{{\\bf e}}\n",
    "\\newcommand{\\vd}{{\\bf d}}\n",
    "\\newcommand{\\vb}{{\\bf b}}\n",
    "\\newcommand{\\vz}{{\\bf z}}\n",
    "\\newcommand{\\mg}{{\\bf G}}\n",
    "\\newcommand{\\ml}{{\\bf L}}\n",
    "\\newcommand{\\mg}{{\\bf G}}\n",
    "\\newcommand{\\mv}{{\\bf V}}\n",
    "\\newcommand{\\ma}{{\\bf A}}\n",
    "\\newcommand{\\mi}{{\\bf I}}\n",
    "\\newcommand{\\mm}{{\\bf M}}\n",
    "\\newcommand{\\mb}{{\\bf B}}\n",
    "\\newcommand{\\ball}{{\\cal B}}\n",
    "\\newcommand{\\ptc}{{\\Psi TC}}\n",
    "\\newcommand{\\diag}{\\mbox{diag}}\n",
    "\\newcommand{\\begeq}{{\\begin{equation}}}\n",
    "\\newcommand{\\endeq}{{\\end{equation}}}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"fanote_init.jl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3.7 Solvers for Chapter 3\n",
    "\n",
    "Contents for Section 3.7\n",
    "\n",
    "[Overview](#Overview)\n",
    "\n",
    "[nsoli.jl](#nsoli.jl)\n",
    "\n",
    "- [Benchmarking the H-equation with nsoli.jl](#Benchmarking-the-H-equation-with-nsoli.jl)\n",
    "\n",
    "- [ Preconditioning the Convection-Diffusion Equation](#Preconditioning-the-Convection-Diffusion-Equation)\n",
    "\n",
    "[ptcsoli.jl](#ptcsoli.jl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will follow the pattern of the previous chapters and present two solvers, a Newton code and a $\\ptc$ code. Both codes are for systems of equations and use Krylov methods to compute the step. We have two Krylov solvers, GMRES and BiCGstab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 3.7.1: nsoli.jl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__nsoli.jl__ solves systems of nonlinear equations with Newton-Krylov methods. As usual, we begin with the docstrings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search: \u001b[0m\u001b[1mn\u001b[22m\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1ml\u001b[22m\u001b[0m\u001b[1mi\u001b[22m \u001b[0m\u001b[1mN\u001b[22m\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1ml\u001b[22m\u001b[0m\u001b[1mi\u001b[22mPDE \u001b[0m\u001b[1mn\u001b[22m\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1ml\u001b[22m \u001b[0m\u001b[1mn\u001b[22m\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1ml\u001b[22msc \u001b[0m\u001b[1mn\u001b[22m\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1ml\u001b[22mheq \u001b[0m\u001b[1mN\u001b[22m\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1ml\u001b[22mPDE\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "nsoli(F!, x0, FS, FPS, Jvec=dirder; rtol=1.e-6, atol=1.e-12,\n",
       "           maxit=20, lmaxit=-1, lsolver=\"gmres\", eta=.1,\n",
       "           fixedeta=true, Pvec=nothing, pside=\"right\",\n",
       "           armmax=10, dx = 1.e-7, armfix=false, pdata = nothing,\n",
       "           printerr = true, keepsolhist = false, stagnationok=false)\n",
       "\\end{verbatim}\n",
       ")\n",
       "\n",
       "C. T. Kelley, 2021\n",
       "\n",
       "Julia versions of the nonlinear solvers from my SIAM books.  Herewith: nsoli\n",
       "\n",
       "You must allocate storage for the function and the Krylov basis in advance –> in the calling program <– ie. in FS and FPS\n",
       "\n",
       "Inputs:\n",
       "\n",
       "\\begin{itemize}\n",
       "\\item F!: function evaluation, the ! indicates that F! overwrites FS, your   preallocated storage for the function.\n",
       "\n",
       "So FS=F!(FS,x) or FS=F!(FS,x,pdata) returns FS=F(x)\n",
       "\n",
       "\\end{itemize}\n",
       "\\begin{itemize}\n",
       "\\item x0: initial iterate\n",
       "\n",
       "\\end{itemize}\n",
       "\\begin{itemize}\n",
       "\\item FS: Preallocated storage for function. It is an N x 1 column vector\n",
       "\n",
       "\\end{itemize}\n",
       "\\begin{itemize}\n",
       "\\item FPS: preallocated storage for the Krylov basis. It is an N x m matrix where      you plan to take at most m-1 GMRES iterations before a restart. \n",
       "\n",
       "\\end{itemize}\n",
       "\\begin{itemize}\n",
       "\\item Jvec: Jacobian vector product, If you leave this out the   default is a finite difference directional derivative.\n",
       "\n",
       "So, FP=Jvec(v,FS,x) or FP=Jvec(v,FS,x,pdata) returns FP=F'(x) v. \n",
       "\n",
       "(v, FS, x) or (v, FS, x, pdata) must be the argument list,    even if FP does not need FS.   One reason for this is that the finite-difference derivative   does and that is the default in the solver.\n",
       "\n",
       "\n",
       "\\item Precision: Lemme tell ya 'bout precision. I designed this code for    full precision functions and linear algebra in any precision you want.    You can declare FPS as Float64 or Float32 and nsoli    will do the right thing. Float16 support is there, but not working well.\n",
       "\n",
       "If the Jacobian is reasonably well conditioned, you can cut the cost   of orthogonalization and storage (for GMRES) in half with no loss.    There is no benefit if your linear solver is not GMRES or if    othogonalization and storage of the Krylov vectors is only a   small part of the cost of the computation. So if your preconditioner   is good and you only need a few Krylovs/Newton, reduced precision won't   help you much.\n",
       "\n",
       "\\end{itemize}\n",
       "\\rule{\\textwidth}{1pt}\n",
       "Keyword Arguments (kwargs):\n",
       "\n",
       "rtol and atol: relative and absolute error tolerances\n",
       "\n",
       "maxit: limit on nonlinear iterations\n",
       "\n",
       "lmaxit: limit on linear iterations. If lmaxit > m-1, where FPS has m columns, and you need more than m-1 linear iterations, then GMRES  will restart. \n",
       "\n",
       "The default is -1. This means that you'll take m-1 iterations, where size(V) = (n,m), and get no restarts.\n",
       "\n",
       "lsolver: the linear solver, default = \"gmres\"\n",
       "\n",
       "Your choices will be \"gmres\" or \"bicgstab\". However, gmres is the only option for now.\n",
       "\n",
       "eta and fixed eta: eta > 0 or there's an error\n",
       "\n",
       "The linear solver terminates when ||F'(x)s + F(x) || <= etag || F(x) ||\n",
       "\n",
       "where \n",
       "\n",
       "etag = eta if fixedeta=true\n",
       "\n",
       "etag = Eisenstat-Walker as implemented in book if fixedeta=false\n",
       "\n",
       "The default, which may change, is eta=.1, fixedeta=true\n",
       "\n",
       "Pvec: Preconditioner-vector product. The rules are similar to Jvec     So, Pv=Pvec(v,x) or Pv=Pvec(v,x,pdata) returns P(x) v where     P(x) is the preconditioner. You must use x as an input even     if your preconditioner does not depend on x\n",
       "\n",
       "pside: apply preconditioner on pside, default = \"right\". I do not       recommend \"left\". See Chapter 3 for the story on this.\n",
       "\n",
       "armmax: upper bound on step size reductions in line search\n",
       "\n",
       "dx: default = 1.e-7\n",
       "\n",
       "difference increment in finite-difference derivatives       h=dx*norm(x,Inf)+1.e-8\n",
       "\n",
       "armfix: default = false\n",
       "\n",
       "The default is a parabolic line search (ie false). Set to true and the step size will be fixed at .5. Don't do this unless you are doing experiments for research.\n",
       "\n",
       "pdata:\n",
       "\n",
       "precomputed data for the function/Jacobian-vector/Preconditioner-vector products.  Things will go better if you use this rather than hide the data  in global variables within the module for your function/Jacobian\n",
       "\n",
       "If you use pdata in any of F!, Jvec, or Pvec, you must use in in all of them.\n",
       "\n",
       "printerr: default = true\n",
       "\n",
       "I print a helpful message when the solver fails. To suppress that message set printerr to false.\n",
       "\n",
       "keepsolhist: default = false\n",
       "\n",
       "Set this to true to get the history of the iteration in the output tuple. This is on by default for scalar equations and off for systems. Only turn it on if you have use for the data, which can get REALLY LARGE.\n",
       "\n",
       "stagnationok: default = false\n",
       "\n",
       "Set this to true if you want to disable the line search and either observe divergence or stagnation. This is only useful for research or writing a book.\n",
       "\n",
       "Output:\n",
       "\n",
       "\\begin{itemize}\n",
       "\\item A named tuple (solution, functionval, history, stats, idid,              errcode, solhist)\n",
       "\n",
       "\\end{itemize}\n",
       "where\n",
       "\n",
       "– solution = converged result\n",
       "\n",
       "– functionval = F(solution)\n",
       "\n",
       "– history = the vector of residual norms (||F(x)||) for the iteration\n",
       "\n",
       "– stats = named tuple of the history of (ifun, ijac, iarm, ikfail), the  number of functions/Jacobian-vector prods/steplength reductions/linear solver failures at each iteration. Linear solver failures DO NOT mean that the nonlinear solver will fail. You should look at this stat if, for example, the line search fails. Increasing the size of FPS and/or lmaxit might solve the problem.\n",
       "\n",
       "I do not count the function values for a finite-difference derivative because they count toward a Jacobian-vector product.\n",
       "\n",
       "– idid=true if the iteration succeeded and false if not.\n",
       "\n",
       "– errcode = 0 if if the iteration succeeded\n",
       "\n",
       "\\begin{verbatim}\n",
       "    = -1 if the initial iterate satisfies the termination criteria\n",
       "\n",
       "    = 10 if no convergence after maxit iterations\n",
       "\n",
       "    = 1  if the line search failed\n",
       "\\end{verbatim}\n",
       "– solhist:\n",
       "\n",
       "\\begin{verbatim}\n",
       "  This is the entire history of the iteration if you've set\n",
       "  keepsolhist=true\n",
       "\\end{verbatim}\n",
       "solhist is an N x K array where N is the length of x and K is the number of iteration + 1. So, for scalar equations, it's a row vector.\n",
       "\n",
       "\\rule{\\textwidth}{1pt}\n",
       "\\subsection{Example from the docstrings for nsoli}\n",
       "\\subsubsection{Simple 2D problem.}\n",
       "You should get the same results as for nsol.jl because GMRES will solve the equation for the step exactly in two iterations. Finite difference Jacobians and analytic Jacobian-vector products for full precision and finite difference Jacobian-vector products for single precision.\n",
       "\n",
       "\\begin{verbatim}\n",
       "julia> function f!(fv,x)\n",
       "       fv[1]=x[1] + sin(x[2])\n",
       "       fv[2]=cos(x[1]+x[2])\n",
       "       end\n",
       "f! (generic function with 1 method)\n",
       "\n",
       "julia> function JVec(v, fv, x)\n",
       "       jvec=zeros(2,);\n",
       "       p=-sin(x[1]+x[2])\n",
       "       jvec[1]=v[1]+cos(x[2])*v[2]\n",
       "       jvec[2]=p*(v[1]+v[2])\n",
       "       return jvec\n",
       "       end\n",
       "JVec (generic function with 1 method)\n",
       "\n",
       "julia> x0=ones(2,); fv=zeros(2,); jv=zeros(2,2); jv32=zeros(Float32,2,2);\n",
       "\n",
       "julia> jvs=zeros(2,3); jvs32=zeros(Float32,2,3);\n",
       "\n",
       "julia> nout=nsol(f!,x0,fv,jv; sham=1);\n",
       "\n",
       "julia> kout=nsoli(f!,x0,fv,jvs,JVec; fixedeta=true, eta=.1, lmaxit=2);\n",
       "\n",
       "julia> kout32=nsoli(f!,x0,fv,jvs32; fixedeta=true, eta=.1, lmaxit=2);\n",
       "\n",
       "julia> [nout.history kout.history kout32.history]\n",
       "5×3 Array{Float64,2}:\n",
       " 1.88791e+00  1.88791e+00  1.88791e+00\n",
       " 2.43119e-01  2.43120e-01  2.43119e-01\n",
       " 1.19231e-02  1.19231e-02  1.19231e-02\n",
       " 1.03266e-05  1.03261e-05  1.03273e-05\n",
       " 1.46416e-11  1.40862e-11  1.45457e-11\n",
       "\\end{verbatim}\n"
      ],
      "text/markdown": [
       "```\n",
       "nsoli(F!, x0, FS, FPS, Jvec=dirder; rtol=1.e-6, atol=1.e-12,\n",
       "           maxit=20, lmaxit=-1, lsolver=\"gmres\", eta=.1,\n",
       "           fixedeta=true, Pvec=nothing, pside=\"right\",\n",
       "           armmax=10, dx = 1.e-7, armfix=false, pdata = nothing,\n",
       "           printerr = true, keepsolhist = false, stagnationok=false)\n",
       "```\n",
       "\n",
       ")\n",
       "\n",
       "C. T. Kelley, 2021\n",
       "\n",
       "Julia versions of the nonlinear solvers from my SIAM books.  Herewith: nsoli\n",
       "\n",
       "You must allocate storage for the function and the Krylov basis in advance –> in the calling program <– ie. in FS and FPS\n",
       "\n",
       "Inputs:\n",
       "\n",
       "  * F!: function evaluation, the ! indicates that F! overwrites FS, your   preallocated storage for the function.\n",
       "\n",
       "    So FS=F!(FS,x) or FS=F!(FS,x,pdata) returns FS=F(x)\n",
       "\n",
       "  * x0: initial iterate\n",
       "\n",
       "  * FS: Preallocated storage for function. It is an N x 1 column vector\n",
       "\n",
       "  * FPS: preallocated storage for the Krylov basis. It is an N x m matrix where      you plan to take at most m-1 GMRES iterations before a restart.\n",
       "\n",
       "  * Jvec: Jacobian vector product, If you leave this out the   default is a finite difference directional derivative.\n",
       "\n",
       "    So, FP=Jvec(v,FS,x) or FP=Jvec(v,FS,x,pdata) returns FP=F'(x) v. \n",
       "\n",
       "    (v, FS, x) or (v, FS, x, pdata) must be the argument list,    even if FP does not need FS.   One reason for this is that the finite-difference derivative   does and that is the default in the solver.\n",
       "  * Precision: Lemme tell ya 'bout precision. I designed this code for    full precision functions and linear algebra in any precision you want.    You can declare FPS as Float64 or Float32 and nsoli    will do the right thing. Float16 support is there, but not working well.\n",
       "\n",
       "    If the Jacobian is reasonably well conditioned, you can cut the cost   of orthogonalization and storage (for GMRES) in half with no loss.    There is no benefit if your linear solver is not GMRES or if    othogonalization and storage of the Krylov vectors is only a   small part of the cost of the computation. So if your preconditioner   is good and you only need a few Krylovs/Newton, reduced precision won't   help you much.\n",
       "\n",
       "---\n",
       "\n",
       "Keyword Arguments (kwargs):\n",
       "\n",
       "rtol and atol: relative and absolute error tolerances\n",
       "\n",
       "maxit: limit on nonlinear iterations\n",
       "\n",
       "lmaxit: limit on linear iterations. If lmaxit > m-1, where FPS has m columns, and you need more than m-1 linear iterations, then GMRES  will restart. \n",
       "\n",
       "The default is -1. This means that you'll take m-1 iterations, where size(V) = (n,m), and get no restarts.\n",
       "\n",
       "lsolver: the linear solver, default = \"gmres\"\n",
       "\n",
       "Your choices will be \"gmres\" or \"bicgstab\". However, gmres is the only option for now.\n",
       "\n",
       "eta and fixed eta: eta > 0 or there's an error\n",
       "\n",
       "The linear solver terminates when ||F'(x)s + F(x) || <= etag || F(x) ||\n",
       "\n",
       "where \n",
       "\n",
       "etag = eta if fixedeta=true\n",
       "\n",
       "etag = Eisenstat-Walker as implemented in book if fixedeta=false\n",
       "\n",
       "The default, which may change, is eta=.1, fixedeta=true\n",
       "\n",
       "Pvec: Preconditioner-vector product. The rules are similar to Jvec     So, Pv=Pvec(v,x) or Pv=Pvec(v,x,pdata) returns P(x) v where     P(x) is the preconditioner. You must use x as an input even     if your preconditioner does not depend on x\n",
       "\n",
       "pside: apply preconditioner on pside, default = \"right\". I do not       recommend \"left\". See Chapter 3 for the story on this.\n",
       "\n",
       "armmax: upper bound on step size reductions in line search\n",
       "\n",
       "dx: default = 1.e-7\n",
       "\n",
       "difference increment in finite-difference derivatives       h=dx*norm(x,Inf)+1.e-8\n",
       "\n",
       "armfix: default = false\n",
       "\n",
       "The default is a parabolic line search (ie false). Set to true and the step size will be fixed at .5. Don't do this unless you are doing experiments for research.\n",
       "\n",
       "pdata:\n",
       "\n",
       "precomputed data for the function/Jacobian-vector/Preconditioner-vector products.  Things will go better if you use this rather than hide the data  in global variables within the module for your function/Jacobian\n",
       "\n",
       "If you use pdata in any of F!, Jvec, or Pvec, you must use in in all of them.\n",
       "\n",
       "printerr: default = true\n",
       "\n",
       "I print a helpful message when the solver fails. To suppress that message set printerr to false.\n",
       "\n",
       "keepsolhist: default = false\n",
       "\n",
       "Set this to true to get the history of the iteration in the output tuple. This is on by default for scalar equations and off for systems. Only turn it on if you have use for the data, which can get REALLY LARGE.\n",
       "\n",
       "stagnationok: default = false\n",
       "\n",
       "Set this to true if you want to disable the line search and either observe divergence or stagnation. This is only useful for research or writing a book.\n",
       "\n",
       "Output:\n",
       "\n",
       "  * A named tuple (solution, functionval, history, stats, idid,              errcode, solhist)\n",
       "\n",
       "where\n",
       "\n",
       "– solution = converged result\n",
       "\n",
       "– functionval = F(solution)\n",
       "\n",
       "– history = the vector of residual norms (||F(x)||) for the iteration\n",
       "\n",
       "– stats = named tuple of the history of (ifun, ijac, iarm, ikfail), the  number of functions/Jacobian-vector prods/steplength reductions/linear solver failures at each iteration. Linear solver failures DO NOT mean that the nonlinear solver will fail. You should look at this stat if, for example, the line search fails. Increasing the size of FPS and/or lmaxit might solve the problem.\n",
       "\n",
       "I do not count the function values for a finite-difference derivative because they count toward a Jacobian-vector product.\n",
       "\n",
       "– idid=true if the iteration succeeded and false if not.\n",
       "\n",
       "– errcode = 0 if if the iteration succeeded\n",
       "\n",
       "```\n",
       "    = -1 if the initial iterate satisfies the termination criteria\n",
       "\n",
       "    = 10 if no convergence after maxit iterations\n",
       "\n",
       "    = 1  if the line search failed\n",
       "```\n",
       "\n",
       "– solhist:\n",
       "\n",
       "```\n",
       "  This is the entire history of the iteration if you've set\n",
       "  keepsolhist=true\n",
       "```\n",
       "\n",
       "solhist is an N x K array where N is the length of x and K is the number of iteration + 1. So, for scalar equations, it's a row vector.\n",
       "\n",
       "---\n",
       "\n",
       "## Example from the docstrings for nsoli\n",
       "\n",
       "### Simple 2D problem.\n",
       "\n",
       "You should get the same results as for nsol.jl because GMRES will solve the equation for the step exactly in two iterations. Finite difference Jacobians and analytic Jacobian-vector products for full precision and finite difference Jacobian-vector products for single precision.\n",
       "\n",
       "```jldoctest\n",
       "julia> function f!(fv,x)\n",
       "       fv[1]=x[1] + sin(x[2])\n",
       "       fv[2]=cos(x[1]+x[2])\n",
       "       end\n",
       "f! (generic function with 1 method)\n",
       "\n",
       "julia> function JVec(v, fv, x)\n",
       "       jvec=zeros(2,);\n",
       "       p=-sin(x[1]+x[2])\n",
       "       jvec[1]=v[1]+cos(x[2])*v[2]\n",
       "       jvec[2]=p*(v[1]+v[2])\n",
       "       return jvec\n",
       "       end\n",
       "JVec (generic function with 1 method)\n",
       "\n",
       "julia> x0=ones(2,); fv=zeros(2,); jv=zeros(2,2); jv32=zeros(Float32,2,2);\n",
       "\n",
       "julia> jvs=zeros(2,3); jvs32=zeros(Float32,2,3);\n",
       "\n",
       "julia> nout=nsol(f!,x0,fv,jv; sham=1);\n",
       "\n",
       "julia> kout=nsoli(f!,x0,fv,jvs,JVec; fixedeta=true, eta=.1, lmaxit=2);\n",
       "\n",
       "julia> kout32=nsoli(f!,x0,fv,jvs32; fixedeta=true, eta=.1, lmaxit=2);\n",
       "\n",
       "julia> [nout.history kout.history kout32.history]\n",
       "5×3 Array{Float64,2}:\n",
       " 1.88791e+00  1.88791e+00  1.88791e+00\n",
       " 2.43119e-01  2.43120e-01  2.43119e-01\n",
       " 1.19231e-02  1.19231e-02  1.19231e-02\n",
       " 1.03266e-05  1.03261e-05  1.03273e-05\n",
       " 1.46416e-11  1.40862e-11  1.45457e-11\n",
       "```\n"
      ],
      "text/plain": [
       "\u001b[36m  nsoli(F!, x0, FS, FPS, Jvec=dirder; rtol=1.e-6, atol=1.e-12,\u001b[39m\n",
       "\u001b[36m             maxit=20, lmaxit=-1, lsolver=\"gmres\", eta=.1,\u001b[39m\n",
       "\u001b[36m             fixedeta=true, Pvec=nothing, pside=\"right\",\u001b[39m\n",
       "\u001b[36m             armmax=10, dx = 1.e-7, armfix=false, pdata = nothing,\u001b[39m\n",
       "\u001b[36m             printerr = true, keepsolhist = false, stagnationok=false)\u001b[39m\n",
       "\n",
       "  )\n",
       "\n",
       "  C. T. Kelley, 2021\n",
       "\n",
       "  Julia versions of the nonlinear solvers from my SIAM books. Herewith: nsoli\n",
       "\n",
       "  You must allocate storage for the function and the Krylov basis in advance\n",
       "  –> in the calling program <– ie. in FS and FPS\n",
       "\n",
       "  Inputs:\n",
       "\n",
       "    •  F!: function evaluation, the ! indicates that F! overwrites FS,\n",
       "       your preallocated storage for the function.\n",
       "       So FS=F!(FS,x) or FS=F!(FS,x,pdata) returns FS=F(x)\n",
       "\n",
       "    •  x0: initial iterate\n",
       "\n",
       "    •  FS: Preallocated storage for function. It is an N x 1 column\n",
       "       vector\n",
       "\n",
       "    •  FPS: preallocated storage for the Krylov basis. It is an N x m\n",
       "       matrix where you plan to take at most m-1 GMRES iterations before\n",
       "       a restart.\n",
       "\n",
       "    •  Jvec: Jacobian vector product, If you leave this out the default\n",
       "       is a finite difference directional derivative.\n",
       "       So, FP=Jvec(v,FS,x) or FP=Jvec(v,FS,x,pdata) returns FP=F'(x) v.\n",
       "       (v, FS, x) or (v, FS, x, pdata) must be the argument list, even if\n",
       "       FP does not need FS. One reason for this is that the\n",
       "       finite-difference derivative does and that is the default in the\n",
       "       solver.\n",
       "\n",
       "    •  Precision: Lemme tell ya 'bout precision. I designed this code for\n",
       "       full precision functions and linear algebra in any precision you\n",
       "       want. You can declare FPS as Float64 or Float32 and nsoli will do\n",
       "       the right thing. Float16 support is there, but not working well.\n",
       "       If the Jacobian is reasonably well conditioned, you can cut the\n",
       "       cost of orthogonalization and storage (for GMRES) in half with no\n",
       "       loss. There is no benefit if your linear solver is not GMRES or if\n",
       "       othogonalization and storage of the Krylov vectors is only a small\n",
       "       part of the cost of the computation. So if your preconditioner is\n",
       "       good and you only need a few Krylovs/Newton, reduced precision\n",
       "       won't help you much.\n",
       "\n",
       "  ────────────────────────────────────────────────────────────────────────────\n",
       "\n",
       "  Keyword Arguments (kwargs):\n",
       "\n",
       "  rtol and atol: relative and absolute error tolerances\n",
       "\n",
       "  maxit: limit on nonlinear iterations\n",
       "\n",
       "  lmaxit: limit on linear iterations. If lmaxit > m-1, where FPS has m\n",
       "  columns, and you need more than m-1 linear iterations, then GMRES will\n",
       "  restart.\n",
       "\n",
       "  The default is -1. This means that you'll take m-1 iterations, where size(V)\n",
       "  = (n,m), and get no restarts.\n",
       "\n",
       "  lsolver: the linear solver, default = \"gmres\"\n",
       "\n",
       "  Your choices will be \"gmres\" or \"bicgstab\". However, gmres is the only\n",
       "  option for now.\n",
       "\n",
       "  eta and fixed eta: eta > 0 or there's an error\n",
       "\n",
       "  The linear solver terminates when ||F'(x)s + F(x) || <= etag || F(x) ||\n",
       "\n",
       "  where\n",
       "\n",
       "  etag = eta if fixedeta=true\n",
       "\n",
       "  etag = Eisenstat-Walker as implemented in book if fixedeta=false\n",
       "\n",
       "  The default, which may change, is eta=.1, fixedeta=true\n",
       "\n",
       "  Pvec: Preconditioner-vector product. The rules are similar to Jvec So,\n",
       "  Pv=Pvec(v,x) or Pv=Pvec(v,x,pdata) returns P(x) v where P(x) is the\n",
       "  preconditioner. You must use x as an input even if your preconditioner does\n",
       "  not depend on x\n",
       "\n",
       "  pside: apply preconditioner on pside, default = \"right\". I do not recommend\n",
       "  \"left\". See Chapter 3 for the story on this.\n",
       "\n",
       "  armmax: upper bound on step size reductions in line search\n",
       "\n",
       "  dx: default = 1.e-7\n",
       "\n",
       "  difference increment in finite-difference derivatives h=dx*norm(x,Inf)+1.e-8\n",
       "\n",
       "  armfix: default = false\n",
       "\n",
       "  The default is a parabolic line search (ie false). Set to true and the step\n",
       "  size will be fixed at .5. Don't do this unless you are doing experiments for\n",
       "  research.\n",
       "\n",
       "  pdata:\n",
       "\n",
       "  precomputed data for the function/Jacobian-vector/Preconditioner-vector\n",
       "  products. Things will go better if you use this rather than hide the data in\n",
       "  global variables within the module for your function/Jacobian\n",
       "\n",
       "  If you use pdata in any of F!, Jvec, or Pvec, you must use in in all of\n",
       "  them.\n",
       "\n",
       "  printerr: default = true\n",
       "\n",
       "  I print a helpful message when the solver fails. To suppress that message\n",
       "  set printerr to false.\n",
       "\n",
       "  keepsolhist: default = false\n",
       "\n",
       "  Set this to true to get the history of the iteration in the output tuple.\n",
       "  This is on by default for scalar equations and off for systems. Only turn it\n",
       "  on if you have use for the data, which can get REALLY LARGE.\n",
       "\n",
       "  stagnationok: default = false\n",
       "\n",
       "  Set this to true if you want to disable the line search and either observe\n",
       "  divergence or stagnation. This is only useful for research or writing a\n",
       "  book.\n",
       "\n",
       "  Output:\n",
       "\n",
       "    •  A named tuple (solution, functionval, history, stats, idid,\n",
       "       errcode, solhist)\n",
       "\n",
       "  where\n",
       "\n",
       "  – solution = converged result\n",
       "\n",
       "  – functionval = F(solution)\n",
       "\n",
       "  – history = the vector of residual norms (||F(x)||) for the iteration\n",
       "\n",
       "  – stats = named tuple of the history of (ifun, ijac, iarm, ikfail), the\n",
       "  number of functions/Jacobian-vector prods/steplength reductions/linear\n",
       "  solver failures at each iteration. Linear solver failures DO NOT mean that\n",
       "  the nonlinear solver will fail. You should look at this stat if, for\n",
       "  example, the line search fails. Increasing the size of FPS and/or lmaxit\n",
       "  might solve the problem.\n",
       "\n",
       "  I do not count the function values for a finite-difference derivative\n",
       "  because they count toward a Jacobian-vector product.\n",
       "\n",
       "  – idid=true if the iteration succeeded and false if not.\n",
       "\n",
       "  – errcode = 0 if if the iteration succeeded\n",
       "\n",
       "\u001b[36m      = -1 if the initial iterate satisfies the termination criteria\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m      = 10 if no convergence after maxit iterations\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m      = 1  if the line search failed\u001b[39m\n",
       "\n",
       "  – solhist:\n",
       "\n",
       "\u001b[36m    This is the entire history of the iteration if you've set\u001b[39m\n",
       "\u001b[36m    keepsolhist=true\u001b[39m\n",
       "\n",
       "  solhist is an N x K array where N is the length of x and K is the number of\n",
       "  iteration + 1. So, for scalar equations, it's a row vector.\n",
       "\n",
       "  ────────────────────────────────────────────────────────────────────────────\n",
       "\n",
       "\u001b[1m  Example from the docstrings for nsoli\u001b[22m\n",
       "\u001b[1m  =======================================\u001b[22m\n",
       "\n",
       "\u001b[1m  Simple 2D problem.\u001b[22m\n",
       "\u001b[1m  ––––––––––––––––––––\u001b[22m\n",
       "\n",
       "  You should get the same results as for nsol.jl because GMRES will solve the\n",
       "  equation for the step exactly in two iterations. Finite difference Jacobians\n",
       "  and analytic Jacobian-vector products for full precision and finite\n",
       "  difference Jacobian-vector products for single precision.\n",
       "\n",
       "\u001b[36m  julia> function f!(fv,x)\u001b[39m\n",
       "\u001b[36m         fv[1]=x[1] + sin(x[2])\u001b[39m\n",
       "\u001b[36m         fv[2]=cos(x[1]+x[2])\u001b[39m\n",
       "\u001b[36m         end\u001b[39m\n",
       "\u001b[36m  f! (generic function with 1 method)\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> function JVec(v, fv, x)\u001b[39m\n",
       "\u001b[36m         jvec=zeros(2,);\u001b[39m\n",
       "\u001b[36m         p=-sin(x[1]+x[2])\u001b[39m\n",
       "\u001b[36m         jvec[1]=v[1]+cos(x[2])*v[2]\u001b[39m\n",
       "\u001b[36m         jvec[2]=p*(v[1]+v[2])\u001b[39m\n",
       "\u001b[36m         return jvec\u001b[39m\n",
       "\u001b[36m         end\u001b[39m\n",
       "\u001b[36m  JVec (generic function with 1 method)\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> x0=ones(2,); fv=zeros(2,); jv=zeros(2,2); jv32=zeros(Float32,2,2);\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> jvs=zeros(2,3); jvs32=zeros(Float32,2,3);\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> nout=nsol(f!,x0,fv,jv; sham=1);\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> kout=nsoli(f!,x0,fv,jvs,JVec; fixedeta=true, eta=.1, lmaxit=2);\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> kout32=nsoli(f!,x0,fv,jvs32; fixedeta=true, eta=.1, lmaxit=2);\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> [nout.history kout.history kout32.history]\u001b[39m\n",
       "\u001b[36m  5×3 Array{Float64,2}:\u001b[39m\n",
       "\u001b[36m   1.88791e+00  1.88791e+00  1.88791e+00\u001b[39m\n",
       "\u001b[36m   2.43119e-01  2.43120e-01  2.43119e-01\u001b[39m\n",
       "\u001b[36m   1.19231e-02  1.19231e-02  1.19231e-02\u001b[39m\n",
       "\u001b[36m   1.03266e-05  1.03261e-05  1.03273e-05\u001b[39m\n",
       "\u001b[36m   1.46416e-11  1.40862e-11  1.45457e-11\u001b[39m"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?nsoli"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 3.7.2: Benchmarking the H-equation with nsoli.jl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will begin by comparing the fastest solution from Chapter 2 with two variants of Newton-GMRES, one with fixed $\\eta = .1$ and one with the Eisenstat-Walker forcing term with $\\eta_{max}=.9$ and $\\gamma = .9$. I'll allocate 20 vectors for the Krylov basis in the array FPK.\n",
    "\n",
    "We'll begin with a small version of the problem and compare the iteration statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=512;\n",
    "FS=ones(n,); FPS=ones(n,n); FPS32=ones(Float32,n,n); x0=ones(n,); c=.5; hdata = heqinit(x0, c);\n",
    "bargs=(atol = 1.e-10, rtol = 1.e-10, sham = 5, resdec = .1, pdata=hdata);\n",
    "FPK=zeros(n,20);\n",
    "# Fixed eta = .1\n",
    "kbargs=(atol = 1.e-10, rtol = 1.e-10, eta=.1, fixedeta=true, pdata=hdata);\n",
    "# Eisenstat-Walker\n",
    "kbargsew=(atol = 1.e-10, rtol = 1.e-10, eta=.9, fixedeta=false, pdata=hdata);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll run the winner from Chapter 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nout=nsol(heqf!, x0, FS, FPS32, heqJ!; bargs...);\n",
    "kout=nsoli(heqf!, x0, FS, FPK; kbargs...);\n",
    "koutew=nsoli(heqf!, x0, FS, FPK; kbargsew...);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's interesting to compare the residual histories. They are essentially the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6×3 Matrix{Float64}:\n",
       " 3.49504e+00  3.49504e+00  3.49504e+00\n",
       " 1.79696e-02  4.98627e-02  4.98627e-02\n",
       " 1.55512e-04  1.84641e-03  1.84641e-03\n",
       " 1.33167e-06  1.82364e-04  1.82364e-04\n",
       " 1.13962e-08  2.34291e-06  2.34291e-06\n",
       " 9.75271e-11  2.42540e-11  2.42540e-11"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[nout.history kout.history koutew.history]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the costs is harder. While a Jacobian-vector product for this problem has the same cost as a call to the function, the cost per iteration for nsol.jl is harder to evaluate in these terms. It's better to look at the benchmark results for a larger problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=4096;\n",
    "FS=ones(n,); FPS=ones(n,n); FPS32=ones(Float32,n,n); x0=ones(n,); c=.5; hdata = heqinit(x0, c);\n",
    "bargs=(atol = 1.e-10, rtol = 1.e-10, sham = 5, resdec = .1, pdata=hdata);\n",
    "FPK=zeros(n,20);\n",
    "kbargs=(atol = 1.e-10, rtol = 1.e-10, eta=.1, fixedeta=true, pdata=hdata);\n",
    "kbargsew=(atol = 1.e-10, rtol = 1.e-10, eta=.9, fixedeta=false, pdata=hdata);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shamanskii, n=5\n",
      "  81.413 ms (8271 allocations: 1.10 MiB)\n",
      "Newton-GMRES, fixed eta\n",
      "  1.682 ms (407 allocations: 1.73 MiB)\n",
      "Newton-GMRES, Eisenstat-Walker\n",
      "  1.680 ms (407 allocations: 1.73 MiB)\n"
     ]
    }
   ],
   "source": [
    "println(\"Shamanskii, n=5\"); @btime nsol(heqf!, $x0, $FS, $FPS32, heqJ!; bargs...);\n",
    "println(\"Newton-GMRES, fixed eta\"); @btime nsoli(heqf!, $x0, $FS, $FPK; kbargs...);\n",
    "println(\"Newton-GMRES, Eisenstat-Walker\"); @btime nsoli(heqf!, $x0, $FS, $FPK; kbargsew...);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Newton-Krylov code is over 50 times faster. This is not unique to this problem. If your Jacobian is well-conditioned or you have a good preconditioner, as we do in the PDE example, Newton-Krylov should perform much better than any variation of Newton's method using direct linear solvers.\n",
    "\n",
    "The other interesting thing in this example is that the two forcing term choices performed equally well. \n",
    "\n",
    "Finally we will see if storing the Krylov basis in single precision improves matters. It's easy to do this by simply replacing ```FPK``` with ```FPK32```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Newton-GMRES, fixed eta\n",
      "  1.719 ms (408 allocations: 1.72 MiB)\n",
      "Newton-GMRES, Eisenstat-Walker\n",
      "  1.727 ms (408 allocations: 1.72 MiB)\n"
     ]
    }
   ],
   "source": [
    "#n=4096;\n",
    "#FS=ones(n,); FPS=ones(n,n); FPS32=ones(Float32,n,n); x0=ones(n,); c=.5; hdata = heqinit(x0, c);\n",
    "FPK32=zeros(Float32,n,20)\n",
    "println(\"Newton-GMRES, fixed eta\"); @btime nsoli(heqf!, $x0, $FS, $FPK32; kbargs...);\n",
    "println(\"Newton-GMRES, Eisenstat-Walker\"); @btime nsoli(heqf!, $x0, $FS, $FPK32; kbargsew...);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is essentially no difference between storing the basis in single and double. It is easy in hindsight to see why. Each function evaluation and forward difference Jacobian-vector product is $O(N \\log N)$ work. The cost of othogonalization for $k$ GMRES iterations with classical Gram-Schmidt twice is $k^2 N$ (can you see why). So if we do $k$ Krylov iterations per Newton the cost of orthogonalization is $k^2 N$ and the cost of calls to the residual is $O(k N \\log N)$. The computation is dominated by the calls to the residual unless $k$ is very large. \n",
    "\n",
    "We will quantify this with a computation to look at the iteration statistics. It is sufficient to look at the\n",
    "fixed $\\eta = .1$ case. The results for the Eisenstat-Walker forcing term are exactly the same.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 1, 1, 1, 2]\n"
     ]
    }
   ],
   "source": [
    "fixedetaout = nsoli(heqf!, x0, FS, FPK; kbargs...);\n",
    "println(fixedetaout.stats.ijac)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The statistics indicate that we converge after a single GMRES iteration and are taking a single Krylov per Newton for most of the iteration (remember that the initial iteration is $\\vs = 0$ when computing the Newton step). So the orthogonalization cost is $N$ and the function evaluation cost is $O(N \\log N)$. We would expect that storing the Krylov basis  in single precision would have very little benefit, and that is exactly what we see.\n",
    "\n",
    "We invite the reader to increase $c$ and the dimension of the problem to see if anything changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 3.7.3: Preconditioning the Convection-Diffusion Equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will benchmark the Newton-GMRES iteration agains the direct solvers from Chapter 2 and explore the differences between left and right preconditioning. We will begin by repeating the computation for the fastest version using __nsol.jl__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nsol, sham=5\n",
      "  6.142 ms (386 allocations: 6.55 MiB)\n"
     ]
    }
   ],
   "source": [
    "n=31;\n",
    "# Get some room for the residual\n",
    "u0=zeros(n*n,);\n",
    "FV=copy(u0);\n",
    "# Get the precomputed data from pdeinit\n",
    "pdata=pdeinit(n)\n",
    "# Storage for the Jacobian, same sparsity pattern as the discrete Laplacian\n",
    "J=copy(pdata.D2);\n",
    "# Iteration Parameters\n",
    "rtol=1.e-7\n",
    "atol=1.e-10\n",
    "println(\"nsol, sham=5\"); @btime nsol(pdeF!, u0, FV, J, pdeJ!; resdec=.5, rtol=rtol, atol=atol, pdata=pdata, sham=5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll set up the problem for nsoli. We need to allocate storage for the Krylov basis. One case will be no preconditioning at all, so the Kryov basis will need more storage. The analytic Jacobian-vector product is __Jvec2d.jl__, which is in __TestProblems/EllipticPDE.jl__. The preconditioner is __Pvec2d.jl__ from __TestProblems/PDE_Tools.jl__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nsoli, not preconditioned\n",
      "  3.384 ms (3946 allocations: 1.06 MiB)\n"
     ]
    }
   ],
   "source": [
    "# Storage for the Krylov basis\n",
    "    JV = zeros(n * n, 100)\n",
    "    eta=.1\n",
    "    fixedeta=false\n",
    "println(\"nsoli, not preconditioned\")\n",
    "@btime nsoli(pdeF!, u0, FV, JV, Jvec2d; rtol=rtol, atol=atol, Pvec=nothing, pdata=pdata, eta=eta,\n",
    "            fixedeta=fixedeta, pside=\"right\");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even with no preconditioning, the iterative solver is almost as fast as __nsol.jl__ using the direct method. When you precondition, which we will do from the right for now, the difference is a factor of almost two over the solve without preconditioning. This difference would increase with a finer mesh. Try it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nsoli, preconditioned, Eisenstat-Walker forcing term\n",
      "  1.857 ms (970 allocations: 700.83 KiB)\n"
     ]
    }
   ],
   "source": [
    "println(\"nsoli, preconditioned, Eisenstat-Walker forcing term\")\n",
    "@btime nsoli(pdeF!, u0, FV, JV, Jvec2d; rtol=rtol, atol=atol, Pvec=Pvec2d, pdata=pdata, eta=eta,\n",
    "            fixedeta=fixedeta, pside=\"right\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will benchmark with a fixed forcing term for our next example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nsoli, preconditioned, fixed eta\n",
      "  2.444 ms (1245 allocations: 1002.52 KiB)\n"
     ]
    }
   ],
   "source": [
    "fixedeta=true;\n",
    "println(\"nsoli, preconditioned, fixed eta\")\n",
    "@btime nsoli(pdeF!, u0, FV, JV, Jvec2d; rtol=rtol, atol=atol, Pvec=Pvec2d, pdata=pdata, eta=eta,\n",
    "            fixedeta=fixedeta, pside=\"right\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example, we see that Eisenstat-Walker is a bit better. Finally, we return to Eisenstat-Walker with $\\eta_{max} = .9$. We see very little difference from $\\eta_{max}=.1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nsoli, preconditioned, Eisenstat-Walker forcing term\n",
      "  1.854 ms (1001 allocations: 797.09 KiB)\n"
     ]
    }
   ],
   "source": [
    "eta=.9; fixedeta=false;\n",
    "println(\"nsoli, preconditioned, Eisenstat-Walker forcing term\")\n",
    "@btime nsoli(pdeF!, u0, FV, JV, Jvec2d; rtol=rtol, atol=atol, Pvec=Pvec2d, pdata=pdata, eta=eta,\n",
    "            fixedeta=fixedeta, pside=\"right\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Left preconditioning? We'll see that even with $\\eta_{max}=.1$ it's a bit slower that right preconditioning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nsoli, left preconditioned, Eisenstat-Walker forcing term\n",
      "  2.089 ms (1112 allocations: 803.78 KiB)\n"
     ]
    }
   ],
   "source": [
    "eta=.1\n",
    "println(\"nsoli, left preconditioned, Eisenstat-Walker forcing term\")\n",
    "@btime nsoli(pdeF!, u0, FV, JV, Jvec2d; rtol=rtol, atol=atol, Pvec=Pvec2d, pdata=pdata, eta=eta,\n",
    "            fixedeta=fixedeta, pside=\"left\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we try left preconditioning with $\\eta_{max} = .9$. We plotted the results in Figure 3.3. While the number of nonlinear itations is roughly double that of the right preconditioned version, the solver time is less than the number of nonlinear iterations would indicate. Can you figure out why that is?\n",
    "\n",
    "Note that we have to increase ```maxit``` to give the nonlinear solver enough iterations to overcome the poor choice of preconditioner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  3.247 ms (2105 allocations: 2.24 MiB)\n"
     ]
    }
   ],
   "source": [
    "eta=.9;\n",
    "@btime nsoli(pdeF!, u0, FV, JV, Jvec2d; rtol=rtol, atol=atol, Pvec=Pvec2d, pdata=pdata, eta=eta, maxit=100,\n",
    "            fixedeta=fixedeta, pside=\"left\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ptcsoli.jl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ptcsoli.jl__ is our Newton-Krylov $\\ptc$ code. Herewith the docstrings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search: \u001b[0m\u001b[1mp\u001b[22m\u001b[0m\u001b[1mt\u001b[22m\u001b[0m\u001b[1mc\u001b[22m\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1ml\u001b[22m \u001b[0m\u001b[1mp\u001b[22m\u001b[0m\u001b[1mt\u001b[22m\u001b[0m\u001b[1mc\u001b[22m\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1ml\u001b[22mi \u001b[0m\u001b[1mp\u001b[22m\u001b[0m\u001b[1mt\u001b[22m\u001b[0m\u001b[1mc\u001b[22m\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1ml\u001b[22msc \u001b[0m\u001b[1mP\u001b[22mar\u001b[0m\u001b[1mt\u001b[22mialQui\u001b[0m\u001b[1mc\u001b[22mk\u001b[0m\u001b[1mS\u001b[22m\u001b[0m\u001b[1mo\u001b[22mrt\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "ptcsol(F!, x0, FS, FPS, J! = diffjac!; rtol=1.e-6, atol=1.e-12,                maxit=20, delta0=1.e-6, dx=1.e-7, pdata = nothing, jfact = klfact,                printerr = true, keepsolhist = false, jknowsdt = false)\n",
       "\n",
       "C. T. Kelley, 2020\n",
       "\n",
       "Julia versions of the nonlinear solvers from my SIAM books.  Herewith: some new stuff ==> ptcsol\n",
       "\n",
       "PTC finds the steady-state solution of u' = -F(u), u(0) = u\\_0. The - sign is a convention.\n",
       "\n",
       "You must allocate storage for the function and Jacobian in advance –> in the calling program <– ie. in FS and FPS\n",
       "\n",
       "Inputs:\n",
       "\n",
       "\\begin{itemize}\n",
       "\\item F!: function evaluation, the ! indicates that F! overwrites FS, your   preallocated storage for the function.\n",
       "\n",
       "So, FV=F!(FV,x) or FV=F!(FV,x,pdata) returns FV=F(x)\n",
       "\n",
       "\n",
       "\\item x0: initial iterate\n",
       "\n",
       "\\end{itemize}\n",
       "\\begin{itemize}\n",
       "\\item FS: Preallocated storage for function. It is an N x 1 column vector.\n",
       "\n",
       "\\end{itemize}\n",
       "You may dimension it as (n,) or (n,1). (n,) is best, but the solvers can deal with it either way.\n",
       "\n",
       "\\begin{itemize}\n",
       "\\item FPS: preallocated storage for Jacobian. It is an N x N matrix\n",
       "\n",
       "If FPS is sparse, you \\textbf{must} allocate storage for the diagonal so I will have room to put 1/dt in there.\n",
       "\n",
       "\n",
       "\\item J!: Jacobian evaluation, the ! indicates that J! overwrites FPS, your   preallocated storage for the Jacobian. If you leave this out the   default is a finite difference Jacobian.\n",
       "\n",
       "So, FP=J!(FP,FV,x) or FP=J!(FP,FV,x,pdata) returns FP=F'(x);   (FP,FV, x) must be the argument list, even if FP does not need FV.   One reason for this is that the finite-difference Jacobian   does and that is the default in the solver.\n",
       "\n",
       "You may have a better way to add (1/dt) I to your Jacobian. If you   want to do this yourself then your Jacobian function should be   FP=J!(FP,FV,x,dt) or FP=J!(FP,FV,x,dt,pdata) and return   F'(x) + (1.0/dt)*I. \n",
       "\n",
       "You will also have to set the kwarg \\textbf{jknowsdt} to true.\n",
       "\n",
       "\n",
       "\\item Precision: Lemme tell ya 'bout precision. I designed this code for    full precision   functions and linear algebra in any precision you want. You can declare   FPS as Float64, Float32, or Float16 and ptcsol will do the right thing if   YOU do not destroy the declaration in your J! function. I'm amazed   that this works so easily. If the Jacobian is reasonably well   conditioned, you can cut the cost of Jacobian factorization and   storage in half with no loss. For large dense Jacobians and inexpensive   functions, this is a good deal.\n",
       "\n",
       "BUT ... There is very limited support for direct sparse solvers in   anything other than Float64. I recommend that you only use Float64   with direct sparse solvers unless you really know what you're doing. I   have a couple examples in the notebook, but watch out.\n",
       "\n",
       "\\end{itemize}\n",
       "\\rule{\\textwidth}{1pt}\n",
       "Keyword Arguments (kwargs):\n",
       "\n",
       "rtol and atol: relative and absolute error tolerances\n",
       "\n",
       "delta0: initial pseudo time step. The default value of 1.e-3 is a bit conservative and is one option you really should play with. Look at the example where I set it to 1.0!\n",
       "\n",
       "maxit: limit on nonlinear iterations, default=100. \n",
       "\n",
       "This is coupled to delta0. If your choice of delta0 is too small (conservative) then you'll need many iterations to converge and will need a larger value of maxit\n",
       "\n",
       "For PTC you'll need more iterations than for a straight-up nonlinear solve. This is part of the price for finding the  stable solution.\n",
       "\n",
       "dx: default = 1.e-7\n",
       "\n",
       "difference increment in finite-difference derivatives       h=dx*norm(x)+1.e-6\n",
       "\n",
       "pdata:\n",
       "\n",
       "precomputed data for the function/Jacobian.  Things will go better if you use this rather than hide the data  in global variables within the module for your function/Jacobian\n",
       "\n",
       "jfact: default = klfact (tries to figure out best choice) \n",
       "\n",
       "If your Jacobian has any special structure, please set jfact to the correct choice for a factorization.\n",
       "\n",
       "I use jfact when I call PTCUpdate to evaluate the Jacobian (using your J!) and factor it. The default is to use klfact (an internal function) to do something reasonable. For general matrices, klfact picks lu! to compute an LU factorization and share storage with the Jacobian.  You may change LU to something else by, for example, setting jfact = cholseky! if your Jacobian is spd.\n",
       "\n",
       "klfact knows about banded matrices and picks qr. You should, however RTFM, allocate the extra two upper bands, and use jfact=qr! to override klfact.\n",
       "\n",
       "If you give me something that klfact does not know how to dispatch on, then nothing happens. I just return the original Jacobian matrix and  ptcsol will use backslash to compute the Newton step.\n",
       "\n",
       "I know that this is probably not optimal in your situation, so it is  good to pick something else, like jfact = lu.\n",
       "\n",
       "printerr: default = true\n",
       "\n",
       "I print a helpful message when the solver fails. To suppress that message set printerr to false.\n",
       "\n",
       "keepsolhist: default = false\n",
       "\n",
       "Set this to true to get the history of the iteration in the output tuple. This is on by default for scalar equations and off for systems. Only turn it on if you have use for the data, which can get REALLY LARGE.\n",
       "\n",
       "jknowsdt: default = false\n",
       "\n",
       "Set this to true if your Jacobian evaluation function retursn F'(x) + (1/dt) I. You'll also need to follow the rules above for the Jacobian evaluation function. I do not recommend this and if your Jacobian is anything other than a matrix I can't promise anything. I've tested this for matrix outputs only.\n",
       "\n",
       "Output:\n",
       "\n",
       "A named tuple (solution, functionval, history, stats, idid,                errcode, solhist) where\n",
       "\n",
       "solution = converged result functionval = F(solution) history = the vector of residual norms (||F(x)||) for the iteration\n",
       "\n",
       "Unlike nsol, nsoli, or even ptcsoli, ptcsol has a fixed cost per  iteration of one function, one Jacobian, and one Factorization. Hence iteration statistics are not interesting and not in the output. \n",
       "\n",
       "idid=true if the iteration succeeded and false if not.\n",
       "\n",
       "errcode = 0 if if the iteration succeeded         = -1 if the initial iterate satisfies the termination criteria         = 10 if no convergence after maxit iterations\n",
       "\n",
       "solhist:\n",
       "\n",
       "This is the entire history of the iteration if you've set keepsolhist=true\n",
       "\n",
       "solhist is an N x K array where N is the length of x and K is the number of iteration + 1. So, for scalar equations, it's a row vector.\n",
       "\n",
       "\\subsection{Example from the docstrings for ptcsol}\n",
       "\\subsubsection{The buckling beam problem.}\n",
       "You'll need to use TestProblems for this to work.\n",
       "\n",
       "\\begin{verbatim}\n",
       "julia> using SIAMFANLEquations.TestProblems\n",
       "\n",
       "julia> n=63; maxit=1000; delta = 0.01; lambda = 20.0;\n",
       "\n",
       "julia> bdata = beaminit(n, 0.0, lambda);\n",
       "\n",
       "julia> x = bdata.x; u0 = x .* (1.0 .- x) .* (2.0 .- x); u0 .*= exp.(-10.0 * u0);\n",
       "\n",
       "\n",
       "julia> FS = copy(u0); FPS = copy(bdata.D2);\n",
       "\n",
       "julia> pout = ptcsol( FBeam!, u0, FS, FPS, BeamJ!; rtol = 1.e-10, pdata = bdata,\n",
       "                delta0 = delta, maxit = maxit);\n",
       "\n",
       "julia> # It takes a few iterations to get there.\n",
       "       length(pout.history)\n",
       "25\n",
       "\n",
       "julia> [pout.history[1:5] pout.history[21:25]]\n",
       "5×2 Array{Float64,2}:\n",
       " 6.31230e+01  9.75412e-01\n",
       " 7.52624e+00  8.35295e-02\n",
       " 8.31545e+00  6.58797e-04\n",
       " 3.15455e+01  4.12697e-08\n",
       " 3.66566e+01  6.75094e-12\n",
       "\n",
       "julia> # We get the nonnegative stedy state.\n",
       "       norm(pout.solution,Inf)\n",
       "2.19086e+00\n",
       "\\end{verbatim}\n"
      ],
      "text/markdown": [
       "ptcsol(F!, x0, FS, FPS, J! = diffjac!; rtol=1.e-6, atol=1.e-12,                maxit=20, delta0=1.e-6, dx=1.e-7, pdata = nothing, jfact = klfact,                printerr = true, keepsolhist = false, jknowsdt = false)\n",
       "\n",
       "C. T. Kelley, 2020\n",
       "\n",
       "Julia versions of the nonlinear solvers from my SIAM books.  Herewith: some new stuff ==> ptcsol\n",
       "\n",
       "PTC finds the steady-state solution of u' = -F(u), u(0) = u_0. The - sign is a convention.\n",
       "\n",
       "You must allocate storage for the function and Jacobian in advance –> in the calling program <– ie. in FS and FPS\n",
       "\n",
       "Inputs:\n",
       "\n",
       "  * F!: function evaluation, the ! indicates that F! overwrites FS, your   preallocated storage for the function.\n",
       "\n",
       "    So, FV=F!(FV,x) or FV=F!(FV,x,pdata) returns FV=F(x)\n",
       "  * x0: initial iterate\n",
       "\n",
       "  * FS: Preallocated storage for function. It is an N x 1 column vector.\n",
       "\n",
       "You may dimension it as (n,) or (n,1). (n,) is best, but the solvers can deal with it either way.\n",
       "\n",
       "  * FPS: preallocated storage for Jacobian. It is an N x N matrix\n",
       "\n",
       "    If FPS is sparse, you **must** allocate storage for the diagonal so I will have room to put 1/dt in there.\n",
       "  * J!: Jacobian evaluation, the ! indicates that J! overwrites FPS, your   preallocated storage for the Jacobian. If you leave this out the   default is a finite difference Jacobian.\n",
       "\n",
       "    So, FP=J!(FP,FV,x) or FP=J!(FP,FV,x,pdata) returns FP=F'(x);   (FP,FV, x) must be the argument list, even if FP does not need FV.   One reason for this is that the finite-difference Jacobian   does and that is the default in the solver.\n",
       "\n",
       "    You may have a better way to add (1/dt) I to your Jacobian. If you   want to do this yourself then your Jacobian function should be   FP=J!(FP,FV,x,dt) or FP=J!(FP,FV,x,dt,pdata) and return   F'(x) + (1.0/dt)*I. \n",
       "\n",
       "    You will also have to set the kwarg **jknowsdt** to true.\n",
       "  * Precision: Lemme tell ya 'bout precision. I designed this code for    full precision   functions and linear algebra in any precision you want. You can declare   FPS as Float64, Float32, or Float16 and ptcsol will do the right thing if   YOU do not destroy the declaration in your J! function. I'm amazed   that this works so easily. If the Jacobian is reasonably well   conditioned, you can cut the cost of Jacobian factorization and   storage in half with no loss. For large dense Jacobians and inexpensive   functions, this is a good deal.\n",
       "\n",
       "    BUT ... There is very limited support for direct sparse solvers in   anything other than Float64. I recommend that you only use Float64   with direct sparse solvers unless you really know what you're doing. I   have a couple examples in the notebook, but watch out.\n",
       "\n",
       "---\n",
       "\n",
       "Keyword Arguments (kwargs):\n",
       "\n",
       "rtol and atol: relative and absolute error tolerances\n",
       "\n",
       "delta0: initial pseudo time step. The default value of 1.e-3 is a bit conservative and is one option you really should play with. Look at the example where I set it to 1.0!\n",
       "\n",
       "maxit: limit on nonlinear iterations, default=100. \n",
       "\n",
       "This is coupled to delta0. If your choice of delta0 is too small (conservative) then you'll need many iterations to converge and will need a larger value of maxit\n",
       "\n",
       "For PTC you'll need more iterations than for a straight-up nonlinear solve. This is part of the price for finding the  stable solution.\n",
       "\n",
       "dx: default = 1.e-7\n",
       "\n",
       "difference increment in finite-difference derivatives       h=dx*norm(x)+1.e-6\n",
       "\n",
       "pdata:\n",
       "\n",
       "precomputed data for the function/Jacobian.  Things will go better if you use this rather than hide the data  in global variables within the module for your function/Jacobian\n",
       "\n",
       "jfact: default = klfact (tries to figure out best choice) \n",
       "\n",
       "If your Jacobian has any special structure, please set jfact to the correct choice for a factorization.\n",
       "\n",
       "I use jfact when I call PTCUpdate to evaluate the Jacobian (using your J!) and factor it. The default is to use klfact (an internal function) to do something reasonable. For general matrices, klfact picks lu! to compute an LU factorization and share storage with the Jacobian.  You may change LU to something else by, for example, setting jfact = cholseky! if your Jacobian is spd.\n",
       "\n",
       "klfact knows about banded matrices and picks qr. You should, however RTFM, allocate the extra two upper bands, and use jfact=qr! to override klfact.\n",
       "\n",
       "If you give me something that klfact does not know how to dispatch on, then nothing happens. I just return the original Jacobian matrix and  ptcsol will use backslash to compute the Newton step.\n",
       "\n",
       "I know that this is probably not optimal in your situation, so it is  good to pick something else, like jfact = lu.\n",
       "\n",
       "printerr: default = true\n",
       "\n",
       "I print a helpful message when the solver fails. To suppress that message set printerr to false.\n",
       "\n",
       "keepsolhist: default = false\n",
       "\n",
       "Set this to true to get the history of the iteration in the output tuple. This is on by default for scalar equations and off for systems. Only turn it on if you have use for the data, which can get REALLY LARGE.\n",
       "\n",
       "jknowsdt: default = false\n",
       "\n",
       "Set this to true if your Jacobian evaluation function retursn F'(x) + (1/dt) I. You'll also need to follow the rules above for the Jacobian evaluation function. I do not recommend this and if your Jacobian is anything other than a matrix I can't promise anything. I've tested this for matrix outputs only.\n",
       "\n",
       "Output:\n",
       "\n",
       "A named tuple (solution, functionval, history, stats, idid,                errcode, solhist) where\n",
       "\n",
       "solution = converged result functionval = F(solution) history = the vector of residual norms (||F(x)||) for the iteration\n",
       "\n",
       "Unlike nsol, nsoli, or even ptcsoli, ptcsol has a fixed cost per  iteration of one function, one Jacobian, and one Factorization. Hence iteration statistics are not interesting and not in the output. \n",
       "\n",
       "idid=true if the iteration succeeded and false if not.\n",
       "\n",
       "errcode = 0 if if the iteration succeeded         = -1 if the initial iterate satisfies the termination criteria         = 10 if no convergence after maxit iterations\n",
       "\n",
       "solhist:\n",
       "\n",
       "This is the entire history of the iteration if you've set keepsolhist=true\n",
       "\n",
       "solhist is an N x K array where N is the length of x and K is the number of iteration + 1. So, for scalar equations, it's a row vector.\n",
       "\n",
       "## Example from the docstrings for ptcsol\n",
       "\n",
       "### The buckling beam problem.\n",
       "\n",
       "You'll need to use TestProblems for this to work.\n",
       "\n",
       "```jldoctest\n",
       "julia> using SIAMFANLEquations.TestProblems\n",
       "\n",
       "julia> n=63; maxit=1000; delta = 0.01; lambda = 20.0;\n",
       "\n",
       "julia> bdata = beaminit(n, 0.0, lambda);\n",
       "\n",
       "julia> x = bdata.x; u0 = x .* (1.0 .- x) .* (2.0 .- x); u0 .*= exp.(-10.0 * u0);\n",
       "\n",
       "\n",
       "julia> FS = copy(u0); FPS = copy(bdata.D2);\n",
       "\n",
       "julia> pout = ptcsol( FBeam!, u0, FS, FPS, BeamJ!; rtol = 1.e-10, pdata = bdata,\n",
       "                delta0 = delta, maxit = maxit);\n",
       "\n",
       "julia> # It takes a few iterations to get there.\n",
       "       length(pout.history)\n",
       "25\n",
       "\n",
       "julia> [pout.history[1:5] pout.history[21:25]]\n",
       "5×2 Array{Float64,2}:\n",
       " 6.31230e+01  9.75412e-01\n",
       " 7.52624e+00  8.35295e-02\n",
       " 8.31545e+00  6.58797e-04\n",
       " 3.15455e+01  4.12697e-08\n",
       " 3.66566e+01  6.75094e-12\n",
       "\n",
       "julia> # We get the nonnegative stedy state.\n",
       "       norm(pout.solution,Inf)\n",
       "2.19086e+00\n",
       "```\n"
      ],
      "text/plain": [
       "  ptcsol(F!, x0, FS, FPS, J! = diffjac!; rtol=1.e-6, atol=1.e-12, maxit=20,\n",
       "  delta0=1.e-6, dx=1.e-7, pdata = nothing, jfact = klfact, printerr = true,\n",
       "  keepsolhist = false, jknowsdt = false)\n",
       "\n",
       "  C. T. Kelley, 2020\n",
       "\n",
       "  Julia versions of the nonlinear solvers from my SIAM books. Herewith: some\n",
       "  new stuff ==> ptcsol\n",
       "\n",
       "  PTC finds the steady-state solution of u' = -F(u), u(0) = u_0. The - sign is\n",
       "  a convention.\n",
       "\n",
       "  You must allocate storage for the function and Jacobian in advance –> in the\n",
       "  calling program <– ie. in FS and FPS\n",
       "\n",
       "  Inputs:\n",
       "\n",
       "    •  F!: function evaluation, the ! indicates that F! overwrites FS,\n",
       "       your preallocated storage for the function.\n",
       "       So, FV=F!(FV,x) or FV=F!(FV,x,pdata) returns FV=F(x)\n",
       "\n",
       "    •  x0: initial iterate\n",
       "\n",
       "    •  FS: Preallocated storage for function. It is an N x 1 column\n",
       "       vector.\n",
       "\n",
       "  You may dimension it as (n,) or (n,1). (n,) is best, but the solvers can\n",
       "  deal with it either way.\n",
       "\n",
       "    •  FPS: preallocated storage for Jacobian. It is an N x N matrix\n",
       "       If FPS is sparse, you \u001b[1mmust\u001b[22m allocate storage for the diagonal so I\n",
       "       will have room to put 1/dt in there.\n",
       "\n",
       "    •  J!: Jacobian evaluation, the ! indicates that J! overwrites FPS,\n",
       "       your preallocated storage for the Jacobian. If you leave this out\n",
       "       the default is a finite difference Jacobian.\n",
       "       So, FP=J!(FP,FV,x) or FP=J!(FP,FV,x,pdata) returns FP=F'(x);\n",
       "       (FP,FV, x) must be the argument list, even if FP does not need FV.\n",
       "       One reason for this is that the finite-difference Jacobian does\n",
       "       and that is the default in the solver.\n",
       "       You may have a better way to add (1/dt) I to your Jacobian. If you\n",
       "       want to do this yourself then your Jacobian function should be\n",
       "       FP=J!(FP,FV,x,dt) or FP=J!(FP,FV,x,dt,pdata) and return F'(x) +\n",
       "       (1.0/dt)*I.\n",
       "       You will also have to set the kwarg \u001b[1mjknowsdt\u001b[22m to true.\n",
       "\n",
       "    •  Precision: Lemme tell ya 'bout precision. I designed this code for\n",
       "       full precision functions and linear algebra in any precision you\n",
       "       want. You can declare FPS as Float64, Float32, or Float16 and\n",
       "       ptcsol will do the right thing if YOU do not destroy the\n",
       "       declaration in your J! function. I'm amazed that this works so\n",
       "       easily. If the Jacobian is reasonably well conditioned, you can\n",
       "       cut the cost of Jacobian factorization and storage in half with no\n",
       "       loss. For large dense Jacobians and inexpensive functions, this is\n",
       "       a good deal.\n",
       "       BUT ... There is very limited support for direct sparse solvers in\n",
       "       anything other than Float64. I recommend that you only use Float64\n",
       "       with direct sparse solvers unless you really know what you're\n",
       "       doing. I have a couple examples in the notebook, but watch out.\n",
       "\n",
       "  ────────────────────────────────────────────────────────────────────────────\n",
       "\n",
       "  Keyword Arguments (kwargs):\n",
       "\n",
       "  rtol and atol: relative and absolute error tolerances\n",
       "\n",
       "  delta0: initial pseudo time step. The default value of 1.e-3 is a bit\n",
       "  conservative and is one option you really should play with. Look at the\n",
       "  example where I set it to 1.0!\n",
       "\n",
       "  maxit: limit on nonlinear iterations, default=100.\n",
       "\n",
       "  This is coupled to delta0. If your choice of delta0 is too small\n",
       "  (conservative) then you'll need many iterations to converge and will need a\n",
       "  larger value of maxit\n",
       "\n",
       "  For PTC you'll need more iterations than for a straight-up nonlinear solve.\n",
       "  This is part of the price for finding the stable solution.\n",
       "\n",
       "  dx: default = 1.e-7\n",
       "\n",
       "  difference increment in finite-difference derivatives h=dx*norm(x)+1.e-6\n",
       "\n",
       "  pdata:\n",
       "\n",
       "  precomputed data for the function/Jacobian. Things will go better if you use\n",
       "  this rather than hide the data in global variables within the module for\n",
       "  your function/Jacobian\n",
       "\n",
       "  jfact: default = klfact (tries to figure out best choice)\n",
       "\n",
       "  If your Jacobian has any special structure, please set jfact to the correct\n",
       "  choice for a factorization.\n",
       "\n",
       "  I use jfact when I call PTCUpdate to evaluate the Jacobian (using your J!)\n",
       "  and factor it. The default is to use klfact (an internal function) to do\n",
       "  something reasonable. For general matrices, klfact picks lu! to compute an\n",
       "  LU factorization and share storage with the Jacobian. You may change LU to\n",
       "  something else by, for example, setting jfact = cholseky! if your Jacobian\n",
       "  is spd.\n",
       "\n",
       "  klfact knows about banded matrices and picks qr. You should, however RTFM,\n",
       "  allocate the extra two upper bands, and use jfact=qr! to override klfact.\n",
       "\n",
       "  If you give me something that klfact does not know how to dispatch on, then\n",
       "  nothing happens. I just return the original Jacobian matrix and ptcsol will\n",
       "  use backslash to compute the Newton step.\n",
       "\n",
       "  I know that this is probably not optimal in your situation, so it is good to\n",
       "  pick something else, like jfact = lu.\n",
       "\n",
       "  printerr: default = true\n",
       "\n",
       "  I print a helpful message when the solver fails. To suppress that message\n",
       "  set printerr to false.\n",
       "\n",
       "  keepsolhist: default = false\n",
       "\n",
       "  Set this to true to get the history of the iteration in the output tuple.\n",
       "  This is on by default for scalar equations and off for systems. Only turn it\n",
       "  on if you have use for the data, which can get REALLY LARGE.\n",
       "\n",
       "  jknowsdt: default = false\n",
       "\n",
       "  Set this to true if your Jacobian evaluation function retursn F'(x) + (1/dt)\n",
       "  I. You'll also need to follow the rules above for the Jacobian evaluation\n",
       "  function. I do not recommend this and if your Jacobian is anything other\n",
       "  than a matrix I can't promise anything. I've tested this for matrix outputs\n",
       "  only.\n",
       "\n",
       "  Output:\n",
       "\n",
       "  A named tuple (solution, functionval, history, stats, idid, errcode,\n",
       "  solhist) where\n",
       "\n",
       "  solution = converged result functionval = F(solution) history = the vector\n",
       "  of residual norms (||F(x)||) for the iteration\n",
       "\n",
       "  Unlike nsol, nsoli, or even ptcsoli, ptcsol has a fixed cost per iteration\n",
       "  of one function, one Jacobian, and one Factorization. Hence iteration\n",
       "  statistics are not interesting and not in the output.\n",
       "\n",
       "  idid=true if the iteration succeeded and false if not.\n",
       "\n",
       "  errcode = 0 if if the iteration succeeded = -1 if the initial iterate\n",
       "  satisfies the termination criteria = 10 if no convergence after maxit\n",
       "  iterations\n",
       "\n",
       "  solhist:\n",
       "\n",
       "  This is the entire history of the iteration if you've set keepsolhist=true\n",
       "\n",
       "  solhist is an N x K array where N is the length of x and K is the number of\n",
       "  iteration + 1. So, for scalar equations, it's a row vector.\n",
       "\n",
       "\u001b[1m  Example from the docstrings for ptcsol\u001b[22m\n",
       "\u001b[1m  ========================================\u001b[22m\n",
       "\n",
       "\u001b[1m  The buckling beam problem.\u001b[22m\n",
       "\u001b[1m  ––––––––––––––––––––––––––––\u001b[22m\n",
       "\n",
       "  You'll need to use TestProblems for this to work.\n",
       "\n",
       "\u001b[36m  julia> using SIAMFANLEquations.TestProblems\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> n=63; maxit=1000; delta = 0.01; lambda = 20.0;\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> bdata = beaminit(n, 0.0, lambda);\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> x = bdata.x; u0 = x .* (1.0 .- x) .* (2.0 .- x); u0 .*= exp.(-10.0 * u0);\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> FS = copy(u0); FPS = copy(bdata.D2);\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> pout = ptcsol( FBeam!, u0, FS, FPS, BeamJ!; rtol = 1.e-10, pdata = bdata,\u001b[39m\n",
       "\u001b[36m                  delta0 = delta, maxit = maxit);\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> # It takes a few iterations to get there.\u001b[39m\n",
       "\u001b[36m         length(pout.history)\u001b[39m\n",
       "\u001b[36m  25\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> [pout.history[1:5] pout.history[21:25]]\u001b[39m\n",
       "\u001b[36m  5×2 Array{Float64,2}:\u001b[39m\n",
       "\u001b[36m   6.31230e+01  9.75412e-01\u001b[39m\n",
       "\u001b[36m   7.52624e+00  8.35295e-02\u001b[39m\n",
       "\u001b[36m   8.31545e+00  6.58797e-04\u001b[39m\n",
       "\u001b[36m   3.15455e+01  4.12697e-08\u001b[39m\n",
       "\u001b[36m   3.66566e+01  6.75094e-12\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> # We get the nonnegative stedy state.\u001b[39m\n",
       "\u001b[36m         norm(pout.solution,Inf)\u001b[39m\n",
       "\u001b[36m  2.19086e+00\u001b[39m"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?ptcsol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.0",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
