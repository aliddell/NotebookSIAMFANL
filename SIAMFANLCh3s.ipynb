{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\newcommand{\\calf}{{\\cal F}}\n",
    "\\newcommand{\\dnu}{d \\nu}\n",
    "\\newcommand{\\mf}{{\\bf F}}\n",
    "\\newcommand{\\md}{{\\bf D}}\n",
    "\\newcommand{\\mP}{{\\bf P}}\n",
    "\\newcommand{\\mU}{{\\bf U}}\n",
    "\\newcommand{\\vu}{{\\bf u}}\n",
    "\\newcommand{\\vx}{{\\bf x}}\n",
    "\\newcommand{\\vw}{{\\bf w}}\n",
    "\\newcommand{\\vy}{{\\bf y}}\n",
    "\\newcommand{\\vf}{{\\bf f}}\n",
    "\\newcommand{\\vs}{{\\bf s}}\n",
    "\\newcommand{\\ve}{{\\bf e}}\n",
    "\\newcommand{\\vd}{{\\bf d}}\n",
    "\\newcommand{\\vb}{{\\bf b}}\n",
    "\\newcommand{\\vz}{{\\bf z}}\n",
    "\\newcommand{\\mg}{{\\bf G}}\n",
    "\\newcommand{\\ml}{{\\bf L}}\n",
    "\\newcommand{\\mg}{{\\bf G}}\n",
    "\\newcommand{\\mv}{{\\bf V}}\n",
    "\\newcommand{\\ma}{{\\bf A}}\n",
    "\\newcommand{\\mi}{{\\bf I}}\n",
    "\\newcommand{\\mm}{{\\bf M}}\n",
    "\\newcommand{\\mb}{{\\bf B}}\n",
    "\\newcommand{\\ball}{{\\cal B}}\n",
    "\\newcommand{\\ptc}{{\\Psi TC}}\n",
    "\\newcommand{\\diag}{\\mbox{diag}}\n",
    "\\newcommand{\\begeq}{{\\begin{equation}}}\n",
    "\\newcommand{\\endeq}{{\\end{equation}}}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"fanote_init.jl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3.X Solvers for Chapter 3\n",
    "\n",
    "Contents for Section 3.X\n",
    "\n",
    "[Overview](#Overview)\n",
    "\n",
    "[nsoli.jl](#nsoli.jl)\n",
    "\n",
    "- [Benchmarking the H-equation with nsoli.jl](#Benchmarking-the-H-equation-with-nsoli.jl)\n",
    "\n",
    "- [ Preconditioning the Convection-Diffusion Equation](#Preconditioning-the-Convection-Diffusion-Equation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will follow the pattern of the previous chapters and present two solvers, a Newton code and a $\\ptc$ code. Both codes are for systems of equations and use Krylov methods to compute the step. We have two Krylov solvers, GMRES and BiCGstab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nsoli.jl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__nsoli.jl__ solves systems of nonlinear equations with Newton-Krlov methods. As usual, we begin with the docstrings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search: \u001b[0m\u001b[1mn\u001b[22m\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1ml\u001b[22m\u001b[0m\u001b[1mi\u001b[22m \u001b[0m\u001b[1mN\u001b[22m\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1ml\u001b[22m\u001b[0m\u001b[1mi\u001b[22mPDE \u001b[0m\u001b[1mn\u001b[22m\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1ml\u001b[22m \u001b[0m\u001b[1mn\u001b[22m\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1ml\u001b[22msc \u001b[0m\u001b[1mn\u001b[22m\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1ml\u001b[22mheq \u001b[0m\u001b[1mN\u001b[22m\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1ml\u001b[22mPDE\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "nsoli(F!, x0, FS, FPS, Jvec=dirder; rtol=1.e-6, atol=1.e-12,\n",
       "           maxit=20, lmaxit=-1, lsolver=\"gmres\", eta=.1,\n",
       "           fixedeta=true, Pvec=nothing, pside=\"left\",\n",
       "           armmax=10, dx = 1.e-7, armfix=false, pdata = nothing,\n",
       "           printerr = true, keepsolhist = false, stagnationok=false)\n",
       "\\end{verbatim}\n",
       ")\n",
       "\n",
       "C. T. Kelley, 2021\n",
       "\n",
       "Julia versions of the nonlinear solvers from my SIAM books.  Herewith: nsoli\n",
       "\n",
       "You must allocate storage for the function and the Krylov basis in advance –> in the calling program <– ie. in FS and FPS\n",
       "\n",
       "Inputs:\n",
       "\n",
       "\\begin{itemize}\n",
       "\\item F!: function evaluation, the ! indicates that F! overwrites FS, your   preallocated storage for the function.\n",
       "\n",
       "So FS=F!(FS,x) or FS=F!(FS,x,pdata) returns FS=F(x)\n",
       "\n",
       "\\end{itemize}\n",
       "\\begin{itemize}\n",
       "\\item x0: initial iterate\n",
       "\n",
       "\\end{itemize}\n",
       "\\begin{itemize}\n",
       "\\item FS: Preallocated storage for function. It is an N x 1 column vector\n",
       "\n",
       "\\end{itemize}\n",
       "\\begin{itemize}\n",
       "\\item FPS: preallocated storage for the Krylov basis. It is an N x m matrix where      you plan to take at most m-1 GMRES iterations before a restart. \n",
       "\n",
       "\\end{itemize}\n",
       "\\begin{itemize}\n",
       "\\item Jvec: Jacobian vector product, If you leave this out the   default is a finite difference directional derivative.\n",
       "\n",
       "So, FP=Jvec(v,FS,x) or FP=Jvec(v,FS,x,pdata) returns FP=F'(x) v. \n",
       "\n",
       "(v, FS, x) or (v, FS, x, pdata) must be the argument list,    even if FP does not need FS.   One reason for this is that the finite-difference derivative   does and that is the default in the solver.\n",
       "\n",
       "\n",
       "\\item Precision: Lemme tell ya 'bout precision. I designed this code for    full precision functions and linear algebra in any precision you want.    You can declare FPS as Float64 or Float32 and nsoli    will do the right thing. Float16 support is there, but not working well.\n",
       "\n",
       "If the Jacobian is reasonably well conditioned, you can cut the cost   of orthogonalization and storage (for GMRES) in half with no loss.    There is no benefit if your linear solver is not GMRES or if    othogonalization and storage of the Krylov vectors is only a   small part of the cost of the computation. So if your preconditioner   is good and you only need a few Krylovs/Newton, reduced precision won't   help you much.\n",
       "\n",
       "\\end{itemize}\n",
       "\\rule{\\textwidth}{1pt}\n",
       "Keyword Arguments (kwargs):\n",
       "\n",
       "rtol and atol: relative and absolute error tolerances\n",
       "\n",
       "maxit: limit on nonlinear iterations\n",
       "\n",
       "lmaxit: limit on linear iterations. If lmaxit > m-1, where FPS has m columns, and you need more than m-1 linear iterations, then GMRES  will restart. \n",
       "\n",
       "The default is -1. This means that you'll take m-1 iterations, where size(V) = (n,m), and get no restarts. –> Restarted GMRES is not ready yet.\n",
       "\n",
       "lsolver: the linear solver, default = \"gmres\"\n",
       "\n",
       "Your choices will be \"gmres\" or \"bicgstab\". However, gmres is the only option for now.\n",
       "\n",
       "eta and fixed eta: eta > 0 or there's an error\n",
       "\n",
       "The linear solver terminates when ||F'(x)s + F(x) || <= etag || F(x) ||\n",
       "\n",
       "where \n",
       "\n",
       "etag = eta if fixedeta=true\n",
       "\n",
       "etag = Eisenstat-Walker as implemented in book if fixedeta=false\n",
       "\n",
       "The default, which may change, is eta=.1, fixedeta=true\n",
       "\n",
       "Pvec: Preconditioner-vector product. The rules are similar to Jvec     So, Pv=Pvec(v,x) or Pv=Pvec(v,x,pdata) returns P(x) v where     P(x) is the preconditioner. You must use x as an input even     if your preconditioner does not depend on x\n",
       "\n",
       "armmax: upper bound on step size reductions in line search\n",
       "\n",
       "dx: default = 1.e-7\n",
       "\n",
       "difference increment in finite-difference derivatives       h=dx*norm(x,Inf)+1.e-8\n",
       "\n",
       "armfix: default = false\n",
       "\n",
       "The default is a parabolic line search (ie false). Set to true and the step size will be fixed at .5. Don't do this unless you are doing experiments for research.\n",
       "\n",
       "pdata:\n",
       "\n",
       "precomputed data for the function/Jacobian-vector/Preconditioner-vector products.  Things will go better if you use this rather than hide the data  in global variables within the module for your function/Jacobian\n",
       "\n",
       "If you use pdata in any of F!, Jvec, or Pvec, you must use in in all of them.\n",
       "\n",
       "printerr: default = true\n",
       "\n",
       "I print a helpful message when the solver fails. To suppress that message set printerr to false.\n",
       "\n",
       "keepsolhist: default = false\n",
       "\n",
       "Set this to true to get the history of the iteration in the output tuple. This is on by default for scalar equations and off for systems. Only turn it on if you have use for the data, which can get REALLY LARGE.\n",
       "\n",
       "stagnationok: default = false\n",
       "\n",
       "Set this to true if you want to disable the line search and either observe divergence or stagnation. This is only useful for research or writing a book.\n",
       "\n",
       "Output:\n",
       "\n",
       "\\begin{itemize}\n",
       "\\item A named tuple (solution, functionval, history, stats, idid,              errcode, solhist)\n",
       "\n",
       "\\end{itemize}\n",
       "where\n",
       "\n",
       "– solution = converged result\n",
       "\n",
       "– functionval = F(solution)\n",
       "\n",
       "– history = the vector of residual norms (||F(x)||) for the iteration\n",
       "\n",
       "– stats = named tuple of the history of (ifun, ijvec, iarm, ikfail), the  number of functions/Jacobian-vector prods/steplength reductions/linear solver failures at each iteration. Linear solver failures DO NOT mean that the nonlinear solver will fail. You should look at this stat if, for example, the line search fails. Increasing the size of FPS and/or lmaxit might solve the problem.\n",
       "\n",
       "I do not count the function values for a finite-difference derivative because they count toward a Jacobian-vector product.\n",
       "\n",
       "– idid=true if the iteration succeeded and false if not.\n",
       "\n",
       "– errcode = 0 if if the iteration succeeded\n",
       "\n",
       "\\begin{verbatim}\n",
       "    = -1 if the initial iterate satisfies the termination criteria\n",
       "\n",
       "    = 10 if no convergence after maxit iterations\n",
       "\n",
       "    = 1  if the line search failed\n",
       "\\end{verbatim}\n",
       "– solhist:\n",
       "\n",
       "\\begin{verbatim}\n",
       "  This is the entire history of the iteration if you've set\n",
       "  keepsolhist=true\n",
       "\\end{verbatim}\n",
       "solhist is an N x K array where N is the length of x and K is the number of iteration + 1. So, for scalar equations, it's a row vector.\n",
       "\n",
       "\\rule{\\textwidth}{1pt}\n",
       "\\section{Examples}\n",
       "\\paragraph{Simple 2D problem. You should get the same results as for nsol.jl because}\n",
       "GMRES will solve the equation for the step exactly in two iterations. Finite difference Jacobians and analytic Jacobian-vector products for full precision and finite difference Jacobian-vector products for single precision.\n",
       "\n",
       "\\begin{verbatim}\n",
       "julia> function f!(fv,x)\n",
       "       fv[1]=x[1] + sin(x[2])\n",
       "       fv[2]=cos(x[1]+x[2])\n",
       "       end\n",
       "f! (generic function with 1 method)\n",
       "\n",
       "julia> function JVec(v, fv, x)\n",
       "       jvec=zeros(2,);\n",
       "       p=-sin(x[1]+x[2])\n",
       "       jvec[1]=v[1]+cos(x[2])*v[2]\n",
       "       jvec[2]=p*(v[1]+v[2])\n",
       "       return jvec\n",
       "       end\n",
       "JVec (generic function with 1 method)\n",
       "\n",
       "julia> x0=ones(2,); fv=zeros(2,); jv=zeros(2,2); jv32=zeros(Float32,2,2);\n",
       "\n",
       "julia> jvs=zeros(2,3); jvs32=zeros(Float32,2,3);\n",
       "\n",
       "julia> nout=nsol(f!,x0,fv,jv; sham=1);\n",
       "\n",
       "julia> kout=nsoli(f!,x0,fv,jvs,JVec; fixedeta=true, eta=.1, lmaxit=2);\n",
       "\n",
       "julia> kout32=nsoli(f!,x0,fv,jvs32; fixedeta=true, eta=.1, lmaxit=2);\n",
       "\n",
       "julia> [nout.history kout.history kout32.history]\n",
       "5×3 Array{Float64,2}:\n",
       " 1.88791e+00  1.88791e+00  1.88791e+00\n",
       " 2.43119e-01  2.43120e-01  2.43119e-01\n",
       " 1.19231e-02  1.19231e-02  1.19231e-02\n",
       " 1.03266e-05  1.03261e-05  1.03273e-05\n",
       " 1.46416e-11  1.40862e-11  1.45457e-11\n",
       "\\end{verbatim}\n"
      ],
      "text/markdown": [
       "```\n",
       "nsoli(F!, x0, FS, FPS, Jvec=dirder; rtol=1.e-6, atol=1.e-12,\n",
       "           maxit=20, lmaxit=-1, lsolver=\"gmres\", eta=.1,\n",
       "           fixedeta=true, Pvec=nothing, pside=\"left\",\n",
       "           armmax=10, dx = 1.e-7, armfix=false, pdata = nothing,\n",
       "           printerr = true, keepsolhist = false, stagnationok=false)\n",
       "```\n",
       "\n",
       ")\n",
       "\n",
       "C. T. Kelley, 2021\n",
       "\n",
       "Julia versions of the nonlinear solvers from my SIAM books.  Herewith: nsoli\n",
       "\n",
       "You must allocate storage for the function and the Krylov basis in advance –> in the calling program <– ie. in FS and FPS\n",
       "\n",
       "Inputs:\n",
       "\n",
       "  * F!: function evaluation, the ! indicates that F! overwrites FS, your   preallocated storage for the function.\n",
       "\n",
       "    So FS=F!(FS,x) or FS=F!(FS,x,pdata) returns FS=F(x)\n",
       "\n",
       "  * x0: initial iterate\n",
       "\n",
       "  * FS: Preallocated storage for function. It is an N x 1 column vector\n",
       "\n",
       "  * FPS: preallocated storage for the Krylov basis. It is an N x m matrix where      you plan to take at most m-1 GMRES iterations before a restart.\n",
       "\n",
       "  * Jvec: Jacobian vector product, If you leave this out the   default is a finite difference directional derivative.\n",
       "\n",
       "    So, FP=Jvec(v,FS,x) or FP=Jvec(v,FS,x,pdata) returns FP=F'(x) v. \n",
       "\n",
       "    (v, FS, x) or (v, FS, x, pdata) must be the argument list,    even if FP does not need FS.   One reason for this is that the finite-difference derivative   does and that is the default in the solver.\n",
       "  * Precision: Lemme tell ya 'bout precision. I designed this code for    full precision functions and linear algebra in any precision you want.    You can declare FPS as Float64 or Float32 and nsoli    will do the right thing. Float16 support is there, but not working well.\n",
       "\n",
       "    If the Jacobian is reasonably well conditioned, you can cut the cost   of orthogonalization and storage (for GMRES) in half with no loss.    There is no benefit if your linear solver is not GMRES or if    othogonalization and storage of the Krylov vectors is only a   small part of the cost of the computation. So if your preconditioner   is good and you only need a few Krylovs/Newton, reduced precision won't   help you much.\n",
       "\n",
       "---\n",
       "\n",
       "Keyword Arguments (kwargs):\n",
       "\n",
       "rtol and atol: relative and absolute error tolerances\n",
       "\n",
       "maxit: limit on nonlinear iterations\n",
       "\n",
       "lmaxit: limit on linear iterations. If lmaxit > m-1, where FPS has m columns, and you need more than m-1 linear iterations, then GMRES  will restart. \n",
       "\n",
       "The default is -1. This means that you'll take m-1 iterations, where size(V) = (n,m), and get no restarts. –> Restarted GMRES is not ready yet.\n",
       "\n",
       "lsolver: the linear solver, default = \"gmres\"\n",
       "\n",
       "Your choices will be \"gmres\" or \"bicgstab\". However, gmres is the only option for now.\n",
       "\n",
       "eta and fixed eta: eta > 0 or there's an error\n",
       "\n",
       "The linear solver terminates when ||F'(x)s + F(x) || <= etag || F(x) ||\n",
       "\n",
       "where \n",
       "\n",
       "etag = eta if fixedeta=true\n",
       "\n",
       "etag = Eisenstat-Walker as implemented in book if fixedeta=false\n",
       "\n",
       "The default, which may change, is eta=.1, fixedeta=true\n",
       "\n",
       "Pvec: Preconditioner-vector product. The rules are similar to Jvec     So, Pv=Pvec(v,x) or Pv=Pvec(v,x,pdata) returns P(x) v where     P(x) is the preconditioner. You must use x as an input even     if your preconditioner does not depend on x\n",
       "\n",
       "armmax: upper bound on step size reductions in line search\n",
       "\n",
       "dx: default = 1.e-7\n",
       "\n",
       "difference increment in finite-difference derivatives       h=dx*norm(x,Inf)+1.e-8\n",
       "\n",
       "armfix: default = false\n",
       "\n",
       "The default is a parabolic line search (ie false). Set to true and the step size will be fixed at .5. Don't do this unless you are doing experiments for research.\n",
       "\n",
       "pdata:\n",
       "\n",
       "precomputed data for the function/Jacobian-vector/Preconditioner-vector products.  Things will go better if you use this rather than hide the data  in global variables within the module for your function/Jacobian\n",
       "\n",
       "If you use pdata in any of F!, Jvec, or Pvec, you must use in in all of them.\n",
       "\n",
       "printerr: default = true\n",
       "\n",
       "I print a helpful message when the solver fails. To suppress that message set printerr to false.\n",
       "\n",
       "keepsolhist: default = false\n",
       "\n",
       "Set this to true to get the history of the iteration in the output tuple. This is on by default for scalar equations and off for systems. Only turn it on if you have use for the data, which can get REALLY LARGE.\n",
       "\n",
       "stagnationok: default = false\n",
       "\n",
       "Set this to true if you want to disable the line search and either observe divergence or stagnation. This is only useful for research or writing a book.\n",
       "\n",
       "Output:\n",
       "\n",
       "  * A named tuple (solution, functionval, history, stats, idid,              errcode, solhist)\n",
       "\n",
       "where\n",
       "\n",
       "– solution = converged result\n",
       "\n",
       "– functionval = F(solution)\n",
       "\n",
       "– history = the vector of residual norms (||F(x)||) for the iteration\n",
       "\n",
       "– stats = named tuple of the history of (ifun, ijvec, iarm, ikfail), the  number of functions/Jacobian-vector prods/steplength reductions/linear solver failures at each iteration. Linear solver failures DO NOT mean that the nonlinear solver will fail. You should look at this stat if, for example, the line search fails. Increasing the size of FPS and/or lmaxit might solve the problem.\n",
       "\n",
       "I do not count the function values for a finite-difference derivative because they count toward a Jacobian-vector product.\n",
       "\n",
       "– idid=true if the iteration succeeded and false if not.\n",
       "\n",
       "– errcode = 0 if if the iteration succeeded\n",
       "\n",
       "```\n",
       "    = -1 if the initial iterate satisfies the termination criteria\n",
       "\n",
       "    = 10 if no convergence after maxit iterations\n",
       "\n",
       "    = 1  if the line search failed\n",
       "```\n",
       "\n",
       "– solhist:\n",
       "\n",
       "```\n",
       "  This is the entire history of the iteration if you've set\n",
       "  keepsolhist=true\n",
       "```\n",
       "\n",
       "solhist is an N x K array where N is the length of x and K is the number of iteration + 1. So, for scalar equations, it's a row vector.\n",
       "\n",
       "---\n",
       "\n",
       "# Examples\n",
       "\n",
       "#### Simple 2D problem. You should get the same results as for nsol.jl because\n",
       "\n",
       "GMRES will solve the equation for the step exactly in two iterations. Finite difference Jacobians and analytic Jacobian-vector products for full precision and finite difference Jacobian-vector products for single precision.\n",
       "\n",
       "```jldoctest\n",
       "julia> function f!(fv,x)\n",
       "       fv[1]=x[1] + sin(x[2])\n",
       "       fv[2]=cos(x[1]+x[2])\n",
       "       end\n",
       "f! (generic function with 1 method)\n",
       "\n",
       "julia> function JVec(v, fv, x)\n",
       "       jvec=zeros(2,);\n",
       "       p=-sin(x[1]+x[2])\n",
       "       jvec[1]=v[1]+cos(x[2])*v[2]\n",
       "       jvec[2]=p*(v[1]+v[2])\n",
       "       return jvec\n",
       "       end\n",
       "JVec (generic function with 1 method)\n",
       "\n",
       "julia> x0=ones(2,); fv=zeros(2,); jv=zeros(2,2); jv32=zeros(Float32,2,2);\n",
       "\n",
       "julia> jvs=zeros(2,3); jvs32=zeros(Float32,2,3);\n",
       "\n",
       "julia> nout=nsol(f!,x0,fv,jv; sham=1);\n",
       "\n",
       "julia> kout=nsoli(f!,x0,fv,jvs,JVec; fixedeta=true, eta=.1, lmaxit=2);\n",
       "\n",
       "julia> kout32=nsoli(f!,x0,fv,jvs32; fixedeta=true, eta=.1, lmaxit=2);\n",
       "\n",
       "julia> [nout.history kout.history kout32.history]\n",
       "5×3 Array{Float64,2}:\n",
       " 1.88791e+00  1.88791e+00  1.88791e+00\n",
       " 2.43119e-01  2.43120e-01  2.43119e-01\n",
       " 1.19231e-02  1.19231e-02  1.19231e-02\n",
       " 1.03266e-05  1.03261e-05  1.03273e-05\n",
       " 1.46416e-11  1.40862e-11  1.45457e-11\n",
       "```\n"
      ],
      "text/plain": [
       "\u001b[36m  nsoli(F!, x0, FS, FPS, Jvec=dirder; rtol=1.e-6, atol=1.e-12,\u001b[39m\n",
       "\u001b[36m             maxit=20, lmaxit=-1, lsolver=\"gmres\", eta=.1,\u001b[39m\n",
       "\u001b[36m             fixedeta=true, Pvec=nothing, pside=\"left\",\u001b[39m\n",
       "\u001b[36m             armmax=10, dx = 1.e-7, armfix=false, pdata = nothing,\u001b[39m\n",
       "\u001b[36m             printerr = true, keepsolhist = false, stagnationok=false)\u001b[39m\n",
       "\n",
       "  )\n",
       "\n",
       "  C. T. Kelley, 2021\n",
       "\n",
       "  Julia versions of the nonlinear solvers from my SIAM books. Herewith: nsoli\n",
       "\n",
       "  You must allocate storage for the function and the Krylov basis in advance\n",
       "  –> in the calling program <– ie. in FS and FPS\n",
       "\n",
       "  Inputs:\n",
       "\n",
       "    •    F!: function evaluation, the ! indicates that F! overwrites FS,\n",
       "        your preallocated storage for the function.\n",
       "      \n",
       "        So FS=F!(FS,x) or FS=F!(FS,x,pdata) returns FS=F(x)\n",
       "\n",
       "    •    x0: initial iterate\n",
       "\n",
       "    •    FS: Preallocated storage for function. It is an N x 1 column\n",
       "        vector\n",
       "\n",
       "    •    FPS: preallocated storage for the Krylov basis. It is an N x m\n",
       "        matrix where you plan to take at most m-1 GMRES iterations before\n",
       "        a restart. \n",
       "\n",
       "    •    Jvec: Jacobian vector product, If you leave this out the default\n",
       "        is a finite difference directional derivative.\n",
       "      \n",
       "        So, FP=Jvec(v,FS,x) or FP=Jvec(v,FS,x,pdata) returns FP=F'(x) v. \n",
       "      \n",
       "        (v, FS, x) or (v, FS, x, pdata) must be the argument list, even if\n",
       "        FP does not need FS. One reason for this is that the\n",
       "        finite-difference derivative does and that is the default in the\n",
       "        solver.\n",
       "\n",
       "    •    Precision: Lemme tell ya 'bout precision. I designed this code for\n",
       "        full precision functions and linear algebra in any precision you\n",
       "        want. You can declare FPS as Float64 or Float32 and nsoli will do\n",
       "        the right thing. Float16 support is there, but not working well.\n",
       "      \n",
       "        If the Jacobian is reasonably well conditioned, you can cut the\n",
       "        cost of orthogonalization and storage (for GMRES) in half with no\n",
       "        loss. There is no benefit if your linear solver is not GMRES or if\n",
       "        othogonalization and storage of the Krylov vectors is only a small\n",
       "        part of the cost of the computation. So if your preconditioner is\n",
       "        good and you only need a few Krylovs/Newton, reduced precision\n",
       "        won't help you much.\n",
       "\n",
       "  ────────────────────────────────────────────────────────────────────────────\n",
       "\n",
       "  Keyword Arguments (kwargs):\n",
       "\n",
       "  rtol and atol: relative and absolute error tolerances\n",
       "\n",
       "  maxit: limit on nonlinear iterations\n",
       "\n",
       "  lmaxit: limit on linear iterations. If lmaxit > m-1, where FPS has m\n",
       "  columns, and you need more than m-1 linear iterations, then GMRES will\n",
       "  restart. \n",
       "\n",
       "  The default is -1. This means that you'll take m-1 iterations, where size(V)\n",
       "  = (n,m), and get no restarts. –> Restarted GMRES is not ready yet.\n",
       "\n",
       "  lsolver: the linear solver, default = \"gmres\"\n",
       "\n",
       "  Your choices will be \"gmres\" or \"bicgstab\". However, gmres is the only\n",
       "  option for now.\n",
       "\n",
       "  eta and fixed eta: eta > 0 or there's an error\n",
       "\n",
       "  The linear solver terminates when ||F'(x)s + F(x) || <= etag || F(x) ||\n",
       "\n",
       "  where \n",
       "\n",
       "  etag = eta if fixedeta=true\n",
       "\n",
       "  etag = Eisenstat-Walker as implemented in book if fixedeta=false\n",
       "\n",
       "  The default, which may change, is eta=.1, fixedeta=true\n",
       "\n",
       "  Pvec: Preconditioner-vector product. The rules are similar to Jvec So,\n",
       "  Pv=Pvec(v,x) or Pv=Pvec(v,x,pdata) returns P(x) v where P(x) is the\n",
       "  preconditioner. You must use x as an input even if your preconditioner does\n",
       "  not depend on x\n",
       "\n",
       "  armmax: upper bound on step size reductions in line search\n",
       "\n",
       "  dx: default = 1.e-7\n",
       "\n",
       "  difference increment in finite-difference derivatives h=dx*norm(x,Inf)+1.e-8\n",
       "\n",
       "  armfix: default = false\n",
       "\n",
       "  The default is a parabolic line search (ie false). Set to true and the step\n",
       "  size will be fixed at .5. Don't do this unless you are doing experiments for\n",
       "  research.\n",
       "\n",
       "  pdata:\n",
       "\n",
       "  precomputed data for the function/Jacobian-vector/Preconditioner-vector\n",
       "  products. Things will go better if you use this rather than hide the data in\n",
       "  global variables within the module for your function/Jacobian\n",
       "\n",
       "  If you use pdata in any of F!, Jvec, or Pvec, you must use in in all of\n",
       "  them.\n",
       "\n",
       "  printerr: default = true\n",
       "\n",
       "  I print a helpful message when the solver fails. To suppress that message\n",
       "  set printerr to false.\n",
       "\n",
       "  keepsolhist: default = false\n",
       "\n",
       "  Set this to true to get the history of the iteration in the output tuple.\n",
       "  This is on by default for scalar equations and off for systems. Only turn it\n",
       "  on if you have use for the data, which can get REALLY LARGE.\n",
       "\n",
       "  stagnationok: default = false\n",
       "\n",
       "  Set this to true if you want to disable the line search and either observe\n",
       "  divergence or stagnation. This is only useful for research or writing a\n",
       "  book.\n",
       "\n",
       "  Output:\n",
       "\n",
       "    •    A named tuple (solution, functionval, history, stats, idid,\n",
       "        errcode, solhist)\n",
       "\n",
       "  where\n",
       "\n",
       "  – solution = converged result\n",
       "\n",
       "  – functionval = F(solution)\n",
       "\n",
       "  – history = the vector of residual norms (||F(x)||) for the iteration\n",
       "\n",
       "  – stats = named tuple of the history of (ifun, ijvec, iarm, ikfail), the\n",
       "  number of functions/Jacobian-vector prods/steplength reductions/linear\n",
       "  solver failures at each iteration. Linear solver failures DO NOT mean that\n",
       "  the nonlinear solver will fail. You should look at this stat if, for\n",
       "  example, the line search fails. Increasing the size of FPS and/or lmaxit\n",
       "  might solve the problem.\n",
       "\n",
       "  I do not count the function values for a finite-difference derivative\n",
       "  because they count toward a Jacobian-vector product.\n",
       "\n",
       "  – idid=true if the iteration succeeded and false if not.\n",
       "\n",
       "  – errcode = 0 if if the iteration succeeded\n",
       "\n",
       "\u001b[36m      = -1 if the initial iterate satisfies the termination criteria\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m      = 10 if no convergence after maxit iterations\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m      = 1  if the line search failed\u001b[39m\n",
       "\n",
       "  – solhist:\n",
       "\n",
       "\u001b[36m    This is the entire history of the iteration if you've set\u001b[39m\n",
       "\u001b[36m    keepsolhist=true\u001b[39m\n",
       "\n",
       "  solhist is an N x K array where N is the length of x and K is the number of\n",
       "  iteration + 1. So, for scalar equations, it's a row vector.\n",
       "\n",
       "  ────────────────────────────────────────────────────────────────────────────\n",
       "\n",
       "\u001b[1m  Examples\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡≡≡≡\u001b[22m\n",
       "\n",
       "\u001b[1m  Simple 2D problem. You should get the same results as for nsol.jl\u001b[22m\n",
       "\u001b[1m because\u001b[22m\n",
       "\u001b[1m  --------------------------\u001b[22m\n",
       "\n",
       "  GMRES will solve the equation for the step exactly in two iterations. Finite\n",
       "  difference Jacobians and analytic Jacobian-vector products for full\n",
       "  precision and finite difference Jacobian-vector products for single\n",
       "  precision.\n",
       "\n",
       "\u001b[36m  julia> function f!(fv,x)\u001b[39m\n",
       "\u001b[36m         fv[1]=x[1] + sin(x[2])\u001b[39m\n",
       "\u001b[36m         fv[2]=cos(x[1]+x[2])\u001b[39m\n",
       "\u001b[36m         end\u001b[39m\n",
       "\u001b[36m  f! (generic function with 1 method)\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> function JVec(v, fv, x)\u001b[39m\n",
       "\u001b[36m         jvec=zeros(2,);\u001b[39m\n",
       "\u001b[36m         p=-sin(x[1]+x[2])\u001b[39m\n",
       "\u001b[36m         jvec[1]=v[1]+cos(x[2])*v[2]\u001b[39m\n",
       "\u001b[36m         jvec[2]=p*(v[1]+v[2])\u001b[39m\n",
       "\u001b[36m         return jvec\u001b[39m\n",
       "\u001b[36m         end\u001b[39m\n",
       "\u001b[36m  JVec (generic function with 1 method)\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> x0=ones(2,); fv=zeros(2,); jv=zeros(2,2); jv32=zeros(Float32,2,2);\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> jvs=zeros(2,3); jvs32=zeros(Float32,2,3);\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> nout=nsol(f!,x0,fv,jv; sham=1);\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> kout=nsoli(f!,x0,fv,jvs,JVec; fixedeta=true, eta=.1, lmaxit=2);\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> kout32=nsoli(f!,x0,fv,jvs32; fixedeta=true, eta=.1, lmaxit=2);\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> [nout.history kout.history kout32.history]\u001b[39m\n",
       "\u001b[36m  5×3 Array{Float64,2}:\u001b[39m\n",
       "\u001b[36m   1.88791e+00  1.88791e+00  1.88791e+00\u001b[39m\n",
       "\u001b[36m   2.43119e-01  2.43120e-01  2.43119e-01\u001b[39m\n",
       "\u001b[36m   1.19231e-02  1.19231e-02  1.19231e-02\u001b[39m\n",
       "\u001b[36m   1.03266e-05  1.03261e-05  1.03273e-05\u001b[39m\n",
       "\u001b[36m   1.46416e-11  1.40862e-11  1.45457e-11\u001b[39m"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?nsoli"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmarking the H-equation with nsoli.jl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will begin by comparing the fastest solution from Chapter 2 with two variants of Newton-GMRES, one with fixed $\\eta = .1$ and one with Eisenstat-Walker with $\\eta_{max}=.9$ and $\\gamma = .9$. I'll allocate 20 vectors for the Krylob basis in the array FPK.\n",
    "\n",
    "We'll begin with a small version of the problem and compare the iteration statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=512;\n",
    "FS=ones(n,); FPS=ones(n,n); FPS32=ones(Float32,n,n); x0=ones(n,); c=.5; hdata = heqinit(x0, c);\n",
    "bargs=(atol = 1.e-10, rtol = 1.e-10, sham = 5, resdec = .1, pdata=hdata);\n",
    "FPK=zeros(n,20);\n",
    "kbargs=(atol = 1.e-10, rtol = 1.e-10, eta=.1, fixedeta=true, pdata=hdata);\n",
    "kbargsew=(atol = 1.e-10, rtol = 1.e-10, eta=.9, fixedeta=false, pdata=hdata);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll run the winner from Chapter 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "nout=nsol(heqf!, x0, FS, FPS32, heqJ!; bargs...);\n",
    "kout=nsoli(heqf!, x0, FS, FPK; kbargs...);\n",
    "koutew=nsoli(heqf!, x0, FS, FPK; kbargsew...);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's interesting to compare the residual histories. They are essentially the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6×3 Array{Float64,2}:\n",
       " 3.49504e+00  3.49504e+00  3.49504e+00\n",
       " 1.79696e-02  4.98627e-02  4.98627e-02\n",
       " 1.55512e-04  1.84641e-03  1.84641e-03\n",
       " 1.33167e-06  1.82364e-04  1.82364e-04\n",
       " 1.13962e-08  2.34291e-06  2.34291e-06\n",
       " 9.75197e-11  2.42540e-11  2.42540e-11"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[nout.history kout.history koutew.history]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the costs is harder. While a Jacobian-vector product for this problem has the same cost as a call to the function, the cost per iteration for nsol.jl is harder to evaluate in these terms. It's better to look at the benchmark results for a larger problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=4096;\n",
    "FS=ones(n,); FPS=ones(n,n); FPS32=ones(Float32,n,n); x0=ones(n,); c=.5; hdata = heqinit(x0, c);\n",
    "bargs=(atol = 1.e-10, rtol = 1.e-10, sham = 5, resdec = .1, pdata=hdata);\n",
    "FPK=zeros(n,20);\n",
    "kbargs=(atol = 1.e-10, rtol = 1.e-10, eta=.1, fixedeta=true, pdata=hdata);\n",
    "kbargsew=(atol = 1.e-10, rtol = 1.e-10, eta=.9, fixedeta=false, pdata=hdata);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shamanskii, n=5\n",
      "  99.526 ms (8274 allocations: 1.10 MiB)\n",
      "Newton-GMRES, fixed eta\n",
      "  1.669 ms (405 allocations: 1.92 MiB)\n",
      "Newton-GMRES, Eisenstat-Walker\n",
      "  1.708 ms (405 allocations: 1.92 MiB)\n"
     ]
    }
   ],
   "source": [
    "println(\"Shamanskii, n=5\"); @btime nsol(heqf!, $x0, $FS, $FPS32, heqJ!; bargs...);\n",
    "println(\"Newton-GMRES, fixed eta\"); @btime nsoli(heqf!, $x0, $FS, $FPK; kbargs...);\n",
    "println(\"Newton-GMRES, Eisenstat-Walker\"); @btime nsoli(heqf!, $x0, $FS, $FPK; kbargsew...);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Newton-Krylov code is over 50 times faster. This is not unique to this problem. If your Jacobian is well-conditioned or you have a good preconditioner, as we do in the PDE example, Newton-Krylov should perform much better than any variation of Newton's method using direct linear solvers.\n",
    "\n",
    "The other interesting thing in this example is that the two forcing term choices performed equally well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preconditioning the Convection-Diffusion Equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will benchmark the Newton-GMRES iteration agains the direct solvers from Chapter 2 and explore the differences between left and right preconditioning. We will begin by repeating the computation for the fastest version using nsoli."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nsol, sham=5\n",
      "  6.219 ms (471 allocations: 7.00 MiB)\n"
     ]
    }
   ],
   "source": [
    "n=31;\n",
    "# Get some room for the residual\n",
    "u0=zeros(n*n,);\n",
    "FV=copy(u0);\n",
    "# Get the precomputed data from pdeinit\n",
    "pdata=pdeinit(n)\n",
    "# Storage for the Jacobian, same sparsity pattern as the discrete Laplacian\n",
    "J=copy(pdata.D2);\n",
    "# Iteration Parameters\n",
    "rtol=1.e-7\n",
    "atol=1.e-10\n",
    "println(\"nsol, sham=5\"); @btime nsol(pdeF!, u0, FV, J, pdeJ!; resdec=.5, rtol=rtol, atol=atol, pdata=pdata, sham=5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll set up the problem for nsoli. We need to allocate storage for the Krylov basis. One case will be no preconditioning at all, so the Kryov basis will need more storage. The analytic Jacobian-vector product is __Jvec2d.jl__, which is in __TestProblems/EllipticPDE.jl__. The preconditioner is __Pvec2d.jl__ from __TestProblems/PDE_Tools.jl__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nsoli, not preconditioned\n",
      "  6.952 ms (5336 allocations: 11.73 MiB)\n"
     ]
    }
   ],
   "source": [
    "# Storage for the Krylov basis\n",
    "    JV = zeros(n * n, 100)\n",
    "    eta=.1\n",
    "    fixedeta=false\n",
    "println(\"nsoli, not preconditioned\")\n",
    "@btime nsoli(pdeF!, u0, FV, JV, Jvec2d; rtol=rtol, atol=atol, Pvec=nothing, pdata=pdata, eta=eta,\n",
    "            fixedeta=fixedeta, pside=\"right\");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even with no preconditioning, the iterative solver is almost as fast as __nsol.jl__ using the direct method. When you precondition, which we will do from the right for now, the difference is a factor of almost three over the solve without preconditioning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nsoli, preconditioned, Eisenstat-Walker forcing term\n",
      "  2.286 ms (1499 allocations: 4.98 MiB)\n"
     ]
    }
   ],
   "source": [
    "println(\"nsoli, preconditioned, Eisenstat-Walker forcing term\")\n",
    "@btime nsoli(pdeF!, u0, FV, JV, Jvec2d; rtol=rtol, atol=atol, Pvec=Pvec2d, pdata=pdata, eta=eta,\n",
    "            fixedeta=fixedeta, pside=\"right\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will benchmark with a fixed forcing term for our next example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nsoli, preconditioned, fixed eta\n",
      "  2.879 ms (1904 allocations: 6.43 MiB)\n"
     ]
    }
   ],
   "source": [
    "fixedeta=true;\n",
    "println(\"nsoli, preconditioned, fixed eta\")\n",
    "@btime nsoli(pdeF!, u0, FV, JV, Jvec2d; rtol=rtol, atol=atol, Pvec=Pvec2d, pdata=pdata, eta=eta,\n",
    "            fixedeta=fixedeta, pside=\"right\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example, we see that Eisenstat-Walker is a bit better. Finally, we return to Eisenstat-Walker with $\\eta_{max} = .9$. We see very little difference from $\\eta_{max}=.1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nsoli, preconditioned, Eisenstat-Walker forcing term\n",
      "  2.253 ms (1516 allocations: 5.03 MiB)\n"
     ]
    }
   ],
   "source": [
    "eta=.9; fixedeta=false;\n",
    "println(\"nsoli, preconditioned, Eisenstat-Walker forcing term\")\n",
    "@btime nsoli(pdeF!, u0, FV, JV, Jvec2d; rtol=rtol, atol=atol, Pvec=Pvec2d, pdata=pdata, eta=eta,\n",
    "            fixedeta=fixedeta, pside=\"right\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Left preconditioning? We'll see that even with $\\eta_{max}=.1$ it's a bit slower that right preconditioning. We leave the experiment with $\\eta_{max} = .9$ to the reader. It's not pretty. Can you figure out why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nsoli, left preconditioned, Eisenstat-Walker forcing term\n",
      "  2.793 ms (1647 allocations: 5.29 MiB)\n"
     ]
    }
   ],
   "source": [
    "eta=.1\n",
    "println(\"nsoli, left preconditioned, Eisenstat-Walker forcing term\")\n",
    "@btime nsoli(pdeF!, u0, FV, JV, Jvec2d; rtol=rtol, atol=atol, Pvec=Pvec2d, pdata=pdata, eta=eta,\n",
    "            fixedeta=fixedeta, pside=\"left\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.5.3",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
