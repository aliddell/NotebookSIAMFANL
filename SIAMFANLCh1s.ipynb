{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "using NotebookSIAMFANL\n",
    "using SIAMFANLEquations\n",
    "using LinearAlgebra\n",
    "using SIAMFANLEquations.TestProblems\n",
    "using PyPlot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1.10 Solvers for Chapter 1\n",
    "\n",
    "Contents for Section 1.10\n",
    "\n",
    "[Overview](#Overview)\n",
    "\n",
    "[nsolsc.jl](#nsolsc.jl)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provide two solvers for this chapter. __nsolsc.jl__ is a scalar Newton code. Its calling sequence and the tuple it returns are very similar to all the codes from this book. All our solvers return a tuple with the solution, the history of the iteration, flags for success or failure, and (optionally) the entire history of the solution. \n",
    "\n",
    "The solution hisory for scalar equations is small and returning it is the default. In the later chapters on systems of equations, we do not return the solution history by default and discourage your asking for it. The solution history might take a lot of space to store and also, especially in Julia, have a severe penalty for allocations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nsolsc.jl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__nsolsc.jl__ is the scalar Newton solver. We will begin, as we will in all the software sections, by looking at the documentation in the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search: \u001b[0m\u001b[1mn\u001b[22m\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1ml\u001b[22m\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1mc\u001b[22m\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "nsolsc(f,x, fp=difffp; rtol=1.e-6, atol=1.e-12, maxit=10,         solver=\"newton\", sham=1, armmax=10, resdec=.1,         armfix=false, printerr=true, keepsolhist=true)\n",
       "\n",
       "Newton's method for scalar equations. Has most of the features a code for systems of equations needs.\n",
       "\n",
       "Input:\n",
       "\n",
       "f: function\n",
       "\n",
       "x: initial iterate\n",
       "\n",
       "fp: derivative. If your derivative function is fp, you give me its name. For example fp=foobar tells me that foobar is your function for the derivative. The default is a forward difference Jacobian that I provide.\n",
       "\n",
       "Options:\n",
       "\n",
       "rtol, atol: real and absolute error tolerances\n",
       "\n",
       "maxit: upper bound on number of nonlinear iterations\n",
       "\n",
       "solver:\n",
       "\n",
       "Your choices are \"newton\"(default), \"secant\", or \"chord\". However,  you have sham at your disposal only if you chose newton. \"chord\" will keep using the initial derivative until the iterate converges, uses the iteration budget, or the line search fails. It is not the same as sham=Inf, which is smarter.\n",
       "\n",
       "If you use secant and your initial iterate is poor, you have made a mistake. I will help you by driving the line search with a finite difference derivative.\n",
       "\n",
       "sham:\n",
       "\n",
       "This is the Shamanskii method. If sham=1, you have Newton. The iteration updates the derivative every sham iterations. The covergence rate has local q-order sham+1 if you only count iteratons where you update the derivative. You need not provide your own derivative function to use this option. sham=Inf is chord only if chord is converging well.\n",
       "\n",
       "armmax: upper bound on stepsize reductions in linesearch\n",
       "\n",
       "resdec: target value for residual reduction. \n",
       "\n",
       "The default value is .1. In the old MATLAB codes it was .5. I only turn Shamanskii on if the residuals are decreasing rapidly, at least a factor of resdec, and the line search is quiescent. If you want to eliminate resdec from the method ( you don't ) then set resdec = 1.0 and you will never hear from it again.  \n",
       "\n",
       "armfix:\n",
       "\n",
       "The default is a parabolic line search (ie false). Set to true and the stepsize will be fixed at .5. Don't do this unless you are doing experiments for research.\n",
       "\n",
       "printerr:\n",
       "\n",
       "I print a helpful message when the solver fails. To supress that message set printerr to false.\n",
       "\n",
       "keepsolhist:\n",
       "\n",
       "Set this to true to get the history of the iteration in the output tuple. This is on by default for scalar equations and off for systems. Only turn it on if you have use for the data, which can get REALLY LARGE.\n",
       "\n",
       "Output:\n",
       "\n",
       "A tuple (solution, functionval, history, stats, idid, solhist) where history is the vector of residual norms (|f(x)|) for the iteration and stats is a tuple of the history of (ifun, ijac, iarm), the number of functions/derivatives/steplength reductions at each iteration.\n",
       "\n",
       "I do not count the function values for a finite-difference derivative because they count toward a Jacobian evaluation. I do count them for the secant method model.\n",
       "\n",
       "idid=true if the iteration succeeded and false if not.\n",
       "\n",
       "solhist:\n",
       "\n",
       "This is the entire history of the iteration if you've set keepsolhist=true\n",
       "\n"
      ],
      "text/markdown": [
       "nsolsc(f,x, fp=difffp; rtol=1.e-6, atol=1.e-12, maxit=10,         solver=\"newton\", sham=1, armmax=10, resdec=.1,         armfix=false, printerr=true, keepsolhist=true)\n",
       "\n",
       "Newton's method for scalar equations. Has most of the features a code for systems of equations needs.\n",
       "\n",
       "Input:\n",
       "\n",
       "f: function\n",
       "\n",
       "x: initial iterate\n",
       "\n",
       "fp: derivative. If your derivative function is fp, you give me its name. For example fp=foobar tells me that foobar is your function for the derivative. The default is a forward difference Jacobian that I provide.\n",
       "\n",
       "Options:\n",
       "\n",
       "rtol, atol: real and absolute error tolerances\n",
       "\n",
       "maxit: upper bound on number of nonlinear iterations\n",
       "\n",
       "solver:\n",
       "\n",
       "Your choices are \"newton\"(default), \"secant\", or \"chord\". However,  you have sham at your disposal only if you chose newton. \"chord\" will keep using the initial derivative until the iterate converges, uses the iteration budget, or the line search fails. It is not the same as sham=Inf, which is smarter.\n",
       "\n",
       "If you use secant and your initial iterate is poor, you have made a mistake. I will help you by driving the line search with a finite difference derivative.\n",
       "\n",
       "sham:\n",
       "\n",
       "This is the Shamanskii method. If sham=1, you have Newton. The iteration updates the derivative every sham iterations. The covergence rate has local q-order sham+1 if you only count iteratons where you update the derivative. You need not provide your own derivative function to use this option. sham=Inf is chord only if chord is converging well.\n",
       "\n",
       "armmax: upper bound on stepsize reductions in linesearch\n",
       "\n",
       "resdec: target value for residual reduction. \n",
       "\n",
       "The default value is .1. In the old MATLAB codes it was .5. I only turn Shamanskii on if the residuals are decreasing rapidly, at least a factor of resdec, and the line search is quiescent. If you want to eliminate resdec from the method ( you don't ) then set resdec = 1.0 and you will never hear from it again.  \n",
       "\n",
       "armfix:\n",
       "\n",
       "The default is a parabolic line search (ie false). Set to true and the stepsize will be fixed at .5. Don't do this unless you are doing experiments for research.\n",
       "\n",
       "printerr:\n",
       "\n",
       "I print a helpful message when the solver fails. To supress that message set printerr to false.\n",
       "\n",
       "keepsolhist:\n",
       "\n",
       "Set this to true to get the history of the iteration in the output tuple. This is on by default for scalar equations and off for systems. Only turn it on if you have use for the data, which can get REALLY LARGE.\n",
       "\n",
       "Output:\n",
       "\n",
       "A tuple (solution, functionval, history, stats, idid, solhist) where history is the vector of residual norms (|f(x)|) for the iteration and stats is a tuple of the history of (ifun, ijac, iarm), the number of functions/derivatives/steplength reductions at each iteration.\n",
       "\n",
       "I do not count the function values for a finite-difference derivative because they count toward a Jacobian evaluation. I do count them for the secant method model.\n",
       "\n",
       "idid=true if the iteration succeeded and false if not.\n",
       "\n",
       "solhist:\n",
       "\n",
       "This is the entire history of the iteration if you've set keepsolhist=true\n"
      ],
      "text/plain": [
       "  nsolsc(f,x, fp=difffp; rtol=1.e-6, atol=1.e-12, maxit=10, solver=\"newton\",\n",
       "  sham=1, armmax=10, resdec=.1, armfix=false, printerr=true, keepsolhist=true)\n",
       "\n",
       "  Newton's method for scalar equations. Has most of the features a code for\n",
       "  systems of equations needs.\n",
       "\n",
       "  Input:\n",
       "\n",
       "  f: function\n",
       "\n",
       "  x: initial iterate\n",
       "\n",
       "  fp: derivative. If your derivative function is fp, you give me its name. For\n",
       "  example fp=foobar tells me that foobar is your function for the derivative.\n",
       "  The default is a forward difference Jacobian that I provide.\n",
       "\n",
       "  Options:\n",
       "\n",
       "  rtol, atol: real and absolute error tolerances\n",
       "\n",
       "  maxit: upper bound on number of nonlinear iterations\n",
       "\n",
       "  solver:\n",
       "\n",
       "  Your choices are \"newton\"(default), \"secant\", or \"chord\". However, you have\n",
       "  sham at your disposal only if you chose newton. \"chord\" will keep using the\n",
       "  initial derivative until the iterate converges, uses the iteration budget,\n",
       "  or the line search fails. It is not the same as sham=Inf, which is smarter.\n",
       "\n",
       "  If you use secant and your initial iterate is poor, you have made a mistake.\n",
       "  I will help you by driving the line search with a finite difference\n",
       "  derivative.\n",
       "\n",
       "  sham:\n",
       "\n",
       "  This is the Shamanskii method. If sham=1, you have Newton. The iteration\n",
       "  updates the derivative every sham iterations. The covergence rate has local\n",
       "  q-order sham+1 if you only count iteratons where you update the derivative.\n",
       "  You need not provide your own derivative function to use this option.\n",
       "  sham=Inf is chord only if chord is converging well.\n",
       "\n",
       "  armmax: upper bound on stepsize reductions in linesearch\n",
       "\n",
       "  resdec: target value for residual reduction. \n",
       "\n",
       "  The default value is .1. In the old MATLAB codes it was .5. I only turn\n",
       "  Shamanskii on if the residuals are decreasing rapidly, at least a factor of\n",
       "  resdec, and the line search is quiescent. If you want to eliminate resdec\n",
       "  from the method ( you don't ) then set resdec = 1.0 and you will never hear\n",
       "  from it again. \n",
       "\n",
       "  armfix:\n",
       "\n",
       "  The default is a parabolic line search (ie false). Set to true and the\n",
       "  stepsize will be fixed at .5. Don't do this unless you are doing experiments\n",
       "  for research.\n",
       "\n",
       "  printerr:\n",
       "\n",
       "  I print a helpful message when the solver fails. To supress that message set\n",
       "  printerr to false.\n",
       "\n",
       "  keepsolhist:\n",
       "\n",
       "  Set this to true to get the history of the iteration in the output tuple.\n",
       "  This is on by default for scalar equations and off for systems. Only turn it\n",
       "  on if you have use for the data, which can get REALLY LARGE.\n",
       "\n",
       "  Output:\n",
       "\n",
       "  A tuple (solution, functionval, history, stats, idid, solhist) where history\n",
       "  is the vector of residual norms (|f(x)|) for the iteration and stats is a\n",
       "  tuple of the history of (ifun, ijac, iarm), the number of\n",
       "  functions/derivatives/steplength reductions at each iteration.\n",
       "\n",
       "  I do not count the function values for a finite-difference derivative\n",
       "  because they count toward a Jacobian evaluation. I do count them for the\n",
       "  secant method model.\n",
       "\n",
       "  idid=true if the iteration succeeded and false if not.\n",
       "\n",
       "  solhist:\n",
       "\n",
       "  This is the entire history of the iteration if you've set keepsolhist=true"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "? nsolsc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input\n",
    "\n",
    "\n",
    "Let's begin with the calling sequence for the solver.\n",
    "\n",
    "```julia\n",
    "nsolsc(\n",
    "    f,\n",
    "    x,\n",
    "    fp = difffp;\n",
    "    rtol = 1.e-6,\n",
    "    atol = 1.e-12,\n",
    "    maxit = 10,\n",
    "    solver = \"newton\",\n",
    "    sham = 1,\n",
    "    armmax = 5,\n",
    "    resdec = .1,\n",
    "    armfix = false,\n",
    "    keepsolhist = true,\n",
    ")\n",
    "```\n",
    "\n",
    "The arguments before the semicolon are required. We are solving $f(x) = 0$ and the solver needs $f$ and the initial iterate $x$. The arguments after the semicolon are __keyword arguments__, usually refered to as __kwargs__, which is not a German cheese product. The semicolon is __very important__. Do not leave it out when using kwargs. The\n",
    "good news about kwargs is that you may use any of them without worrying about the others, which will take their default values. So\n",
    "\n",
    "```julia\n",
    "nsolout0 = nsolsc(atan, 1.0)\n",
    "\n",
    "nsolout1 = nsolsc(atan, 1.0; solver = secant)\n",
    "\n",
    "nsolout2=nsolsc(atan, 3.0; sham=2, resdec=.5)\n",
    "```\n",
    "are all correct.\n",
    "\n",
    "\n",
    "You have seen many of the kwargs before. The realtive and absolute error tolerances, the solver, the parameters\n",
    "for the Shamanskii method and line search should be familiar. The new things are __resedec__, __armfix__, and __keepsolshist__. For example, the derivative is updated every __sham__ iterations. Newton's method is sham=1.\n",
    "\n",
    "The default for derivative evaluation is a forward difference derivative. That is an internal function __difffp__.\n",
    "If you have an analytic derivative, say __fpanal.jl__, then set fp=fpanal and the solver will use your derivative.\n",
    "\n",
    "The documentation explains these parameters. We have mentioned the solution history before. Pleaes leave __keepsolhist__ alone unless there's a good reason to change it. It is set to true for scalar codes and false for\n",
    "the solvers in the following chapters.\n",
    "\n",
    "__resdec__ is how we manage Shamanskii iterations. In this scalar code, it is used for some examples and to prepare you for its more serious use in the codes for systems of equations. In __nsolsc.jl__ the default solver is Newton's method (so sham=1). Newton with sham=2 is the Shamanskii method with a derivative update every two iterations. \n",
    "__But__ we safeguard the skipping of the update by doing the update anyhow if (1) the line search fails on the first attempt (ie with step length = 1) or (2) the residual decrease is more than __resdec__. If you want to eliminate the second of these, set resdec = 1. The only exception to the first criterion is the chord method. If you set solver='chord' then you will get chord. That's in there for research and a few internal tests of the code. \n",
    "If you set sham=Inf, then you'll get the chord method with derivative updates when a step length of 1 fails to produce sufficient decrease or the reduction in residuals is not enough.\n",
    "\n",
    "We also compute a difference derivative in the secant method if the line search kicks in. As I said in the comment lines near that part of the code ...\n",
    "\n",
    "```Julia\n",
    "\n",
    "    # If you like the secant or sham=large methods, I will do a\n",
    "    # difference Jacobian anyhow if the line search kicks in.\n",
    "    # You will thank me for this.\n",
    "    # Even if you don't thank me, I will do it anyhow.\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output\n",
    "\n",
    "The output of all the solvers is a tuple. This is a data structure in Julia that can pack differnet structures (including more tuples) in one thing. It's a good way to manage complex output.\n",
    "\n",
    "__nsolsc.jl__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.4.1",
   "language": "julia",
   "name": "julia-1.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
