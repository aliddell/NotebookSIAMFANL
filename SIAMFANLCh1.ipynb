{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\newcommand{\\calf}{{\\cal F}}\n",
    "\\newcommand{\\dnu}{d \\nu}\n",
    "\\newcommand{\\mf}{{\\bf F}}\n",
    "\\newcommand{\\vu}{{\\bf u}}\n",
    "\\newcommand{\\vx}{{\\bf x}}\n",
    "\\newcommand{\\vy}{{\\bf y}}\n",
    "\\newcommand{\\vs}{{\\bf s}}\n",
    "\\newcommand{\\ve}{{\\bf e}}\n",
    "\\newcommand{\\vd}{{\\bf d}}\n",
    "\\newcommand{\\mg}{{\\bf G}}\n",
    "\\newcommand{\\ml}{{\\bf L}}\n",
    "\\newcommand{\\mg}{{\\bf G}}\n",
    "\\newcommand{\\mv}{{\\bf V}}\n",
    "\\newcommand{\\ma}{{\\bf A}}\n",
    "\\newcommand{\\mi}{{\\bf I}}\n",
    "\\newcommand{\\mm}{{\\bf M}}\n",
    "\\newcommand{\\ball}{{\\cal B}}\n",
    "\\newcommand{\\ptc}{{\\Psi TC}}\n",
    "\\newcommand{\\diag}{\\mbox{diag}}\n",
    "\\newcommand{\\begeq}{{\\begin{equation}}}\n",
    "\\newcommand{\\endeq}{{\\end{equation}}}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"fanote_init.jl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1: Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1.1: What is the problem?\n",
    "\n",
    "Solving nonlinear equations is part of almost all simulations of\n",
    "physical processes. Physical models that are expressed as\n",
    "nonlinear partial differential equations, for example, become\n",
    "large systems of nonlinear equations when discretized. Developers of\n",
    "simulation codes must either use a nonlinear solver as a tool or\n",
    "write one from scratch. One purpose of this book is to show\n",
    "these developers what technology is available, sketch the implementation,\n",
    "and warn of the problems. We do this via algorithmic outlines,\n",
    "nonlinear solvers in Julia that can be used for\n",
    "production work, a suite of example problems, an IJulia notebook,\n",
    "and chapter-ending projects.\n",
    "\n",
    "We use the standard notation \n",
    "\n",
    "\\begin{equation}\n",
    "\\mf(\\vx) = 0\n",
    "\\end{equation}\n",
    "\n",
    "for systems of $N$ equations in $N$ unknowns. We will refer to this as the\n",
    "__nonlinear equations formulation__.\n",
    "Here $\\mf: \\Omega   \\to R^N$ where $\\Omega \\subset R^N$ is a open set.\n",
    "We will call $\\mf$ the\n",
    "__nonlinear residual__\n",
    " or simply the __residual__.\n",
    "Rarely can the solution of a nonlinear equation be given by\n",
    "a closed-form expression, so iterative methods must be used\n",
    "to approximate the solution numerically.\n",
    "The output of an iterative method\n",
    "is a sequence of approximations to a solution.\n",
    "\n",
    "The __fixed-point__ formulation of a nonlinear equation is\n",
    "\n",
    "$$\n",
    "\\vx = \\mg(\\vx) .\n",
    "$$\n",
    "\n",
    "The difference between the two formulations is not simply a matter of replacing\n",
    "$\\mf(\\vx)$ by $\\vx - \\mg(\\vx)$. Effective algorithms for $\\mf(\\vx) = 0$ take\n",
    "a very different approach from those for fixed-point prolbems.\n",
    "\n",
    "We will spend\n",
    "most of our time in this introductory section on methods for the nonlinear equations formulation.\n",
    "The reason for this is that much of the theory can be explored in the\n",
    "simple context of scalar equations. We will consider fixed-point problems\n",
    "seriously in [Chapter 4](SIAMFANLCh4.ipynb).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 1.1.1: Notation\n",
    "\n",
    "\n",
    "In this book, following the convention in \n",
    "<cite data-cite=\"ctk:roots\"><a href=\"siamfa.html#ctk:roots\">(Kel95)</cite>\n",
    "and\n",
    "<cite data-cite=\"ctk:newton\"><a href=\"siamfa.html#ctk:newton\">(Kel03)</cite>,\n",
    "vectors are to be understood as column vectors.\n",
    "Following\n",
    "<cite data-cite=\"ctk:acta\"><a href=\"siamfa.html#ctk:acta\">(Kel18)</cite>\n",
    "denote vectors by boldfaced lower case letters\n",
    "and matrices by boldfaced upper case letters,\n",
    "for example $\\vx$  and $\\ma$. We denote the $i$th component of $\\vx$\n",
    "by $x_i$ to distinguish between the $i$th member of a sequence of\n",
    "vectors $\\vx_i$. We denote the $ij$ entry of $\\ma$ by $\\ma_{ij}$.\n",
    "\n",
    "The vector\n",
    "$\\vx^*$ will denote a solution, $x$ a potential solution,\n",
    "and $\\{ \\vx_n \\}_{n \\ge 0}$ the sequence of iterates. We will refer to\n",
    "$\\vx_0$ as the\n",
    "\\index{Initial!guess}\n",
    "\\index{Initial!iterate}{\\bf initial iterate (not guess!)}.\n",
    "We will denote\n",
    "the $i$th component of a vector\n",
    "$\\vx_n$ from a sequence by $x_{ni}$. We will rarely need\n",
    "to refer to individual components of vectors.\n",
    "We will let\n",
    "$\\partial \\mf/\\partial x_i$ denote the partial derivative of $\\mf$\n",
    "with respect to\n",
    "$x_i$. As is standard\n",
    "<cite data-cite=\"dens\"><a href=\"siamfa.html#dens\">(DS96)</cite>,\n",
    "<cite data-cite=\"ctk:roots\"><a href=\"siamfa.html#ctk:roots\">(Kel95)</cite>,\n",
    "$\\ve = \\vx - \\vx^*$ will denote the error. So, for example,\n",
    "$\\ve_n = \\vx_n - \\vx^*$ is the error in the $n$th iterate.\n",
    "    \n",
    "If the components of $\\mf$ are differentiable at $\\vx \\in R^N$,\n",
    "we define the __Jacobian matrix__\n",
    "$\\mf'(\\vx)$ by\n",
    "    \n",
    "$$\n",
    "\\mf'(\\vx)_{ij} = \\frac{\\partial f_i}{\\partial x_j} (\\vx).\n",
    "$$\n",
    "Throughout the book, $\\| \\cdot \\|$ will denote the Euclidean norm\n",
    "on $R^N$:\n",
    "    \n",
    "$$\n",
    "\\| \\vx \\| = \\left( \\sum_{i=1}^N x_i^2 \\right)^{1/2}.\n",
    "$$\n",
    "\n",
    "\n",
    "We treat scalar equations with lowercase letters. So a scalar equation\n",
    "is $f(x) = 0$ and the derivative is $f'(x)$.\n",
    "Many of the essential ideas in this book can be illustrated with scalar\n",
    "equations and we do that in this chapter. The exception is the need to\n",
    "solve linear systems of equations and linear least squares problems, which\n",
    "will be the focus of the remaining chapters in the book.\n",
    "The Julia codes for the examples in this section are in the\n",
    "[src/Chapter1](src/Chapter1)\n",
    "directory for the notebook. The solvers and\n",
    "test problems are part of the [SIAMFANLEquations](https://github.com/ctkelley/SIAMFANLEquations.jl) Julia package.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1.2: Newton's Method\n",
    "\n",
    "Most of the methods in this book are variations of Newton's method.\n",
    "The exception will be Anderson acceleration, a solver for fixed-point problems, which we cover in\n",
    "[Chapter 4](SIAMFANLCh4.ipynb).\n",
    "\n",
    "The Newton sequence is\n",
    "\n",
    "$$\n",
    "\\vx_{n+1} = \\vx_n - \\mf'(\\vx_n)^{-1} \\mf(\\vx_n).\n",
    "$$\n",
    "\n",
    "The interpretation of the formula for the Newton iteration is that\n",
    "we model $\\mf$ at the current iterate $\\vx_n$ with the linear\n",
    "function\n",
    "$$\n",
    "\\mm_n(x) = \\mf(\\vx_n) + \\mf'(\\vx_n) (\\vx - \\vx_n)\n",
    "$$\n",
    "and let the root of $\\mm_n$ be the next iteration.\n",
    "$\\mm_n$ is called the\n",
    "__local linear model__.\n",
    "If $\\mf'(\\vx_n)$ is\n",
    "nonsingular, then the Newton sequence is the solution of $\\mm_n(\\vx_{n+1}) = 0$.\n",
    "\n",
    "We illusrate  the local linear model and the\n",
    "Newton iteration for the scalar equation\n",
    "$$\n",
    "\\arctan(x) = 0\n",
    "$$\n",
    "with initial iterate $x_0 = 1$. We graph the local linear model\n",
    "$$\n",
    "m_j(x) = f(x_j) + f'(x_j) (x - x_j)\n",
    "$$\n",
    "at $x_j$ from the point $(x_j, y_j) = (x_j, f(x_j))$ to the next iteration\n",
    "$(x_{j+1},0)$. The iteration converges rapidly and one can see the\n",
    "linear model becoming more and more accurate. The third iterate is\n",
    "visually indistinguishable from the solution.\n",
    "\n",
    "Run the code window below to see the plot. The code __fig1dot1.jl__ runs the solver\n",
    "__nsolsc.jl__ from the[SIAMFANLEquations](https://github.com/ctkelley/NotebookSIAMFANL) Julia package,\n",
    "collects the data for the plot, and then runs some messy PyPlot commands. We will discuss the solver in detail\n",
    "in [Section 1.10](#Section-1.10:-Scalar-Equation-Solver). For now we will focus on the results\n",
    "The semicolon after the function call suppresses some unnecessary output from PyPlot. Remove that semicolon to see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1dot1();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The computation of a Newton iteration requires\n",
    "\n",
    "1. evaluation of $\\mf(\\vx_n)$ and a test for termination,\n",
    "2. approximate solution of the equation\n",
    "$\\mf'(\\vx_n) \\vs = - \\mf(\\vx_n)$\n",
    "for the Newton step $\\vs$, and\n",
    "3. construction of $\\vx_{n+1} = \\vx_n + \\lambda \\vs$, where the\n",
    "step length $\\lambda$ is selected to guarantee decrease in $\\| \\mf \\|$.\n",
    "\n",
    "\n",
    "The computation of the Newton step,\n",
    "consumes most of the work, and the variations\n",
    "in Newton's method that we discuss in this book differ most significantly\n",
    "in how the Newton step is approximated. Computing the\n",
    "step may require evaluation and factorization of the Jacobian matrix\n",
    "or the solution of the linear equation by an iterative method.\n",
    "Not all methods for computing\n",
    "the Newton step require the complete Jacobian matrix, which, as we\n",
    "will see in [Chapter 2](SIAMFANLCh2.ipynb)\n",
    "can be very expensive.\n",
    "\n",
    "In the example from Figure 1.1, the step $s$\n",
    "in  was\n",
    "satisfactory, and we can use $\\lambda=1$ in step 3. The reader\n",
    "should be warned that attention to the step length is generally very\n",
    "important. One should not write one's own nonlinear solver without\n",
    "step-length control (see [Section 1.6](#Section-1.10:-Global_Convergence_and_the_Armijo_Rule)).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 1.2.1: Local Convergence Theory\n",
    "\n",
    "The convergence theory for Newton's method \n",
    "<cite data-cite=\"dens\"><a href=\"siamfa.html#dens\">(DS96)</cite>,\n",
    "<cite data-cite=\"ctk:roots\"><a href=\"siamfa.html#ctk:roots\">(Kel95)</cite>,\n",
    "<cite data-cite=\"ortega\"><a href=\"siamfa.html#ortega\">(OR70)</cite>,\n",
    "that is most often seen in an elementary course in numerical methods\n",
    "is __local__. This means\n",
    "that one assumes that the __initial iterate__ $\\vx_0$ is near a solution.\n",
    "The local convergence\n",
    "theory from\n",
    "<cite data-cite=\"dens\"><a href=\"siamfa.html#dens\">(DS96)</cite>,\n",
    "<cite data-cite=\"ctk:roots\"><a href=\"siamfa.html#ctk:roots\">(Kel95)</cite>,\n",
    "<cite data-cite=\"ortega\"><a href=\"siamfa.html#ortega\">(OR70)</cite>,\n",
    "requires the __standard assumptions__.\n",
    "    \n",
    "---\n",
    "__Assumption:__\n",
    "\n",
    "1. The equation $\\mf(\\vx) = 0$ has a solution $\\vx^*$.\n",
    "2. $\\mf': \\Omega \\to R^{N \\times N}$ is Lipschitz continuous near $\\vx^*$.}\n",
    "3. $\\mf'(\\vx^*)$ is nonsingular.\n",
    "---\n",
    "\n",
    "Recall that Lipschitz continuity near $\\vx^*$\n",
    "means that there is $\\gamma > 0$\n",
    "(the __Lipschitz constant__) such that\n",
    "    \n",
    "$$\n",
    "\\| \\mf'(\\vx) - \\mf'(\\vy) \\| \\le \\gamma \\| \\vx - \\vy \\|\n",
    "$$\n",
    "for all $\\vx, \\vy$ sufficiently near $\\vx^*$.\n",
    "\n",
    "We state the classic local convergence theorem.\n",
    "\n",
    "___\n",
    "__Theorem 1.1__ Let the standard assumptions hold. If $\\vx_0$ is sufficiently\n",
    "near $\\vx^*$, then the Newton sequence exists\n",
    "(i.e., $\\mf'(\\vx_n)$ is nonsingular\n",
    "for all $n \\ge 0$) and converges to $\\vx^*$ and there is $K > 0$ such that\n",
    "    \n",
    "$$\n",
    "\\| \\ve_{n+1} \\| \\le K \\| \\ve_n \\|^2\n",
    "$$\n",
    "for $n$ sufficiently large.\n",
    "___\n",
    "\n",
    "\n",
    "The convergence described by the theorem, in which\n",
    "the error in the solution will be roughly squared with each iteration,\n",
    "is called  __q-quadratic__. \n",
    "Squaring the error roughly means that the number of significant\n",
    "figures in the result doubles with each iteration.  Of course,\n",
    "one cannot examine the error without knowing the solution. However,\n",
    "we can observe the quadratic reduction in the error computationally,\n",
    "if $\\mf'(\\vx*)$ is well conditioned,\n",
    "because the nonlinear residual\n",
    "will also be roughly squared with each iteration.\n",
    "Therefore, we should see the\n",
    "exponent field of the norm of the nonlinear residual roughly double\n",
    "with each iteration.\n",
    "    \n",
    "In Table 1.1 we report the Newton iteration\n",
    "for the scalar ($N=1$) nonlinear equation\n",
    "    \n",
    "$$\n",
    "f(x) = \\tan(x) - x = 0, \\, x_0 = 4.5.\n",
    "$$\n",
    "The solution is $x^* \\approx 4.493$.\n",
    "\n",
    "The decrease in the function\n",
    "is as the theory predicts for the first three iterations, then progress\n",
    "slows down for iteration 4 and stops completely after that. The\n",
    "reason for this __stagnation__\n",
    "is clear: one cannot evaluate the function\n",
    "to higher precision than (roughly) machine unit roundoff, which in\n",
    "the IEEE\n",
    "<cite data-cite=\"ieee\"><a href=\"siamfa.html#ieee\">(IEE85)</cite>,\n",
    "<cite data-cite=\"IEEEnew\"><a href=\"siamfa.html#IEEEnew\">(IEE19)</cite>,\n",
    "<cite data-cite=\"overtonbook\"><a href=\"siamfa.html#overtonbook\">(Ove01)</cite>\n",
    "floating point system is about $10^{-16}$.\n",
    "\n",
    "We make Table 1.1 with __tab1dot1.jl__.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tab1dot1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stagnation is not affected by the accuracy in the derivative. The\n",
    "results reported in Table 1.1 used a\n",
    "forward difference approximation to the derivative with a difference\n",
    "increment of $10^{-6}$. With this choice of difference increment, the\n",
    "convergence speed of the nonlinear iteration is as fast as\n",
    "that for Newton's method, at least for this example, until stagnation\n",
    "takes over.\n",
    "The reader should be aware that difference approximations to derivatives,\n",
    "while usually (but not always) reliable, are often expensive.\n",
    "An inaccurate Jacobian can\n",
    "cause many problems (see \n",
    "[Section 1.9](#Section-1.9:-What_Can_Go_Wrong) )\n",
    "\n",
    "An analytic Jacobian\n",
    "can require some human effort, but can be worth it in terms of\n",
    "computer time and robustness when a difference Jacobian performs poorly.\n",
    "Automatic differentiation (AD) is also becoming an attractive option\n",
    "<cite data-cite=\"grautodiff\"><a href=\"siamfa.html#grautodiff\">(Gri00)</cite>,\n",
    "<cite data-cite=\"anlautodiff\"><a href=\"siamfa.html#anlautodiff\">(HN02)</cite>,\n",
    "<cite data-cite=\"forwarddiff\"><a href=\"siamfa.html#forwarddiff\">(RPL16)</cite>,\n",
    "<cite data-cite=\"zygote\"><a href=\"siamfa.html#zygote\">(CoRR)</cite>.\n",
    "\n",
    "\n",
    "One can quantify this stagnation by adding the errors in the function\n",
    "evaluation and derivative evaluations to __Theorem 1.1__\n",
    "The messages of __Theorem 1.2__ are:\n",
    "\n",
    "---\n",
    "1. Small errors, for example, machine roundoff, in the function evaluation can lead to stagnation. This type of stagnation is usually benign and, if the Jacobian is well conditioned (see the estimate in [Section 1.5](#Section1.5:-Termination_of_the_Iteration) ),the results will be as accurate as the evaluation of $\\mf$\n",
    "\n",
    "2. Errors in the Jacobian and in the solution of the linear equation for the Newton step will affect the speed of the nonlinear iteration, but not the limit of the sequence.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "__Theorem 1.2__ \n",
    "Let the standard assumptions hold. Let a matrix-valued\n",
    "function $\\Delta(\\vx)$ and a vector-valued function $\\epsilon(\\vx)$\n",
    "be such that\n",
    "$$\n",
    "\\| \\Delta(\\vx) \\| < \\delta_J \\mbox{ and }\n",
    "\\| \\epsilon(\\vx) \\| < \\epsilon_F\n",
    "$$\n",
    "for all $\\vx$ near $\\vx^*$.\n",
    "Then, if $\\vx_0$ is sufficiently near $\\vx^*$ and $\\delta_J$ and\n",
    "$\\epsilon_F$ are sufficiently small, the sequence\n",
    "$$\n",
    "\\vx_{n+1} = \\vx_n -\n",
    "( \\mf'(\\vx_n) + \\Delta(\\vx_n))^{-1} (\\mf(\\vx_n) + \\epsilon(\\vx_n))\n",
    "$$\n",
    "is defined (i.e., $\\mf'(\\vx_n) + \\Delta(\\vx_n)$ is nonsingular for all $n$)\n",
    "and satisfies\n",
    "$$\n",
    "\\| \\ve_{n+1} \\| \\le {\\bar K} ( \\| \\ve_n \\|^2 + \\| \\Delta(\\vx_n) \\|\n",
    "\\| \\ve_n \\|\n",
    "+ \\| \\epsilon(\\vx_n)\\| )\n",
    "$$\n",
    "for some ${\\bar K} > 0$.\n",
    "\n",
    "***\n",
    "\n",
    "One can ignore the errors in the function in most cases,\n",
    "but one needs to be aware that stagnation of the nonlinear iteration\n",
    "is all but certain in finite-precision arithmetic. However, the\n",
    "asymptotic convergence results for exact arithmetic\n",
    "describe the observations well for most problems.\n",
    "\n",
    "An important application of __Theorem 1.2__ is the special case where\n",
    "$\\delta_J \\approx \\sqrt{\\epsilon_F}$, _ie_ the Jacobian error is roughly the same\n",
    "as the square root of the function error. In this case the middle term in the error\n",
    "estimate $\\| \\Delta(\\vx_n) \\| \\| \\ve_n \\|$ is smaller than the maximum of the other two.\n",
    "We state this as a corollary.\n",
    "\n",
    "---\n",
    "\n",
    "__Corollary 1.3:__\n",
    "Let the assumptions of __Theorem 1.2__ hold. Assume that\n",
    "$\\delta_J = O(\\sqrt{\\epsilon_F})$. Then $\\vx_0$ is sufficiently near $\\vx^*$\n",
    "$$\n",
    "\\| \\ve_{n+1} \\| = O(  \\| \\ve_n \\|^2  + \\epsilon_F ).\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "There are two examples of particular interest. Suppose $\\epsilon_F$ is double precision\n",
    "unit roundoff. If one approximates the Jacobian with a forward difference approximation\n",
    "with a difference increment of $\\sqrt{\\epsilon_F}$. In that case the corollary states that\n",
    "the iteration statistics will be almost indistinguishable from those with an\n",
    "analytic Jacobian. Similarly, if one stores the Jacobian in single precision, where\n",
    "the unit roundoff is the square root of $\\epsilon_F$, then the estimate in the corollary also holds.\n",
    "Our solvers exploit this by (1) using a forward difference Jacobian as the default\n",
    "and (2) allowing a reduced precision Jacobian. The advantage of using a forward difference\n",
    "Jacobian is that the user (*ie* you) does not have to invest human effort in an analytic Jacobian. The\n",
    "disadvantage is that an analytic Jacobian is usually faster, in terms of computer time.\n",
    "The advantages of using a single precision Jacobian are that both the computer time and\n",
    "the storage burden for the linear solve are cut in half. There are very few disadvantages,\n",
    "as the corollary indicates. We will explore this in detail later in the book.\n",
    "\n",
    "While Table 1.1\n",
    "gives a clear picture of quadratic convergence, it's\n",
    "easier to appreciate a graph.\n",
    "Figure 1.2 is a semilog plot of\n",
    "__residual history__ i.e.,\n",
    "the norm of the nonlinear residual against the iteration number.\n",
    "The concavity of the plot for the first few iterations is the signature of superlinear convergence.\n",
    "The final two iterations show the onset of stagnation.\n",
    "One uses the __semilogy__ command from the __PyPlot__ package\n",
    "in Julia for this. See the\n",
    "file __fig1dot2.jl__, which generated Figure 1.2\n",
    "for an example.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1dot2();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1.3: Approximating the Jacobian\n",
    "\n",
    "As we will see in the subsequent chapters, it is usually most efficient\n",
    "to approximate the Newton step in some way.\n",
    "The price for such an approximation is that the nonlinear iteration\n",
    "converges more slowly; i.e., more nonlinear iterations\n",
    "are needed to solve the problem.\n",
    "However, the overall cost of the solve\n",
    "is usually significantly less, because the computation of the\n",
    "Newton step is less expensive.\n",
    "\n",
    "One way to approximate the Jacobian is to\n",
    "compute $\\mf'(\\vx_0)$ and use that as an approximation to\n",
    "$\\mf'(\\vx_n)$\n",
    "throughout the iteration. In this way one amortizes a single evaluation and factorization of $\\mf'$ over\n",
    "the entire iteration.\n",
    "This is the __chord method__\n",
    "or __modified Newton method__.\n",
    "The convergence\n",
    "of the chord iteration is not as fast as Newton's method. Assuming\n",
    "that the initial iteration is near enough to $x^*$, the convergence\n",
    "is __q-linear__.\n",
    "This means that there is $\\rho \\in (0,1)$ such that\n",
    "$$\n",
    "\\| \\ve_{n+1} \\| \\le \\rho \\| \\ve_{n} \\|\n",
    "$$\n",
    "for $n$ sufficiently large.\n",
    "We can apply __Theorem: 1.2__ to the chord method with\n",
    "$\\epsilon=0$ and\n",
    "$\\| \\Delta(\\vx_n) \\| = O(\\| \\ve_0 \\|)$ and conclude that $\\rho$ is\n",
    "proportional to the initial error.\n",
    "The constant $\\rho$ is called the  __q-factor__.\n",
    "The formal definition of q-linear convergence allows for faster\n",
    "convergence.\n",
    "Q-quadratic convergence is also q-linear, as you can see from the\n",
    "definition.\n",
    "In many cases of q-linear convergence, one observes that\n",
    "$$\n",
    "\\| \\ve_{n+1} \\| \\approx \\rho \\| \\ve_{n} \\|\n",
    "\\mbox{ or }\n",
    "\\| \\mf(\\vx_{n+1}) \\| \\approx \\rho \\| \\mf(\\vx_{n}) \\|.\n",
    "$$\n",
    "In these cases,\n",
    "q-linear convergence is usually easy to see on a semilog plot of the\n",
    "residual norms against the iteration number. The curve appears to\n",
    "be a line with slope $\\approx \\log(\\rho)$ (can you see why?).\n",
    "\n",
    "The __secant method__ \n",
    "for scalar equations approximates the\n",
    "derivative using a finite difference, but, rather than a forward\n",
    "difference, uses the most recent two iterations to form\n",
    "the difference quotient. So\n",
    "$$\n",
    "x_{n+1} = x_n - \\frac{f(x_n)(x_n - x_{n-1})}{f(x_n)- f(x_{n-1})},\n",
    "$$\n",
    "where $x_n$ is the current iteration and $x_{n-1}$ is the\n",
    "iteration before that. The secant method must be initialized\n",
    "with two points. One way to do that is to let $x_{-1}$ be a small perturbation of $x_0$, for example\n",
    "$x_{-1} = 0.99 x_0$. This is what we do in our Julia code __secant.jl__.\n",
    "\n",
    "The formula for the secant method does not extend to\n",
    "systems of equations ($N > 1$) because the denominator in the fraction\n",
    "would be a difference of vectors. We discuss one of the\n",
    "many generalizations of the secant method for systems of equations\n",
    "in \n",
    "[Chapter 5](SIAMFANLCh5.ipynb).\n",
    "\n",
    "The secant method's approximation to $f'(x_n)$ converges to $f'(x^*)$\n",
    "as the iteration progresses. __Theorem 1.2__, with\n",
    "$\\epsilon=0$ and $\\| \\Delta(x_n) \\| = O(\\| e_{n-1} \\|)$, implies that\n",
    "the iteration\n",
    "converges __q-superlinearly__. This means that either $x_n = x^*$\n",
    "for some finite $n$ or\n",
    "$$\n",
    "\\lim_{n \\to \\infty} \\dfrac{\\| e_{n+1} \\|}{\\| e_{n} \\|} = 0.\n",
    "$$\n",
    "Q-superlinear convergence is hard to distinguish from q-quadratic\n",
    "convergence by visual inspection of the semilog plot of the residual history.\n",
    "The residual curve for q-superlinear convergence is concave down\n",
    "but drops less rapidly than the one for Newton's method.\n",
    "\n",
    "Q-quadratic convergence is a special case of q-superlinear convergence. More\n",
    "generally, if $\\vx_n \\to \\vx^*$ and, for some $p > 1$,\n",
    "$$\n",
    "\\| \\ve_{n+1} \\| = O(\\| \\ve_n \\|^p),\n",
    "$$\n",
    "we say that $\\vx_n \\to \\vx^*$ q-superlinearly with\n",
    "__q-order__ $p$.\n",
    "\n",
    "In __Figure 1.3__ we compare Newton's method with the\n",
    "chord method and the secant method for our model problem that generated __Table 1.1__\n",
    "and __Figure 1.2__\n",
    "We see the convergence behavior that the theory\n",
    "predicts in the linear curve for the chord method and in the concave\n",
    "curves for Newton's method and the secant method. We also see\n",
    "the stagnation in the terminal phase.\n",
    "\n",
    "The figure does not show the division by zero that\n",
    "halted the secant method computation\n",
    "at iteration $6$. The secant method\n",
    "has the dangerous property that the difference between $x_n$\n",
    "and $x_{n-1}$ could be too small for an accurate difference\n",
    "approximation. The division by zero that we observed is an extreme\n",
    "case. We will talk about how we generated this figure with the code __fig1dot3.jl__ later in this chapter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1dot3();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1.4: Inexact Newton Methods\n",
    "\n",
    "Rather than approximate the Jacobian, one could instead solve\n",
    "the equation for the Newton step approximately. An\n",
    "__inexact Newton method__\n",
    "<cite data-cite=\"demboes\"><a href=\"siamfa.html#demboes\">(Des82)</cite>\n",
    "replaces the Newton step with \n",
    "a vector $\\vs$ that satisfies the\n",
    "__inexact Newton condition__\n",
    "\n",
    "$$\n",
    "\\| \\mf'(\\vx_n) \\vs + \\mf(\\vx_n) \\| \\le \\eta \\| \\mf(\\vx_n) \\|.\n",
    "$$\n",
    "    \n",
    "The parameter $\\eta$ (the __forcing term__)\n",
    "can be varied as the Newton iteration progresses.\n",
    "Choosing a small value of $\\eta$ will make the iteration more like\n",
    "Newton's method, therefore leading to convergence in fewer iterations.\n",
    "However, a small value of $\\eta$ may make computing a step\n",
    "that satisfies \\eqnok{inexact} very expensive. The local convergence\n",
    "theory\n",
    "<cite data-cite=\"demboes\"><a href=\"siamfa.html#demboes\">(Des82)</cite>,\n",
    "<cite data-cite=\"ctk:roots\"><a href=\"siamfa.html#ctk:roots\">(Kel95)</cite>\n",
    "for inexact Newton methods reflects the intuitive idea that a small\n",
    "value of $\\eta$ leads to fewer iterations. __Theorem 1.4__\n",
    "is a typical example of such a convergence result.\n",
    "    \n",
    "    \n",
    "---\n",
    "__Theorem 1.4__\n",
    "Let the standard assumptions hold. Then there are $\\delta$ and $\\bar \\eta$\n",
    "such that, if $\\| \\ve_0 \\| \\le \\delta$ and\n",
    "$\\{ \\eta_n \\} \\subset [0, \\bar \\eta]$,\n",
    "then the inexact Newton iteration\n",
    "\\[\n",
    "\\vx_{n+1} = \\vx_n + \\vs_n,\n",
    "\\]\n",
    "where\n",
    "$$\n",
    "\\| \\mf'(\\vx_n) \\vs_n + \\mf(\\vx_n) \\| \\le \\eta_n \\| \\mf(\\vx_n) \\|,\n",
    "$$\n",
    "converges q-linearly to $\\vx^*$. Moreover,\n",
    "\n",
    "- if $\\eta_n \\to 0$, the convergence is q-superlinear, and\n",
    "- if $\\eta_n \\le K_\\eta \\| \\mf(\\vx_n) \\|^p$ for some\n",
    "$K_\\eta > 0$, the convergence is q-superlinear with q-order $1+p$.\n",
    "    \n",
    "___\n",
    "\n",
    "\n",
    "One can use __Theorem 1.4__ to analyze the chord method or\n",
    "the secant method. In the case of the chord method, the steps\n",
    "satisfy the inexact Newton condition with\n",
    "$$\n",
    "\\eta_n = O(\\| \\ve_0 \\|),\n",
    "$$\n",
    "which implies q-linear convergence if $\\| \\ve_0 \\|$ is sufficiently small.\n",
    "For the secant method, $\\eta_n = O(\\| \\ve_{n-1} \\|)$, implying\n",
    "q-superlinear convergence. \n",
    "    \n",
    "Iterative methods (such as GMRES\n",
    "<cite data-cite=\"gmres\"><a href=\"siamfa.html#gmres\">(SS86)</cite>) for solving\n",
    "the equation for the Newton step would typically use\n",
    "the inexact Newton condition as a termination criterion. In this case, the\n",
    "overall nonlinear solver is called a\n",
    "__Newton iterative method__.\n",
    "Newton iterative methods are named by the\n",
    "particular iterative method used for the linear equation. For\n",
    "example, the Newton-Krylov solvers in the __nsol.jl__ code,\n",
    "which we describe in [Chapter 3](SIAMFANLCh3.ipynb), include Newton-GMRES.\n",
    "\n",
    "An unfortunate choice of the forcing term $\\eta$ can lead to\n",
    "very poor results. The reader is invited to try the two\n",
    "choices $\\eta = 10^{-6}$ and $\\eta = .9$ in __nsol.jl__\n",
    "to see this. Better choices of $\\eta$ include $\\eta = 0.1$,\n",
    "the author's personal favorite, and a more complex approach\n",
    "(see [Chapter 3](SIAMFANLCh3.ipynb))\n",
    "from \n",
    "<cite data-cite=\"homerstan\"><a href=\"siamfa.html#homerstan\">(EW94)</cite>\n",
    "and\n",
    "<cite data-cite=\"ctk:roots\"><a href=\"siamfa.html#ctk:roots\">(Kel95)</cite>\n",
    "that is the\n",
    "default in __nsol.jl__. Either of these  usually leads to\n",
    "rapid convergence near the solution, but at a much lower cost\n",
    "for the linear solver than a very small forcing term such as\n",
    "$\\eta = 10^{-4}$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1.5: Termination of the Iteration\n",
    "\n",
    "While one cannot know the error without\n",
    "knowing the solution, in most cases the norm of $\\mf(\\vx)$ can be used\n",
    "as a reliable indicator of the rate of decay in $\\| \\ve \\|$ as the\n",
    "iteration progresses \\cite{ctk:roots}. Based on this heuristic, we\n",
    "terminate the iteration in our codes when\n",
    "\n",
    "$$\n",
    "\\| \\mf(\\vx) \\| \\le \\tau_r \\| \\mf(\\vx_0) \\| + \\tau_a.\n",
    "$$\n",
    "\n",
    "The relative $\\tau_r$\n",
    "and absolute $\\tau_a$ error tolerances are\n",
    "both important. Using only the relative reduction in the\n",
    "nonlinear residual as a basis for termination (i.e., setting $\\tau_a = 0$)\n",
    "is a poor idea because an initial iterate that is near the solution\n",
    "may make the criterion impossible to satisfy with $\\tau_a = 0$.\n",
    "\n",
    "One way to quantify the utility of termination when\n",
    "$\\| \\mf(\\vx) \\|$\n",
    "is small is to compare a relative reduction in the norm of the\n",
    "error with a relative reduction in the norm of the nonlinear residual.\n",
    "If the standard assumptions hold and\n",
    "$\\vx_0$ and $\\vx$ are sufficiently near the root, then\n",
    "<cite data-cite=\"ctk:roots\"><a href=\"siamfa.html#ctk:roots\">(Kel95)</cite>\n",
    "\n",
    "$$\n",
    "\\frac{\\| \\ve \\|}{4 \\| \\ve_0 \\| \\kappa(\\mf'(\\vx^*))} \\le\n",
    "\\frac{\\| \\mf(\\vx) \\|}{\\| \\mf(\\vx_0) \\|}\n",
    "\\le \\frac{4 \\kappa(\\mf'(x^*)) \\| \\ve \\|}{\\| \\ve_0 \\|},\n",
    "$$\n",
    "    \n",
    "where\n",
    "    \n",
    "$$\n",
    "\\kappa(\\mf'(\\vx^*)) = \\| \\mf'(\\vx^*)\\| \\| \\mf'(\\vx^*)^{-1} \\|\n",
    "$$\n",
    "    \n",
    "is the\n",
    "__condition number__ of $\\mf'(\\vx^*)$ relative to the norm\n",
    "$\\| \\cdot \\|$. From the estimate above which compares relative error and\n",
    "relative resiual, we conclude that, if the\n",
    "Jacobian is well conditioned (i.e., $\\kappa(\\mf'(\\vx^*))$ is not\n",
    "very large), then our termination criterion useful.\n",
    "This is analogous to the linear case, where\n",
    "a small residual implies a small error if the matrix is well conditioned.\n",
    "    \n",
    "Another approach, which is supported by theory only for superlinearly\n",
    "convergent methods,\n",
    "is to exploit the fast convergence to estimate the error in\n",
    "terms of the step. If the iteration is converging superlinearly, then\n",
    "    \n",
    "$$\n",
    "\\ve_{n+1} = \\ve_n + \\vs_n = o(\\| \\ve_n \\|)\n",
    "$$\n",
    "    \n",
    "and hence\n",
    "    \n",
    "$$\n",
    "\\vs_n = -\\ve_n + o(\\| \\ve_n \\|).\n",
    "$$\n",
    "    \n",
    "Therefore, when the iteration is converging superlinearly, one may\n",
    "use $\\| \\vs_n \\|$ as an estimate of $\\| \\ve_n \\|$. One can estimate the\n",
    "current rate of convergence from above by\n",
    "    \n",
    "$$\n",
    "\\rho_n = \\| \\vs_n \\|/\\| \\vs_{n-1} \\| \\approx \\| \\ve_n \\|/\\| \\ve_{n-1} \\|\n",
    "\\ge \\| \\ve_{n+1} \\|/\\| \\ve_n \\|.\n",
    "$$\n",
    "    \n",
    "Hence, for $n$ sufficiently large,\n",
    "    \n",
    "$$\n",
    "\\| \\ve_{n+1} \\| \\le \\rho_n \\| \\ve_n \\| \\approx \\| \\vs_n \\|^2/\\| \\vs_{n-1}\\|.\n",
    "$$\n",
    "    \n",
    "So, for a superlinearly convergent method, terminating the iteration\n",
    "with $\\vx_{n+1}$ as soon as\n",
    "\n",
    "$$\n",
    "\\| \\vs_n \\|^2/\\|\\vs_{n-1}\\| < \\tau\n",
    "$$\n",
    "will imply that $\\| \\ve_{n+1} \\| < \\tau$. This is __termination on small steps__.\n",
    "    \n",
    "Termination on small steps\n",
    "is only supported by theory for superlinearly convergent\n",
    "methods, but is used for linearly convergent methods in some\n",
    "initial value problem solvers \n",
    "<cite data-cite=\"slc\"><a href=\"siamfa.html#slc\">(BCP96)</cite>,\n",
    "<cite data-cite=\"lsode\"><a href=\"siamfa.html#lsode\">(RH93)</cite>.\n",
    "The trick is\n",
    "to estimate the q-factor $\\rho$, say, by\n",
    "    \n",
    "$$\n",
    "\\rho \\approx \\| \\vs_n \\|/\\| \\vs_{n-1} \\|\n",
    "\\mbox{ or }\n",
    "\\rho \\approx (\\| \\vs_n \\|/\\| \\vs_0 \\|)^{1/n}.\n",
    "$$\n",
    "    \n",
    "Assuming that the estimate of $\\rho$ is reasonable, then\n",
    "    \n",
    "$$\n",
    "\\| \\ve_n \\| - \\| \\vs_n \\| \\le \\| \\ve_{n+1} \\| \\approx \\rho \\| \\ve_n \\|\n",
    "$$\n",
    "    \n",
    "implies that\n",
    "    \n",
    "$$\n",
    "\\| \\ve_{n+1} \\|/\\rho \\approx \\| \\ve_n \\| \\le \\| \\vs_n \\|/(1 - \\rho).\n",
    "$$\n",
    "    \n",
    "Hence, if we terminate the iteration when\n",
    "\n",
    "$$\n",
    "\\| \\vs_n \\| \\le \\tau (1 - \\rho)/\\rho\n",
    "$$\n",
    "    \n",
    "and the estimate of $\\rho$ is an __overestimate__, then\n",
    "the termination on small steps criterion will imply that\n",
    "    \n",
    "$$\n",
    "\\| \\ve_{n+1} \\| \\le \\rho \\| \\vs_n \\|/(1-\\rho)  \\le \\tau.\n",
    "$$\n",
    "    \n",
    "In practice, a safety factor is used on the left side of these criteria\n",
    "to guard against an underestimate.\n",
    "\n",
    "If, however, the estimate of $\\rho$ is much smaller than the actual\n",
    "q-factor, the iteration can terminate too soon. This can happen\n",
    "in practice if the Jacobian is ill conditioned and the initial iterate\n",
    "is far from the solution\n",
    "<cite data-cite=\"ctk:mike2\"><a href=\"siamfa.html#ctk:mike2\">(KMT98)</cite>.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1.6: Global Convergence and the Armijo Rule\n",
    "\n",
    "The requirement in the local convergence theory\n",
    "that the initial iterate be near the solution is\n",
    "more than mathematical pedantry. To see this, we apply\n",
    "Newton's method to find the root $x^* = 0$ of the\n",
    "function $f(x) = \\arctan(x)$ with initial iterate $x_0 = 10$. This\n",
    "initial iterate is too far from the root for the local convergence\n",
    "theory to hold. In fact, the step\n",
    "\n",
    "$$\n",
    "s = \\frac{f(x_0)}{f'(x_0)} \\approx \\frac{1.5}{-0.01} \\approx -150,\n",
    "$$\n",
    "\n",
    "while in the correct direction, is far too large in magnitude.\n",
    "\n",
    "The initial iterate and the four subsequent iterates are\n",
    "\n",
    "$$\n",
    "10, -138, 2.9 \\times 10^4, -1.5 \\times 10^9, 9.9 \\times 10^{17}.\n",
    "$$\n",
    "\n",
    "As you can see, the Newton step points in the correct direction,\n",
    "i.e., toward $x^* = 0$, but overshoots by larger and larger amounts.\n",
    "The simple artifice of reducing the step by half\n",
    "until $\\|\\mf(\\vx)\\|$ has been reduced will usually solve this problem.\n",
    "\n",
    "In order to clearly describe this, we will now\n",
    "make a distinction between the\n",
    "__Newton direction__\n",
    "$\\vd = -\\mf'(\\vx)^{-1} \\mf(\\vx)$ and the\n",
    "__Newton step__ when we discuss global\n",
    "convergence. For the methods in this book, the Newton step\n",
    "will be a positive scalar multiple of the Newton direction.\n",
    "When we talk about local convergence and are taking\n",
    "full steps ($\\lambda = 1$ and $\\vs = \\vd$), we will not make this\n",
    "distinction and only refer to the step, as we have been doing\n",
    "up to now.\n",
    "\n",
    "A rigorous convergence analysis requires a bit more detail. We\n",
    "begin by computing the __Newton direction__\n",
    "$$\n",
    "\\vd = - \\mf'(\\vx_n)^{-1} \\mf(\\vx_n).\n",
    "$$\n",
    "\n",
    "To keep the step from going too far, we find the smallest integer $m \\ge 0$\n",
    "such that the __sufficient decrease condition__\n",
    "\n",
    "$$\n",
    "\\|\\mf(\\vx_n + 2^{-m} \\vd) \\| < (1 - \\alpha 2^{-m} ) \\| \\mf(\\vx_n) \\|\n",
    "$$\n",
    "\n",
    "holds. We let the step be $\\vs = 2^{-m} \\vd$ and\n",
    "$\\vx_{n+1} = \\vx_n + 2^{-m} \\vd$.\n",
    "\n",
    "The parameter $\\alpha \\in (0,1)$ is a small\n",
    "number intended to make the sufficient decrease condition as easy as possible\n",
    "to satisfy.\n",
    "$\\alpha = 10^{-4}$ is typical and used in our codes.\n",
    "\n",
    "In the figure (Figure 1.4 in the print book) \n",
    "created by __fig1dot4.jl__,\n",
    "we show how this approach, called the\n",
    "__Armijo rule__ \n",
    "<cite data-cite=\"armijo\"><a href=\"siamfa.html#armijo\">(Arm66)</cite>,\n",
    "succeeds. The\n",
    "circled points are iterations for which $m > 1$ and the value of\n",
    "$m$ is above the circle.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1dot4();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Methods like the Armijo rule are called\n",
    "__line search__ methods because one searches\n",
    "for a decrease in $\\| \\mf \\|$ along the line segment\n",
    "$[\\vx_n, \\vx_n + \\vd]$.\n",
    "\n",
    "The line search in our codes manages the reduction in the step size\n",
    "with more sophistication than simply halving an unsuccessful step.\n",
    "The motivation\n",
    "for this is that some problems respond well to one or two reductions\n",
    "in the step length by modest amounts (such as 1/2)\n",
    "and others require many such reductions, but might do much\n",
    "better if a more\n",
    "aggressive step-length reduction (by factors of 1/10, say) is used.\n",
    "To address this possibility, after two reductions by halving do\n",
    "not lead to sufficient decrease, we build a quadratic polynomial model of\n",
    "$$\n",
    "\\phi(\\lambda) = \\|F(\\vx_n + \\lambda \\vd) \\|^2\n",
    "$$\n",
    "based on interpolation of $\\phi$ at $\\lambda=0$ (the current iterate)\n",
    "and the two most\n",
    "recent values of $\\lambda$. The next $\\lambda$ is the\n",
    "minimizer of the quadratic model, subject to the\n",
    "__safeguard__ that the reduction in $\\lambda$ be at least\n",
    "a factor of two and at most a factor of ten. So\n",
    "the algorithm generates a sequence of candidate step-length factors\n",
    "$\\{ \\lambda_m \\}$ with $\\lambda_0 = 0, \\lambda_1=1$ (a full step) and\n",
    "$$\n",
    "1/10 \\le \\lambda_{m+1}/\\lambda_m \\le 1/2.\n",
    "$$\n",
    "The norm in this equation is, at least for theory, understood to be the\n",
    "$\\ell^2$ norm and is squared to make $\\phi$ a smooth\n",
    "function that can be accurately modeled by a quadratic over\n",
    "small ranges of $\\lambda$.\n",
    "\n",
    "In the advanced codes from the subsequent chapters,\n",
    "we use the three-point parabolic model from \n",
    "<cite data-cite=\"ctk:roots\"><a href=\"siamfa.html#ctk:roots\">(Kel95)</cite>. In\n",
    "this approach, $\\lambda_1 = 1/2$. To compute $\\lambda_m$ for\n",
    "$m > 1$, a parabola is fitted to the data $\\phi(0)$,\n",
    "$\\phi(\\lambda_m)$, and\n",
    "$\\phi(\\lambda_{m-1})$. $\\lambda_m$ is the minimum of this parabola on the\n",
    "interval $[\\lambda_{m-1}/10,\\lambda_{m-1}/2]$. We refer the reader\n",
    "to \n",
    "<cite data-cite=\"ctk:roots\"><a href=\"siamfa.html#ctk:roots\">(Kel95)</cite> for the details and\n",
    "to \n",
    "<cite data-cite=\"ctk:roots\"><a href=\"siamfa.html#ctk:roots\">(Kel95)</cite>,\n",
    "<cite data-cite=\"ortega\"><a href=\"siamfa.html#ortega\">(OR70)</cite>,\n",
    "<cite data-cite=\"dens\"><a href=\"siamfa.html#dens\">(DS96)</cite>,\n",
    "<cite data-cite=\"homerstan\"><a href=\"siamfa.html#homerstan\">(EW94)</cite>\n",
    "for a discussion of other ways\n",
    "to implement a line search.\n",
    "\n",
    "In Figure 1.5 we apply the \n",
    "parabolic line search to the problem from Figure 1.4.\n",
    "As you can see iteration with the polynomial line search avoids the \n",
    "repeated stepsize reductions early in the iteration and finds the \n",
    "solution in significantly fewer function evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1dot5();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 1.6.1:  A Basic Algorithm\n",
    "\n",
    "Algorithm __nsolg__ is a general formulation of an inexact\n",
    "Newton--Armijo iteration. The methods in \n",
    "[Chapter 2](SIAMFANLCh2.ipynb) and\n",
    "[Chapter 3](SIAMFANLCh3.ipynb)\n",
    "are special cases of __nsolg__. There is\n",
    "a lot of freedom in Algorithm __nsolg__. The essential input\n",
    "arguments are the initial iterate $\\vx$, the function $\\mf$, and\n",
    "the relative and absolute termination tolerances $\\tau_a$\n",
    "and $\\tau_r$. If __nsolg__ terminates successfully,\n",
    "$\\vx$ will be the approximate solution on output.\n",
    "\n",
    "Within the algorithm, the computation of the\n",
    "Newton direction $\\vd$ can be done with direct or iterative\n",
    "linear solvers, using either the Jacobian $\\mf'(\\vx)$ or an\n",
    "approximation of it. If you use a direct solver, then\n",
    "the forcing term $\\eta$ is determined implicitly; you do\n",
    "not need to provide one. For example, if you solve the\n",
    "equation for the Newton step with a direct method, then\n",
    "$\\eta = 0$ in exact arithmetic. If you use an approximate\n",
    "Jacobian and solve with a direct method, then $\\eta$ is\n",
    "proportional to the error in the Jacobian. Knowing about\n",
    "$\\eta$ helps you understand and apply the theory, but is not\n",
    "necessary in practice if you use direct solvers.\n",
    "\n",
    "If you use an iterative linear solver, then usually\n",
    "the inexact Newton condition is the termination criterion for that\n",
    "linear solver. You'll need to make a decision about the forcing term\n",
    "in that case (or accept the defaults from a code like\n",
    "__nsol.jl__, which we describe in [Chapter 3](SIAMFANLCh3.ipynb).\n",
    "The theoretical\n",
    "requirements on the forcing term $\\eta$ are that it be safely\n",
    "bounded away from one.\n",
    "\n",
    "Having computed the Newton direction, we compute a step length\n",
    "$\\lambda$ and a step $\\vs = \\lambda \\vd$ so that the sufficient\n",
    "decrease condition holds. It's standard in line search\n",
    "implementations to use a polynomial model like the one we described\n",
    "above.\n",
    "\n",
    "The algorithm does not cover all aspects of a useful implementation.\n",
    "The number of nonlinear iterations, linear iterations, and\n",
    "changes in the step length all should be limited. Failure of any\n",
    "of these loops to terminate reasonably rapidly\n",
    "indicates that something is wrong. We list some of the potential\n",
    "causes of failure in the subsequent sections.\n",
    "\n",
    "In\n",
    "<cite data-cite=\"ctk:newton\"><a href=\"siamfa.html#ctk:newton\">(Kel03)</cite>\n",
    "the argument list began with $\\vx, \\mf$. Here, following\n",
    "Julia convention \n",
    "<cite data-cite=\"Julia20\"><a href=\"siamfa.html#Julia20\">(Pro20)</cite>,\n",
    "we put the function argument first in Algorithm __nsolg__.\n",
    "\n",
    "\n",
    "![Alg1.1](Images/Alg1dot1.png)\n",
    "\n",
    "\n",
    "The theory for Algorithm __nsolg__ is very satisfying.\n",
    "If $\\mf$ is sufficiently smooth, $\\eta$\n",
    "is safely bounded away from one,\n",
    "the Jacobians remain well conditioned throughout the iteration, and\n",
    "the sequence $\\{ \\vx_n \\}$\n",
    "remains bounded, then the iteration converges to a solution\n",
    "and, when near the solution, the convergence is as fast as the quality of\n",
    "the linear solver permits. __Theorem 1.4__ states this precisely,\n",
    "but not as generally as the results in \n",
    "<cite data-cite=\"ctk:roots\"><a href=\"siamfa.html#ctk:roots\">(Kel95)</cite>,\n",
    "<cite data-cite=\"dens\"><a href=\"siamfa.html#dens\">(DS96)</cite>,\n",
    "<cite data-cite=\"ortega\"><a href=\"siamfa.html#ortega\">(OR70)</cite>.\n",
    "The important thing that you should remember is that, for smooth $F$,\n",
    "there are only three possibilities for the iteration of\n",
    "Algorithm __nsolg__:\n",
    "\n",
    "- $\\{ \\vx_n \\}$ will converge to a solution $\\vx^*$, at which the standard\n",
    "assumptions hold,\n",
    "- $\\{ \\vx_n \\}$ will be unbounded, or\n",
    "- $\\mf'(\\vx_n)$ will become singular.\n",
    "\n",
    "\n",
    "While the line search paradigm is the simplest way to find a solution\n",
    "if the initial iterate is far from a root, other methods are available.\n",
    "__Trust region__ globalization \n",
    "<cite data-cite=\"dens\"><a href=\"siamfa.html#dens\">(DS96)</cite>,\n",
    "<cite data-cite=\"powelltreq\"><a href=\"siamfa.html#powelltreq\">(Pow70)</cite>\n",
    "is widely used for\n",
    "optimiztion problems, but less so for nonlinear equations.    \n",
    "    \n",
    "__Pseudo-transient continuation__ \n",
    "<cite data-cite=\"ctk:pst\"><a href=\"siamfa.html#ctk:pst\">(KK98)</cite>,\n",
    "<cite data-cite=\"ctk:ptc2\"><a href=\"siamfa.html#ctk:ptc2\">(CKK03)</cite>,\n",
    "<cite data-cite=\"highamptc\"><a href=\"siamfa.html#highamptc\">(Hig99)</cite>,\n",
    "<cite data-cite=\"deufptc\"><a href=\"siamfa.html#deufptc\">(Deu02)</cite>\n",
    "is designed to steer the iteration to dynamically stable solutions.\n",
    "__Homotopy__ methods \n",
    "<cite data-cite=\"bertini\"><a href=\"siamfa.html#bertini\">(BHSW13)</cite>,\n",
    "<cite data-cite=\"hompack\"><a href=\"siamfa.html#hompack\">(WBM87)</cite>\n",
    "an be deployed to find all\n",
    "solutions of nonlinear equations.\n",
    "We will cover pseudo-transient continuation in some detail the next section.\n",
    "\n",
    "---\n",
    "    \n",
    "__Theorem 1.5:__\n",
    "Let $\\vx_0 \\in R^N$ and $\\alpha \\in (0,1)$ be given.\n",
    "Assume that $\\{ \\vx_n \\}$ is given by Algorithm __nsolg__,\n",
    "$\\mf$ is Lipschitz continuously differentiable,\n",
    "    \n",
    "$$\n",
    "\\{ \\eta_n \\} \\subset (0, \\bar \\eta] \\subset (0,1-\\alpha),\n",
    "$$\n",
    "    \n",
    "and $\\{ \\vx_n \\}$ and $\\{\\| \\mf'(\\vx_n)^{-1} \\|\\}$ are bounded.\n",
    "Then $\\{ \\vx_n \\}$ converges to a root $\\vx^*$ of\n",
    "$\\mf$ at which the standard assumptions hold, full steps\n",
    "($\\lambda = 1$) are taken for\n",
    "$n$ sufficiently large, and the convergence behavior in the final phase\n",
    "of the iteration is that given by the local theory for inexact\n",
    "Newton methods (__Theorem 1.4__).\n",
    "    \n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 1.6.2: Warning!\n",
    "\n",
    "The theory for global convergence of the inexact Newton--Armijo iteration\n",
    "is only valid if $\\mf'(\\vx_n)$, or a very good approximation\n",
    "(forward difference, for example), is used to compute the step.\n",
    "A poor approximation to the Jacobian will cause the Newton step\n",
    "to be inaccurate. While this can result in slow convergence when\n",
    "the iterations are near the root, the outcome can be much worse\n",
    "when far from a solution. The reason for this is that the success\n",
    "of the line search is very sensitive to the direction.\n",
    "In particular, if\n",
    "$\\vx_0$ is far from $\\vx^*$ there is __no reason__ to expect the\n",
    "secant or chord method, even with a line search, to converge. Sometimes methods like\n",
    "the secant and chord methods work fine with a line search when\n",
    "the initial iterate is far from a solution, but users of\n",
    "nonlinear solvers should be aware that the line search can fail.\n",
    "A good code will watch for this failure and respond by\n",
    "using a more accurate Jacobian or Jacobian-vector product or reporting an error.\n",
    "\n",
    "Difference approximations to the Jacobian are usually sufficiently\n",
    "accurate. However, there are particularly hard problems\n",
    "<cite data-cite=\"kerksaad\"><a href=\"siamfa.html#kerksaad\">(KS92)</cite>\n",
    "for which differentiation in the coordinate\n",
    "directions is very inaccurate, whereas differentiation in\n",
    "the directions of the iterations, residuals, and steps,\n",
    "which are natural directions\n",
    "for the problem, is very accurate.\n",
    "The inexact Newton methods, such as the\n",
    "Newton-Krylov methods in \n",
    "[Chapter 3](SIAMFANLCh3.ipynb), use a forward\n",
    "difference approximation for Jacobian-vector products\n",
    "(with vectors that are natural for the problem) and, therefore,\n",
    "will usually (but not always) work well when far from a solution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1.7: Pseudo-Transient Continuation\n",
    "\n",
    "Nonlinear equations can have multiple solutions and even if the Newton-Armijo iteration converges to a solution, that may not be the solution you want. __Pseudo-Transient Continuation__ ($\\ptc$) is one way to differentiate solutions you want from the ones you don't.\n",
    "\n",
    "Much of this section is taken from \n",
    "<cite data-cite=\"ctk:pst\"><a href=\"siamfa.html#ctk:pst\">(KK98)</cite>\n",
    "and \n",
    "cite{ctk:acta}.\n",
    "<cite data-cite=\"ctk:acta\"><a href=\"siamfa.html#ctk:acta\">(Kel18)</cite>.\n",
    "We refer the reader to \n",
    "<cite data-cite=\"ctk:pst\"><a href=\"siamfa.html#ctk:pst\">(KK98)</cite>,\n",
    "<cite data-cite=\"ctk:ptc2\"><a href=\"siamfa.html#ctk:ptc2\">(CKK03)</cite>\n",
    "and \n",
    "<cite data-cite=\"ctk:katie2\"><a href=\"siamfa.html#ctk:katie2\">(FK05)</cite>\n",
    "for the details of the analysis.\n",
    "\n",
    "The objective is\n",
    "to find stable steady-state solutions of time-dependent problems\n",
    "$$\n",
    "\\frac{d \\vx}{dt} = -\\mv \\mf(\\vx), \\  \\vx(0) = \\vx_0.\n",
    "$$\n",
    "The minus sign on the right side of the equation is\n",
    "a convention.\n",
    "The matrix $\\mv$ is a nonsingular\n",
    "scaling matrix, usually diagonal.\n",
    "$\\mv$ plays an important role in applications \n",
    "<cite data-cite=\"keyes-smooke\"><a href=\"siamfa.html#keyes-smooke\">(KS87)</cite>\n",
    "and we include it in the theory, even though we\n",
    "set $\\mv = \\mi$ in the examples in this book.\n",
    "    \n",
    "We assume that a solution $\\vx(t)$ exists for $0 \\le t < \\infty$.\n",
    "Here $\\mf$ is a Lipschitz continuously differentiable\n",
    "function in a neighborhood of the trajectory\n",
    "$\\{ \\vx(t) \\, | \\, 0 \\le t \\le \\infty \\}$.\n",
    "We assume that a __steady-state__ solution $\\vx^*$ of the time-dependent problem\n",
    "exists. This means that\n",
    "$$\n",
    "\\lim_{t \\to \\infty} \\vx(t) = \\vx^*.\n",
    "$$\n",
    "The convergence of $\\vx(t)$ implies that\n",
    "$$\n",
    "\\frac{d \\vx^*}{dt} = 0 = - \\mf(\\vx^*).\n",
    "$$\n",
    "A steady state solution is __stable__ if\n",
    "if the solution of\n",
    "the initial value problem with initial data\n",
    "sufficiently near $\\vx^*$ converges to $\\vx^*$ as $t \\to \\infty$. We\n",
    "will only consider linear stability, which for us will mean\n",
    "that there is $\\beta > 0$ such that\n",
    "$$\n",
    "\\| (\\mi + \\delta \\mv^{-1} \\mf'(\\vx^*) \\| \\le (1 + \\beta \\delta)^{-1}\n",
    "$$\n",
    "for all $\\delta > 0$. This will imply stability, but the converse is not\n",
    "true.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 1.7.1: A Simple Example\n",
    "\n",
    "A simple ordinary differential equation makes the point.\n",
    "In this example $f(x) = (x^3 - \\lambda x)$ where $\\lambda$ is a\n",
    "parameter in the equation.\n",
    "The equation\n",
    "$$\n",
    "\\frac{d x}{dt} = - (x^3 - \\lambda x)\n",
    "$$\n",
    "has a unique steady-state solution $x \\equiv 0$ if $\\lambda \\le 0$.\n",
    "That solution is stable because $f'(0) = - \\lambda \\ge 0$. When\n",
    "$\\lambda > 0$, however, there are three steady state solutions\n",
    "$$\n",
    "x \\equiv \\pm \\sqrt{\\lambda} \\mbox{ and } x \\equiv 0.\n",
    "$$\n",
    "The two nonzero solutions are stable and $x \\equiv 0$ is not.\n",
    "If one solves $f(x) = 0$ with Newton's method the iteration is\n",
    "$$\n",
    "x_+ = \\frac{-2 x_c^3}{\\lambda - 3 x_c^2}.\n",
    "$$\n",
    "Hence Newton's method will converge to the unstable solution if\n",
    "the initial iterate $x_0$ is sufficiently small.\n",
    "\n",
    "The solution of initial value problem, on the other hand, will\n",
    "converge to one of the steady state solutions.\n",
    "Numerical integration with Euler's method\n",
    "$$\n",
    "x_{k+1} = x_k - h f(x_k) = x_k - h (x_k^3 - \\lambda x_k)\n",
    "= (1 + h \\lambda) x_k - h x_k^3\n",
    "$$\n",
    "will converge as $k \\to \\infty$ to a stable \n",
    "steady-stage solution solution if $x_0 \\ne 0$ and\n",
    "$h$ is sufficiently small.\n",
    "To see that, \n",
    "note that $x_{k+1} > x_k$ if $x_k > 0$ is small. Hence\n",
    "the numerical integration converges to the stable steady-state solution\n",
    "for sufficiently small $h$. So the problem is that Newton's method\n",
    "can converge to the unstable solution, which is not of interest, but\n",
    "a highly accurate temporal integration, while finding a steady-state\n",
    "solution, is very costly.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 1.7.2: The $\\ptc$ Algorithm\n",
    "\n",
    "The $\\ptc$ sequence $\\{ \\vx_n \\}$ is\n",
    "$$\n",
    "\\vx_{n+1} = \\vx_n - ( \\delta_n^{-1} \\mv + \\mf'(\\vx_n) )^{-1} \\mf(\\vx_n).\n",
    "$$\n",
    "If $\\delta_n$ were small and fixed, this would be a\n",
    "Rosenbrock method\n",
    "<cite data-cite=\"gear\"><a href=\"siamfa.html#gear\">(Gea71)</cite>\n",
    "for temporal integration. The objective of $\\ptc$ is fast convergece to steady-state. That\n",
    "is very different from accurate temporal integration, hence the name __pseudo-transient__\n",
    "continuation. Another way to view the iteration is as a\n",
    "single Newton step for the implicit Euler time step\n",
    "$$\n",
    "\\vx_{n+1}  = \\vx_n + \\delta_n \\mv \\mf(\\vx_{n+1}).\n",
    "$$\n",
    "This viewpoint plays an important role in the theory from\n",
    "<cite data-cite=\"ctk:todd1\"><a href=\"siamfa.html#ctk:todd1\">(CMKM03)</cite>,\n",
    "<cite data-cite=\"ctk:ptc2\"><a href=\"siamfa.html#ctk:ptc2\">(CKK03)</cite>,\n",
    "<cite data-cite=\"ctk:katie2\"><a href=\"siamfa.html#ctk:katie2\">(FK05)</cite>, and\n",
    "<cite data-cite=\"ctk:pst\"><a href=\"siamfa.html#ctk:pst\">(KK98)</cite>.\n",
    "We think of $\\ptc$ as a variable timestep algorithm that\n",
    "attempts to increase the time step as $\\| \\mf(\\vx(t)) \\|$ becomes\n",
    "small. So we manage $\\delta$ very differently from a variable-step\n",
    "initial value problem solver, where time step control is for stability\n",
    "and temporal accuracy.\n",
    "\n",
    "We will consider the simplest case for smooth $\\mf$ and ordinary\n",
    "differential equation dynamics. $\\ptc$ has also succeeded with\n",
    "differential algebraic equations and nonsmooth dynamics\n",
    "<cite data-cite=\"ctk:todd1\"><a href=\"siamfa.html#ctk:todd1\">(CMKM03)</cite>,\n",
    "<cite data-cite=\"ctk:ptc2\"><a href=\"siamfa.html#ctk:ptc2\">(CKK03)</cite>,\n",
    "<cite data-cite=\"ctk:katie2\"><a href=\"siamfa.html#ctk:katie2\">(FK05)</cite>.\n",
    "\n",
    "The formal statement of the algorithm is a bit simpler than that for\n",
    "Newton-Armijo.\n",
    "The implementation is also simpler. For Newton-Armijo,\n",
    "as we shall see, one has options in the frequency of the Jacobian\n",
    "evaluation and factorization, the nature of any approximation of the\n",
    "Jacobian, and the rules for stepsize control. $\\ptc$, on the other hand,\n",
    "needs to use the Jacobian at the current iteration to preserve the\n",
    "dynamics early in the iteration and controls $\\delta$ with a formula.\n",
    "\n",
    "We update $\\delta$ with the\n",
    "__switched evolution relaxation__ (SER) method\n",
    "<cite data-cite=\"mulder_vanleer\"><a href=\"siamfa.html#mulder_vanleer\">(ML85)</cite>\n",
    "approach which is widely used in aerodynamics\n",
    "<cite data-cite=\"Keyes\"><a href=\"siamfa.html#Keyes\">(Key95)</cite>,\n",
    "<cite data-cite=\"mcraeork\"><a href=\"siamfa.html#mcraeork\">(OM92b)</cite>,\n",
    "<cite data-cite=\"mcraeork2\"><a href=\"siamfa.html#mcraeork2\">(OM92a)</cite>,\n",
    "<cite data-cite=\"venkat\"><a href=\"siamfa.html#venkat\">(Ven89)</cite>,\n",
    "$$\n",
    "\\delta_n = \\delta_{n-1} \\|\\mf(\\vx_{n-1})\\|/\\|\\mf(\\vx_n)\\| =\n",
    "\\delta_0 \\|\\mf(\\vx_0)\\|/\\|\\mf(\\vx_n)\\|\n",
    "$$\n",
    "or\n",
    "$$\n",
    "\\delta_n =\n",
    "\\phi\\left(\\delta_{n-1}\\frac{\\|\\mf(\\vx_{n-1})\\|}{\\|\\mf(\\vx_n)\\|}\\right).\n",
    "$$\n",
    "In the SER equation\n",
    "$$\n",
    "\\phi(\\xi) = \\left\\{\\begin{array}{@{}lc@{}}\n",
    "\\xi, & \\xi \\leq \\xi_t, \\\\\n",
    "\\delta_{\\max} , & \\xi > \\xi_t, \\end{array}\\right.\n",
    "$$\n",
    "where either $\\xi_t = \\delta_{\\max}$ or $\\xi_t < \\infty$\n",
    "and $\\delta_{\\max} = \\infty$.\n",
    "The choice $\\delta_{\\max} = \\infty$ is the original form of SER.\n",
    "\n",
    "Algorithm __ptc__ has been used in aerodynamics \n",
    "<cite data-cite=\"venkat\"><a href=\"siamfa.html#venkat\">(Ven89)</cite>,\n",
    "hydrology \n",
    "<cite data-cite=\"ctk:ptcre\"><a href=\"siamfa.html#ctk:ptcre\">(FKC<sup>+</sup>03)</cite>,\n",
    "mechanics\n",
    "<cite data-cite=\"ctk:rich\"><a href=\"siamfa.html#ctk:rich\">(GLK09)</cite>, magnetohydrodynamics <cite data-cite=\"knoll1\"><a href=\"siamfa.html#knoll1\">(KR97)</cite>, radiation transport <cite data-cite=\"Shestakov00\"><a href=\"siamfa.html#Shestakov00\">(SM00)</cite>, reacting flow  <cite data-cite=\"smooke2\"><a href=\"siamfa.html#smooke2\">(SMK89)</cite>, structural analysis  <cite data-cite=\"kant90\"><a href=\"siamfa.html#kant90\">(KP90)</cite>, optimization <cite data-cite=\"ctk:ptcopt\"><a href=\"siamfa.html#ctk:ptcopt\">(KLQ<sup>+</sup>08)</cite>, and circuit simulation \n",
    "<cite data-cite=\"grasser99\"><a href=\"siamfa.html#grasser99\">(Gra99)</cite>.    \n",
    "In all of these applications,\n",
    "time-accurate integration to steady-state is far too costly to be useful.\n",
    "\n",
    "![Alg1.2](Images/Alg1dot2.png)\n",
    "    \n",
    "Unlike the Newton-Armijo algorithm, $\\ptc$ allows $\\| \\mf (\\vx) \\|$ to\n",
    "increase as the iteration progresses. \\ptc responds to such an increase\n",
    "by decreasing $\\delta$ and thereby better following transient behavior.\n",
    "This is exactly the right thing to do. Insisting on a decrease in\n",
    "$\\| \\mf(\\vx) \\|$ as Newton-Armijo does, can lead to convergence to\n",
    "to an unstable steady state\n",
    "or even stagnation at local minimum\n",
    "of $\\| \\mf \\|$ \n",
    "<cite data-cite=\"keyes-smooke\"><a href=\"siamfa.html#keyes-smooke\">(KS87)</cite>, at which $\\mf'$ is singular.\n",
    "These problems are\n",
    "common when when the time-dependent solution has complex features\n",
    "such as shocks and rarefactions on the trajectory between $\\vx_0$ and\n",
    "$\\vx^*$ (see \n",
    "<cite data-cite=\"mcraeork\"><a href=\"siamfa.html#mcraeork\">(OM92b)</cite> and\n",
    "<cite data-cite=\"mcraeork2\"><a href=\"siamfa.html#mcraeork2\">(OM92a)</cite>\n",
    "for example).\n",
    "$\\ptc$ succeeds in many of these cases by taking advantage of the\n",
    "dynamic structure of the problem and following the dynamics early in\n",
    "the iteration.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 1.7.3: Convergence Theory\n",
    "\n",
    "We begin by summarizing the assumptions from \n",
    "<cite data-cite=\"ctk:pst\"><a href=\"siamfa.html#ctk:pst\">(KK98)</cite> which\n",
    "we need for convergence.\n",
    "\n",
    "---\n",
    "__Assumption:__\n",
    "\n",
    "- The initial value problem $d \\vx/dt = - \\mf(\\vx), \\ \\vx(0)=\\vx_0$ has a steady-state\n",
    "solution $\\vx^* = lim_{t \\to \\infty} \\vx(t)$.\n",
    "\n",
    "- There is a neighborhood of the trajectory\n",
    "$\\{ \\vx(t) \\, | \\, 0 \\le t < \\infty \\}$ in which\n",
    "$\\mf'$ is uniformly Lipschitz continuous.\n",
    "\\item There is $\\beta > 0$ such that\n",
    "$$\n",
    "\\| (\\mi + \\delta \\mv^{-1} \\mf'(\\vx^*) \\| \\le (1 + \\beta \\delta)^{-1}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "The convergence result is very different from __Theorem 1.5__.\n",
    "In __Theorem 1.6__ we specify the target solution $\\vx^*$ in advance\n",
    "and link $\\vx_0$ to $\\vx^*$ with the dynamics. The nonsingularity\n",
    "assumption on $\\mf(\\vx^*)$ is stronger than in the standard assumptions\n",
    "for local convergence as well.\n",
    "\n",
    "---\n",
    "__Theorem 1.6:__\n",
    "Let the $\\ptc$ Assumptions hold and assume that\n",
    "$\\delta$ is updated by SER.\n",
    "Let $\\{ \\vx_n \\}$ be the iteration from Algorithm __ptc__.\n",
    "Assume that $\\eta_n \\le {\\bar \\eta}$ for all $n$. Then if $\\delta_0$\n",
    "and ${\\bar \\eta}$ are sufficiently small, then\n",
    "$\\vx_n \\to \\vx^*$ and $\\delta_n \\to \\delta_{max}$. Moreover,\n",
    "for $n$ sufficiently large,\n",
    "\n",
    "$$\n",
    "\\| \\ve_{n+1} \\| = O{ ( \\eta_n + \\delta_n^{-1} ) \\| \\ve_n \\|\n",
    "+ \\| \\ve_n \\|^2}.\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 1.7.4: Computational Example\n",
    "\n",
    "We illustrate the effects of $\\ptc$ with the example from\n",
    "Section 1.7.1. We compare a Newton iteration to\n",
    "a $\\ptc$ iteration for $\\lambda = .5$ with $x_0 = .1$. \\ptc will\n",
    "move away from the unstable solution $x^*_{us} = 0$ and converge\n",
    "to the stable solution $x^* = \\sqrt{1/2}$.\n",
    "\n",
    "The reader must be aware that\n",
    "$\\ptc$ __is NOT a general-purpose nonlinear solver__. We will return\n",
    "to this theme in \n",
    "[Chapter 2](SIAMFANLCh2.ipynb) and\n",
    "[Chapter 3](SIAMFANLCh3.ipynb).\n",
    "$\\ptc$ was\n",
    "invented to find steady-state solutions of time-dependent problems.\n",
    "The price for that ability is that $\\ptc$ generally needs many more iterations\n",
    "to do its work and must (in both theory and practice) use an accurate\n",
    "Jacobian, not, for example, the Jacobian from a prior iteration.\n",
    "\n",
    "Figure 1.6 compares the convergence of $\\ptc$\n",
    "and Newton's method on the example problem\n",
    "$f(x) = (x^3 - \\lambda x)$ from Section 1.7.1. Here we take\n",
    "$\\lambda = 1/2$ and $x_0 = .1$. The $\\ptc$ iteration will converge to\n",
    "the postive steady-state solution $x^* = \\sqrt{1/2}$ while Newton's method\n",
    "converges to the unstable solution $x^* = 0$. We plot both the residual\n",
    "norm and the error. There are two things to mention in this plot.\n",
    "\n",
    "\n",
    "- $\\ptc$ takes many more iterations to converge. This is because it\n",
    "accurately tracks the dynamics early in the iteration and takes several\n",
    "steps as it does that.\n",
    "\n",
    "- The two plots look\n",
    "very similar because the derivative of $f$ at either of the two solutions\n",
    "is neither near zero nor too large.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1dot6();\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 1.7 shows how the choice of $dt_0$ affects\n",
    "the convergence. In this example reducing $dt_0$ forces a more\n",
    "accurate and costly resolution of the transients in the dynamics.\n",
    "Since our choice of $dt_0 = .1$ found the correct steady-state solution,\n",
    "smaller values of $dt_0$ only result in wasted effort. Of course, a value\n",
    "of $dt_0$ that is too large will lead to convergence to the unstable\n",
    "solution, as we will see in [Section 1.10](SIAMFANLCh1a.ipynb).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1dot7();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1.8: Things to Consider\n",
    "\n",
    "Here is a short list of things to think about when you select\n",
    "and use a nonlinear solver.\n",
    "\n",
    "### Human Time and Public Domain Codes\n",
    "\n",
    "When you select a nonlinear solver for your problem, you need\n",
    "to consider not only the computational cost (in time and\n",
    "storage) but also __YOUR TIME__. A fast code for your problem\n",
    "that takes ten years to write has little value.\n",
    "\n",
    "Unless your problem is very simple, or you're an expert in\n",
    "this field, your best bet is to use a\n",
    "open source or\n",
    "public domain code.\n",
    "The Julia codes that accompany this book\n",
    "are a good start and\n",
    "can be used for small- to medium-scale production work.\n",
    "The Matlab codes from \n",
    "<cite data-cite=\"ctk:roots\"><a href=\"siamfa.html#ctk:roots\">(Kel95)</cite> and\n",
    "<cite data-cite=\"ctk:newton\"><a href=\"siamfa.html#ctk:newton\">(Kel03)</cite>\n",
    "serve the same purpose.\n",
    "However, if you need support for other languages\n",
    "(meaning C, C++, or FORTRAN) or leadership-class\n",
    "computing environments at the national labortories, there are several\n",
    "sources for public domain implementations of the algorithms\n",
    "in this book.\n",
    "\n",
    "The Newton--Krylov solvers we discuss in [Chapter 3](SIAMFANLCh3.ipynb)\n",
    "are at present (2020) the solvers\n",
    "of choice for most large problems on advanced computers. Therefore,\n",
    "these algorithms are getting most of the attention from\n",
    "the people who build libraries. The NOX solver in the\n",
    "Trilinos framework \n",
    "<cite data-cite=\"trilinos\"><a href=\"siamfa.html#trilinos\">(HBH<sup>+</sup>05)</cite>,    \n",
    "the\n",
    "SNES  solver in the\n",
    "PETSc library \n",
    "<cite data-cite=\"petsc-web-page\"><a href=\"siamfa.html#petsc-web-page\">(BAA<sup>+</sup>19)</cite>,    \n",
    "and the NITSOL\n",
    "<cite data-cite=\"nitsol\"><a href=\"siamfa.html#nitslo\">(PW98)</cite>,\n",
    "NKSOL\n",
    "<cite data-cite=\"brown/saad90\"><a href=\"siamfa.html#brown/saad90\">(BS90)</cite>,\n",
    "and KINSOL\n",
    "<cite data-cite=\"kinsol\"><a href=\"siamfa.html#kinslo\">(TH98)</cite> \n",
    "codes\n",
    "are good implementations. KINSOL has been ported to Julia as part\n",
    "of the [SUNDIALS.jl](https://github.com/SciML/Sundials.jl) package\n",
    "<cite data-cite=\"sundials\"><a href=\"siamfa.html#sundials\">(HBG<sup>+</sup>05)</cite>,\n",
    "<cite data-cite=\"dejl\"><a href=\"siamfa.html#dejl\">(RN17)</cite>.\n",
    "    \n",
    "The methods from [Chapter 2](SIAMFANLCh2.ipynb), which are based on direct\n",
    "factorizations, have received less attention recently and many of the codes are quite old.   \n",
    "Some\n",
    "careful implementations can be found in the MINPACK and UNCMIN\n",
    "libraries. The MINPACK\n",
    "<cite data-cite=\"minpack1\"><a href=\"siamfa.html#minpack1\">(MGH80)</cite>\n",
    "library is a suite of FORTRAN\n",
    "codes that includes an implementation of Newton's method\n",
    "for dense Jacobians. The globalization is via a trust region\n",
    "approach \n",
    "<cite data-cite=\"dens\"><a href=\"siamfa.html#dens\">(DS96)</cite>,\n",
    "<cite data-cite=\"powelltreq\"><a href=\"siamfa.html#powelltreq\">(Pow70)</cite>,      \n",
    "rather than the line search method we use here. The UNCMIN \n",
    "<cite data-cite=\"uncmin\"><a href=\"siamfa.html#uncmin\">(SKW85)</cite> library is based\n",
    "on the algorithms from\n",
    "<cite data-cite=\"dens\"><a href=\"siamfa.html#dens\">(DS96)</cite>and includes a Newton--Armijo\n",
    "nonlinear equations solver.\n",
    "MINPACK and several other codes for solving nonlinear equations\n",
    "are available from the [NETLIB](http://www.netlib.org)\n",
    "repository. Some of the\n",
    "NETLIB codes are very old and may not be maintained. \n",
    "    \n",
    "The solvers in [SIAMFANLEquations](https://github.com/ctkelley/SIAMFANLEquations.jl) are designed to support\n",
    "this book project and help the reader understand how to use the algorithms and explore the options. This is\n",
    "a very different mission from that of the other nonlinear solver packages in Julia. Three of the most complete packages are Sundials.jl, BifurcationKit.jl, and NLsolve.jl.\n",
    "At the high end, KINSOL is part of the \n",
    "Julia interface to Sundials, [Sundials.jl](https://github.com/SciML/Sundials.jl)\n",
    "<cite data-cite=\"dejl\"><a href=\"siamfa.html#dejl\">(RN17)</cite>,\n",
    "<cite data-cite=\"sundials\"><a href=\"siamfa.html#sundials\">(HBG<sup>+</sup>05)</cite>.\n",
    "As we said above, Sundials is a suite of solvers from Lawrence Livermore National Laboratory that is designed for scalable performance on high-end supercomputers. This is a well-done and important project, but not one designed for a novice to understand.\n",
    "The pathfollowing and bifurcation analysis package \n",
    "[BifurcationKit.jl](https://github.com/rveltz/BifurcationKit.jl)\n",
    "<cite data-cite=\"bifkit\"><a href=\"siamfa.html#bifkit\">(Vel20)</cite>\n",
    "has speical-purpose nonlinear solvers that communicate well with continuation algorithms.\n",
    "Solvers like [NLsolve.jl](https://github.com/JuliaNLSolvers/NLsolve.jl)\n",
    "<cite data-cite=\"nlsjl\"><a href=\"siamfa.html#nlsjl\">(Mog20)</cite>\n",
    "are highly abstracted and very general. The Julia ecosystem has many codes like this for all kinds of things.\n",
    "They are very useful but hard to learn from. __NLSolve.jl__ seems to be based on \n",
    "<cite data-cite=\"dens\"><a href=\"siamfa.html#dens\">(DS96)</cite>.\n",
    "\n",
    "Jacobians and even directional derivatives are difficult or\n",
    "impossible to obtain in some applications. The SCF (self-consistent\n",
    "field) iteration in electronic\n",
    "structure computations (see <cite data-cite=\"dedrichszeller\"><a href=\"siamfa.html#dedrichszeller\">(DZ83)</cite>,\n",
    "<cite data-cite=\"linbook\"><a href=\"siamfa.html#linbook\">(LL19)</cite>,\n",
    "<cite data-cite=\"Saad2010\"><a href=\"siamfa.html#Saad2010\">(SCS10)</cite>,    \n",
    "and the references in these papers) in computational physics and\n",
    "chemistry is one such example. Anderson acceleration \n",
    "<cite data-cite=\"Anderson1965\"><a href=\"siamfa.html#Anderson1965\">(And65)</cite>,\n",
    "which we cover in [Chapter 4](SIAMFANLCh4.ipynb)\n",
    "is a one approach for\n",
    "such problems. There are good implementations of Anderson acceleration in SUNDIALS and Trilinos.\n",
    "\n",
    "There is an implementation of Broyden's method, covered in [Chapter 5](SIAMFANLCh5.ipynb), in\n",
    "UNCMIN. This implementation is based\n",
    "on dense matrix methods. The Julia implementation\n",
    "that accompanies this book requires much less storage and\n",
    "computation. Our Julia implementation and the one in Trilinos are based on \n",
    "<cite data-cite=\"ctk:roots\"><a href=\"siamfa.html#ctk:roots\">(Kel95)</cite>.    \n",
    "    \n",
    "\n",
    "### The Initial Iterate\n",
    "    \n",
    "Picking an initial iterate at random or without thinking (the famous ``initial guess'')\n",
    "is a bad idea. Some\n",
    "problems come with a good initial iterate. However, it is\n",
    "usually your job to create one that has as many properties\n",
    "of the solution as possible. Thinking about the problem\n",
    "and the qualitative properties of the solution while\n",
    "choosing the initial iterate can ensure that the solver converges\n",
    "more rapidly and avoids solutions that are not the ones you\n",
    "want.\n",
    "\n",
    "In some applications the initial iterate is known to be good, so\n",
    "methods like the chord, the secant, and Broyden's method become\n",
    "very attractive, since the problems with the line search discussed in\n",
    "in this chapter\n",
    "are not an issue. Two examples of this\n",
    "are implicit methods for temporal integration,\n",
    "in which the initial iterate is the output of a predictor, and\n",
    "nested iteration, where problems\n",
    "such as differential equations are solved on a coarse mesh and the\n",
    "initial iterate for the solution on finer meshes is an interpolation\n",
    "of the solution from a coarser mesh.\n",
    "\n",
    "It is more common to have a little information about the solution in\n",
    "advance, in which case one should try to exploit those data\n",
    "about the solution. For example, if your problem is a discretized\n",
    "differential equation, make sure that any boundary conditions\n",
    "are reflected in your initial iterate. If you know the signs of\n",
    "some components of the solution, be sure that the signs of\n",
    "the corresponding components of the initial iterate agree with\n",
    "those of the solution.\n",
    "\n",
    "### Computing the Newton Step\n",
    "    \n",
    "If function and Jacobian evaluations are very costly, the\n",
    "Newton--Krylov methods from [Chapter 3](SIAMFANLCh3.ipynb)\n",
    "Broyden's method from [Chapter 5](SIAMFANLCh5.ipynb) are worth\n",
    "exploring. Both methods avoid explicit computation of\n",
    "Jacobians, but usually require preconditioning.\n",
    "\n",
    "For very large problems, storing a Jacobian is difficult\n",
    "and factoring one may be impossible. Low-storage Newton--Krylov\n",
    "methods, such as Newton-BiCGSTAB, may be the only choice.\n",
    "Even if the storage is available,\n",
    "factorization of the Jacobian is usually a poor choice for\n",
    "very large problems, so it is worth considerable effort\n",
    "to build a good preconditioner for an iterative method.\n",
    "If these efforts fail and the linear iteration fails to\n",
    "converge, then you must either reformulate the problem or\n",
    "find the storage for a direct method.\n",
    "\n",
    "A direct method is not always the best choice for a small\n",
    "problem, though.  Integral equations, such as the example we use in the subsequent chapters,\n",
    "are one type for which\n",
    "iterative methods perform better than direct methods even for\n",
    "problems with small numbers of unknowns and dense Jacobians.\n",
    "\n",
    "### Choosing a Solver\n",
    "    \n",
    "The most important issues in selecting a solver are\n",
    "\n",
    "- the size of the problem,\n",
    "- if derivative information is possible to get or approximate,\n",
    "- the cost of evaluating $\\mf$ and $\\mf'$, and\n",
    "- the way linear systems of equations will be solved.\n",
    "\n",
    "The items in the list above are not independent.\n",
    "\n",
    "The reader in a hurry could use the outline\n",
    "below and probably do well.\n",
    "\n",
    "- If $N$ is small and evaluation of $\\mf$ is cheap,\n",
    "computing $\\mf'$ with forward differences and using direct solvers\n",
    "for linear algebra makes sense. The methods from [Chapter 2](SIAMFANLCh2.ipynb)\n",
    "are a good choice. These methods\n",
    "are probably the optimal choice in terms of saving your time.\n",
    "\n",
    "- If the Jacobian is sparse and you can compute it by hand, then\n",
    "storing it in sparse matrix format is all you need to do to inform\n",
    "__nsol.jl__ that it should use sparse linear solvers. You should also exploit any\n",
    "special structure you can. For example,\n",
    "if the Jacobian is banded with a small bandwidth, you should exploit\n",
    "that and store the Jacobian as a banded matrix rather than a general sparse\n",
    "matrix. We use the package\n",
    "[BandedMatrices.jl](https://github.com/JuliaMatrices/BandedMatrices.jl) \n",
    "<cite data-cite=\"bandedjl\"><a href=\"siamfa.html#bandedjl\">(Olv20)</cite>\n",
    "in an example in\n",
    "__Section 2.7.3__.\n",
    "\n",
    "Sparse differencing can be done in considerable generality\n",
    "<cite data-cite=\"colmore\"><a href=\"siamfa.html#colmore\">(CM83)</cite>,\n",
    "<cite data-cite=\"curtispr\"><a href=\"siamfa.html#curtisor\">(CPR74)</cite>.\n",
    "If you can exploit sparsity in the\n",
    "Jacobian, you will save a significant amount of work in the\n",
    "computation of the Jacobian and may be able to use a direct\n",
    "solver. We do not include sparse differencing in our solvers\n",
    "because we believe that is best left to the user and we want\n",
    "to limit the dependencies in the\n",
    "__SIAMFANLEquations.jl__ package.\n",
    "The __SparseDiffTools.jl__ \n",
    "<cite data-cite=\"sparsediff\"><a href=\"siamfa.html#sparsediff\">(RMGH20)</cite>\n",
    "package is a comprehensive suite of sparse differencing\n",
    "tools.\n",
    "    \n",
    "\n",
    "You should know that a sparse factorization may have a large storage\n",
    "cost and that allocating that memory, especially in Julia, can be\n",
    "very costly.  If you can obtain the\n",
    "sparsity pattern easily, preallocating that memory is a very good idea\n",
    "and our solvers insist on that. The __SparseDiffTools.jl__ package\n",
    "can help you do that if you do not know the sparsity pattern.\n",
    "\n",
    "Sparse direct solvers are very efficient. The Julia package\n",
    "__SuiteSparse.jl__ uses the codes from\n",
    "<cite data-cite=\"davisbook\"><a href=\"siamfa.html#davisbook\">(Dav06)</cite>,\n",
    "<cite data-cite=\"umfpack\"><a href=\"siamfa.html#umfpack\">(Dav04)</cite>.\n",
    "We will say more about using these methods in [Chapter 2](SIAMFANLCh2.ipynb).\n",
    "\n",
    "- If $N$ is large or computing and storing $\\mf'$ is very expensive,\n",
    "you may not be able to use a direct method.\n",
    "    \n",
    "    - If you can't compute or store $\\mf'$ at all,  but can compute\n",
    "or approximate matrix-vector products, then\n",
    "the matrix-free methods\n",
    "in [Chapter 3](SIAMFANLCh3.ipynb) and\n",
    "[Chapter 5](SIAMFANLCh5.ipynb)\n",
    "may be your best\n",
    "options.\n",
    "If you have a good preconditioner, a Newton--Krylov code\n",
    "is a good start. The discussion in [Chapter 3](SIAMFANLCh3.ipynb) will\n",
    "help you choose a Krylov method. Broyden's method can also use a\n",
    "good preconditioner.\n",
    "    \n",
    "    -  If $\\mf'$ is sparse, but you are not able to store the sparse\n",
    "factorization, you may still be able to exploit that structure with\n",
    "an incomplete factorization\n",
    "<cite data-cite=\"ilu\"><a href=\"siamfa.html#ilu\">(Saa96)</cite>\n",
    "preconditioner or an in-place\n",
    "solve which may require significantly less storage.\n",
    "\n",
    "    - If computing derivative information is too costly or impossible,\n",
    "then consider Anderson acceleration, which we discuss in [Chapter 4](SIAMFANLCh4.ipynb). \n",
    "While Newton-Krylov\n",
    "should be more efficient if you have a good \n",
    "preconditioner <cite data-cite=\"ctk:coupling\"><a href=\"siamfa.html#ctk:coupling\">(HBC<sup>+</sup>16)</cite>, there are\n",
    "important applications where Newton-Krylov methods are not practical.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1.9: What Can Go Wrong?\n",
    "\n",
    "Even the best and most robust codes can (and do) fail in practice.\n",
    "In this section we give some guidance that may help you troubleshoot\n",
    "your own solvers or interpret\n",
    "hard-to-understand results from solvers written\n",
    "by others. These are some problems that can\n",
    "arise for all choices of methods. We will also repeat some of\n",
    "these things in subsequent chapters, when we discuss problems\n",
    "that are specific to a method for approximating the Newton direction.\n",
    "\n",
    "### Nonsmooth Functions\n",
    "\n",
    "Most nonlinear equation codes, including the ones that\n",
    "accompany this book, are intended to solve problems for\n",
    "which $\\mf'$ is Lipschitz continuous. The codes will behave\n",
    "unpredictably if your function is not Lipschitz continuously\n",
    "differentiable. If, for example, the code for your function\n",
    "contains\n",
    "\n",
    "-  nondifferentiable functions such as the absolute value,\n",
    "a vector norm, or a fractional power,\n",
    "\n",
    "- internal interpolations from tabulated data,\n",
    "\n",
    "- randomized algorithms, such as Monte Carlo simulations,\n",
    "\n",
    "- control structures like _case_ or _if-then-else_\n",
    "that govern the value returned by $\\mf$; or\n",
    "\n",
    "- calls to other codes,\n",
    "\n",
    "then you may well have a nondifferentiable problem.\n",
    "\n",
    "If your function is close to a smooth function, the codes\n",
    "may do very well. On the other hand, a nonsmooth nonlinearity\n",
    "can cause any of the failures listed in this section.\n",
    "\n",
    "There are generalizations of Newton's method for broad\n",
    "classes of nonsmooth problems\n",
    "<cite data-cite=\"qisun\"><a href=\"siamfa.html#qisun\">(QS93)</cite>,\n",
    "but those methods, while very much worth learning,\n",
    "are far beyond the scope of this book.\n",
    "\n",
    "\n",
    "### Failure to Converge\n",
    "    \n",
    "The theory for Newton's method,\n",
    "as stated in __Theorem 1.4__, does not\n",
    "imply that the iteration will converge, only that nonconvergence\n",
    "can be identified easily. So,\n",
    "if the iteration fails to converge to a root, then\n",
    "either the iteration will become unbounded or the Jacobian will\n",
    "become singular. Pseudo-transient continuation and Anderson acceleration\n",
    "can also fail to converge. When this happens, you may have to reformulate\n",
    "your problem, add more/better physics, and think about the units you're\n",
    "using.\n",
    "    \n",
    "#### Inaccurate Function Evaluation\n",
    "    \n",
    "Most nonlinear solvers, including the ones that accompany this\n",
    "book, assume that the errors in the evaluation are on the order\n",
    "of machine roundoff and, therefore, use a difference increment\n",
    "of $\\approx 10^{-7}$ for finite difference Jacobians and\n",
    "Jacobian-vector products. If the error in your function evaluation\n",
    "is larger than that, the Newton direction can be poor enough\n",
    "for the iteration to fail. \n",
    "Thinking about the errors in your function\n",
    "and, if necessary,\n",
    "changing the difference increment in the solvers will usually\n",
    "solve this problem. __nsol.jl__ lets you change $dx$ if you know\n",
    "the error in $\\mf$ is large.\n",
    "    \n",
    "Of course, if your errors are in the formulation rather than\n",
    "in the computation of the function, you may well happily converge\n",
    "to an incorrect result. \n",
    "    \n",
    "#### No Solution\n",
    "\n",
    "If your problem has no solution, then any solver will have\n",
    "trouble. The clear symptoms of this are divergence of the\n",
    "iteration to infinity or failure of the residual to converge to zero.\n",
    "The causes in practice are less clear; errors in programming\n",
    "(a.k.a. bugs) are the likely source. If $\\mf$ is a model\n",
    "of a physical problem, the model itself may be wrong.\n",
    "The algorithm for computing $\\mf$, while technically\n",
    "correct, may have been realized in a way that destroys\n",
    "the solution. For example, internal tolerances to algorithms\n",
    "within the computation of $\\mf$ may be too loose, internal\n",
    "calculations based on table lookup and interpolation\n",
    "may be inaccurate, and if-then-else constructs can make $F$\n",
    "nondifferentiable.\n",
    "\n",
    "For example, if $f(x) = e^{-x}$, then the Newton iteration will diverge to\n",
    "$+\\infty$ from any starting point. If $f(x) = x^2 + 1$, the\n",
    "Newton--Armijo iteration will converge to $0$, the minimum of\n",
    "$|f(x)|$, which is not a root.\n",
    "    \n",
    "#### Singular Jacobian\n",
    "    \n",
    "The case where $\\mf'$ approaches singularity is particularly\n",
    "dangerous. In this case the step lengths approach zero, so if\n",
    "one terminates when the step is small and fails to check that\n",
    "$\\mf$ is approaching zero, one can incorrectly conclude that a root has\n",
    "been found. An example in [Chapter 2](SIAMFANLCh2.ipynb)  illustrates\n",
    "how an unfortunate choice of initial iterate can lead to\n",
    "this behavior.\n",
    "\n",
    "If $\\mf'(\\vx^*)$ is singular the local convergence theory does not\n",
    "hold. However the iteration may still converge\n",
    "<cite data-cite=\"ctk:n1\"><a href=\"siamfa.html#ctk:n1\">(DK80)</cite>,\n",
    "but\n",
    "not superlinearly. The equation $f(x) = x^2 = 0$ is a good example of this\n",
    "In this example $x^* = 0$ and for any $x_c \\ne 0$, $x_+ = x_c/2$. Hence\n",
    "if the initial iterate is non zero, the iteration will converge with\n",
    "q-factor $1/2$.\n",
    "    \n",
    "#### Alternatives to Newton-Armijo\n",
    " \n",
    "If you find that a Newton--Armijo code fails for your\n",
    "problem, there are alternatives to line search globalization that,\n",
    "while complex and often more costly, can be more robust than Newton--Armijo.\n",
    "Among these are trust region\n",
    "<cite data-cite=\"dens\"><a href=\"siamfa.html#dens\">(DS96)</cite>,\n",
    "<cite data-cite=\"powelltreq\"><a href=\"siamfa.html#powelltreq\">(Pow70)</cite>\n",
    "and\n",
    "homotopy methods\n",
    "<cite data-cite=\"hompack\"><a href=\"siamfa.html#hompack\">(WBM87)</cite>,\n",
    "<cite data-cite=\"bertini\"><a href=\"siamfa.html#bertini\">(BHSW13)</cite>.\n",
    "\n",
    "\n",
    "The field of computational algebraic geometry seeks to find all\n",
    "solutions of a polynomial system. There is software for this\n",
    "<cite data-cite=\"bertini\"><a href=\"siamfa.html#bertini\">(BHSW13)</cite>.\n",
    "Pseudo-transient continuation <cite data-cite=\"ctk:pst\"><a href=\"siamfa.html#ctk:pst\">(KK98)</cite>\n",
    "is also an alternative and we provide codes for that.\n",
    "If these methods fail, you should see if you've made a modeling\n",
    "error and thus posed a problem with no solution. \n",
    "\n",
    "### Failure of the Line Search\n",
    " \n",
    "If the line search reduces the step size to an unacceptably small\n",
    "value and the Jacobian is not becoming singular, then the quality of\n",
    "the Newton direction is poor. We repeat the caution that\n",
    "the theory for convergence of the\n",
    "Armijo rule depends on using an accurate Jacobian (analytic or forward difference). A difference\n",
    "approximation to a Jacobian or Jacobian-vector product\n",
    "is usually, but not always, sufficient.\n",
    "\n",
    "The difference increment in a\n",
    "forward difference approximation to a Jacobian or a Jacobian-vector\n",
    "product should be a bit more than the square root of the error in the\n",
    "function. Our codes use $h = 10^{-7}$, which is a good choice unless\n",
    "the function contains components such as a table lookup or output\n",
    "from an instrument that would reduce the accuracy. Central difference\n",
    "approximations, where the optimal increment is roughly the cube root of the\n",
    "error in the function, might (rarely) improve the performance of the solver, but\n",
    "for large problems the cost, twice that of a forward difference,\n",
    "is rarely justified. If you're using a direct method to compute the Newton step,\n",
    "an analytic Jacobian may make the line search perform\n",
    "better.\n",
    "Finally, one should __scale__ the finite difference\n",
    "increment to reflect the size of $\\vx$ (see [Chapter 2](SIAMFANLCh2.ipynb)).\n",
    "\n",
    "Failure of the line search in a Newton--Krylov iteration or the solution of the \n",
    "least squares problem in Anderson acceleration \n",
    "may be a symptom of loss of orthogonality in the linear solver. See\n",
    "[Chapter 3](SIAMFANLCh3.ipynb) and [Chapter 4](SIAMFANLCh4.ipynb) for more about this problem.\n",
    "    \n",
    "### Slow Convergence\n",
    "    \n",
    "If you use Newton's method and observe slow convergence,\n",
    "the chances are good that the Jacobian, Jacobian-vector\n",
    "product, or linear solver is inaccurate.\n",
    "The local superlinear convergence results from\n",
    "__Theorem 1.1__  and __Theorem 1.3__ only hold if the\n",
    "correct linear system is solved to high accuracy.\n",
    "\n",
    "If you expect to see superlinear convergence, but do not, you\n",
    "might consider these things:\n",
    "\n",
    "\n",
    "- If the errors in $\\mf$ are significantly larger than floating\n",
    "point roundoff, then increase the difference increment in a difference Jacobian\n",
    " to roughly the square root of the errors in\n",
    "the function <cite data-cite=\"ctk:roots\"><a href=\"siamfa.html#ctk:roots\">(Kel95)</cite>.\n",
    "\n",
    "- Check your computation of the Jacobian (by comparing it to a difference,\n",
    "for example).\n",
    "\n",
    "- If you are using a sparse-matrix code to solve for the Newton step,\n",
    "be sure that you have specified the correct sparsity pattern.\n",
    "\n",
    "- Make sure the tolerances for an iterative linear solver are set tightly\n",
    "enough to get the convergence you want. Check for errors in the\n",
    "preconditioner and try to investigate its quality.\n",
    "\n",
    "- If you are using a GMRES solver, make sure that you have not\n",
    "lost orthogonality (see [Chapter 3](SIAMFANLCh3.ipynb) ).\n",
    "    \n",
    "- Do the standard assumptions hold? In particular, is $\\mf'(\\vx^*)$ singular?    \n",
    "    \n",
    "### Multiple Solutions\n",
    "    \n",
    "In general, there is no guarantee that an equation has a unique\n",
    "solution. The solvers we discuss in this book, as well as the\n",
    "alternatives we listed in the section failure to converge, are supported by\n",
    "theory that says that either the solver will converge to a root\n",
    "or it will fail in some well-defined manner. No theory, even for \n",
    "$\\ptc$ can say\n",
    "that the iteration will converge to the solution that you want.\n",
    "We will discuss several problems in this book that\n",
    "have multiple solutions.\n",
    "\n",
    "### Storage Problems\n",
    "    \n",
    "If your problem is large and the Jacobian is dense, you may\n",
    "be unable to store that Jacobian. If your Jacobian is sparse,\n",
    "you may not be able to store the factors that the sparse\n",
    "Gaussian elimination in Julia creates. Even if you use an\n",
    "iterative method, you may not be able to store the data\n",
    "that the method needs to converge. GMRES needs a vector\n",
    "for each linear iteration, for example. \n",
    "Many computing environments,\n",
    "Julia among them, will tell you that there is not enough\n",
    "storage for your job. Julia,\n",
    "for example, will print this message:\n",
    "\n",
    "```Julia\n",
    "ERROR: OutOfMemoryError()\n",
    "```\n",
    "When this happens, you can find a way to obtain more\n",
    "memory or a larger computer, or use a solver that requires\n",
    "less storage. The Newton--Krylov methods, Anderson acceleration,\n",
    "and Broyden's method\n",
    "are good candidates for the latter.\n",
    "\n",
    "\n",
    "Other computing environments solve run-time storage problems\n",
    "with virtual memory. This means that data are sent to and from\n",
    "disk as the computation proceeds. This is called\n",
    "__paging__ and will slow down the\n",
    "computation by factors of 100 or more. This is rarely acceptable.\n",
    "Your best option is to find a computer with more memory.\n",
    "\n",
    "Modern computer architectures have complex memory hierarchies.\n",
    "The registers in the CPU are the fastest, so you do best if you\n",
    "can keep data in registers as long as possible. Below the registers\n",
    "can be several layers of cache memory. Below the cache is RAM,\n",
    "and below that is disk. Cache memory is faster than RAM,\n",
    "but much more expensive, so a cache is small. Simple things such as\n",
    "ordering loops to improve the locality of reference can speed up a\n",
    "code dramatically. Thinking about this is a good idea in\n",
    "Julia, as it is in FORTRAN or C. The discussion of loop ordering\n",
    "in <cite data-cite=\"demmel\"><a href=\"siamfa.html#demmel\">(Dem97)</cite>\n",
    "is a good place to start learning about efficient\n",
    "programming for computers with memory hierarchies.\n",
    "    \n",
    "Memory allocation is expensive in most computing environments and Julia\n",
    "is particularly sensitive to this. Throughout the book we will point out\n",
    "ways to avoid allocations. The more mature reader may remember having to\n",
    "do this in Fortran years ago <cite data-cite=\"linpack\"><a href=\"siamfa.html#linpack\">(DMBS79)</cite>.\n",
    "The world has not changed much\n",
    "and the parallels with Julia's use of LAPACK \n",
    "<cite data-cite=\"lapack\"><a href=\"siamfa.html#lapack\">(ABB<sup>+</sup>92)</cite>\n",
    "and the way\n",
    "people did things 40 years ago are remarkable.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Next notebook = [Section 1.10: Scalar Equation Solvers](SIAMFANLCh1s.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.5.2",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
