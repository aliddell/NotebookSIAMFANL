{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\newcommand{\\calf}{{\\cal F}}\n",
    "\\newcommand{\\dnu}{d \\nu}\n",
    "\\newcommand{\\mf}{{\\bf F}}\n",
    "\\newcommand{\\vu}{{\\bf u}}\n",
    "\\newcommand{\\vx}{{\\bf x}}\n",
    "\\newcommand{\\vy}{{\\bf y}}\n",
    "\\newcommand{\\vs}{{\\bf s}}\n",
    "\\newcommand{\\ve}{{\\bf e}}\n",
    "\\newcommand{\\vd}{{\\bf d}}\n",
    "\\newcommand{\\mg}{{\\bf G}}\n",
    "\\newcommand{\\ml}{{\\bf L}}\n",
    "\\newcommand{\\mg}{{\\bf G}}\n",
    "\\newcommand{\\ma}{{\\bf A}}\n",
    "\\newcommand{\\mi}{{\\bf I}}\n",
    "\\newcommand{\\mm}{{\\bf M}}\n",
    "\\newcommand{\\ball}{{\\cal B}}\n",
    "\\newcommand{\\ptc}{{\\Psi TC}}\n",
    "\\newcommand{\\diag}{\\mbox{diag}}\n",
    "\\newcommand{\\begeq}{{\\begin{equation}}}\n",
    "\\newcommand{\\endeq}{{\\end{equation}}}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using SIAMFANLEquations\n",
    "using SIAMFANLEquations.TestProblems\n",
    "using PyPlot\n",
    "using NotebookSIAMFANL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1: Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1.1: What is the problem?\n",
    "\n",
    "Nonlinear equations are solved as part of almost all simulations of\n",
    "physical processes. Physical models that are expressed as\n",
    "nonlinear partial differential equations, for example, become\n",
    "large systems of nonlinear equations when discretized. Authors of\n",
    "simulation codes must either use a nonlinear solver as a tool or\n",
    "write one from scratch. The purpose of this book is to show\n",
    "these authors what technology is available, sketch the implementation,\n",
    "and warn of the problems. We do this via algorithmic outlines,\n",
    "nonlinear solvers in Julia that can be used for\n",
    "production work, a suite of example problems, an IJulia notebook,\n",
    "and chapter-ending projects.\n",
    "\n",
    "We use the standard notation \n",
    "\n",
    "\\begin{equation}\n",
    "\\mf(\\vx) = 0\n",
    "\\end{equation}\n",
    "\n",
    "for systems of $N$ equations in $N$ unknowns. We will refer to this as the\n",
    "__nonlinear equations formulation__.\n",
    "Here $\\mf:R^N  \\to R^N$. We will call $\\mf$ the\n",
    "__nonlinear residual__\n",
    " or simply the __residual__.\n",
    "Rarely can the solution of a nonlinear equation be given by\n",
    "a closed-form expression, so iterative methods must be used\n",
    "to approximate the solution numerically.\n",
    "The output of an iterative method\n",
    "is a sequence of approximations to a solution.\n",
    "\n",
    "The __fixed-point__ formulation of a nonlinear equation is\n",
    "\n",
    "$$\n",
    "\\vx = \\mg(\\vx) .\n",
    "$$\n",
    "\n",
    "The difference between the two formulations is not simply replacing\n",
    "$\\mf(\\vx)$ by $\\vx - \\mg(\\vx)$. The algorithms for $\\mf(\\vx) = 0$ take\n",
    "a very different approach from those for fixed-point prolbems.\n",
    "\n",
    "We will spend\n",
    "most of our time in this introductory section on methods for the nonlinear equations formulation.\n",
    "The reason for this is that much of the theory can be explored in the\n",
    "simple context of scalar equations. We will consider fixed-point problems\n",
    "seriously in [Chapter 4](SIAMFANLCh4.ipynb).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 1.1.1: Notation\n",
    "\n",
    "\n",
    "In this book, following the convention in \n",
    "<cite data-cite=\"ctk:roots\"><a href=\"siamfa.html#ctk:roots\">(Kel95)</cite>\n",
    "and\n",
    "<cite data-cite=\"ctk:newton\"><a href=\"siamfa.html#ctk:newton\">(Kel03)</cite>,\n",
    "vectors are to be understood as column vectors.\n",
    "Following\n",
    "<cite data-cite=\"ctk:acta\"><a href=\"siamfa.html#ctk:acta\">(Kel18)</cite>\n",
    "denote vectors by boldfaced lower case letters\n",
    "and matrices by boldfaced upper case letters,\n",
    "for example $\\vx$  and $\\ma$. We denote the $i$th component of $\\vx$\n",
    "by $x_i$ to distinguish between the $i$th member of a sequence of\n",
    "vectors $\\vx_i$. We denote the $ij$ entry of $\\ma$ by $\\ma_{ij}$.\n",
    "\n",
    "The vector\n",
    "$\\vx^*$ will denote a solution, $x$ a potential solution,\n",
    "and $\\{ \\vx_n \\}_{n \\ge 0}$ the sequence of iterates. We will refer to\n",
    "$\\vx_0$ as the\n",
    "\\index{Initial!guess}\n",
    "\\index{Initial!iterate}{\\bf initial iterate (not guess!)}.\n",
    "We will denote\n",
    "the $i$th component of a vector $\\vx$ by $x_i$\n",
    "and the $i$th component of an index vector\n",
    "$\\vx_n$ by $x_{ni}$. We will rarely need\n",
    "to refer to individual components of vectors.\n",
    "We will let\n",
    "$\\partial \\mf/\\partial x_i$ denote the partial derivative of $\\mf$\n",
    "with respect to\n",
    "$x_i$. As is standard\n",
    "<cite data-cite=\"dens\"><a href=\"siamfa.html#dens\">(DS96)</cite>,\n",
    "<cite data-cite=\"ctk:roots\"><a href=\"siamfa.html#ctk:roots\">(Kel95)</cite>,\n",
    "$\\ve = \\vx - \\vx^*$ will denote the error. So, for example,\n",
    "$\\ve_n = \\vx_n - \\vx^*$ is the error in the $n$th iterate.\n",
    "    \n",
    "If the components of $\\mf$ are differentiable at $\\vx \\in R^N$,\n",
    "we define the __Jacobian matrix__\n",
    "$\\mf'(\\vx)$ by\n",
    "    \n",
    "$$\n",
    "\\mf'(\\vx)_{ij} = \\frac{\\partial f_i}{\\partial x_j} (\\vx).\n",
    "$$\n",
    "Throughout the book, $\\| \\cdot \\|$ will denote the Euclidean norm\n",
    "on $R^N$:\n",
    "    \n",
    "$$\n",
    "\\| \\vx \\| = \\left( \\sum_{i=1}^N x_i^2 \\right)^{1/2}.\n",
    "$$\n",
    "\n",
    "\n",
    "We treat scalar equations with lowercase letters. So a scalar equation\n",
    "is $f(x) = 0$ and the derivative is $f'(x)$.\n",
    "Many of the essential ideas in this book can be illustrated with scalar\n",
    "equations and we do that in this chapter. The exception is the need to\n",
    "solver linear systems of equations and linear least squares problems, which\n",
    "will be the focus of the remaining chapters in the book.\n",
    "The Julia codes for the examples in this section are in the\n",
    "[src/Chapter1](src/Chapter1)\n",
    "directory for the notebook. The solvers and\n",
    "test problems are part of the [SIAMFANLEquations](https://github.com/ctkelley/NotebookSIAMFANL) Julia package.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1.2: Newton's Method\n",
    "\n",
    "Most of the methods in this book are variations of Newton's method.\n",
    "The exception will be Anderson acceleration, a solver for fixed-point problems, which we cover in\n",
    "Chapter~\\ref{ch:anderson}.\n",
    "\n",
    "The Newton sequence is\n",
    "\n",
    "$$\n",
    "\\vx_{n+1} = \\vx_n - \\mf'(\\vx_n)^{-1} \\mf(\\vx_n).\n",
    "$$\n",
    "\n",
    "The interpretation of \\eqnok{newtonseq} is that\n",
    "we model $\\mf$ at the current iterate $\\vx_n$ with a linear\n",
    "function\n",
    "$$\n",
    "\\mm_n(x) = \\mf(\\vx_n) + \\mf'(\\vx_n) (\\vx - \\vx_n)\n",
    "$$\n",
    "and let the root of $\\mm_n$ be the next iteration.\n",
    "$\\mm_n$ is called the\n",
    "__local linear model__.\n",
    "If $\\mf'(\\vx_n)$ is\n",
    "nonsingular, then the Newton sequence is the solution of $\\mm_n(\\vx_{n+1}) = 0$.\n",
    "\n",
    "We illusrate  the local linear model and the\n",
    "Newton iteration for the scalar equation\n",
    "\\[\n",
    "\\arctan(x) = 0\n",
    "\\]\n",
    "with initial iterate $x_0 = 1$. We graph the local linear model\n",
    "\\[\n",
    "m_j(x) = f(x_j) + f'(x_j) (x - x_j)\n",
    "\\]\n",
    "at $x_j$ from the point $(x_j, y_j) = (x_j, f(x_j))$ to the next iteration\n",
    "$(x_{j+1},0)$. The iteration converges rapidly and one can see the\n",
    "linear model becoming more and more accurate. The third iterate is\n",
    "visually indistinguishable from the solution.\n",
    "\n",
    "Run the code window below to see the plot. The code __atan_test.jl__ runs the solver\n",
    "__nsolsc.jl__ from the[SIAMFANLEquations](https://github.com/ctkelley/NotebookSIAMFANL) Julia package,\n",
    "collects the data for the plot, and then runs some messy PyPlot commands. We will discuss the solver in detail\n",
    "in [Section 1.10](#Section-1.10:-Scalar-Equation-Solver). For now we will focus on the results\n",
    "\n",
    "The Julia program __fig1dot1.jl__\n",
    "creates Figure 1.1 in this chapter. The semicolon after the function call suppresses some unnecessary output from PyPlot. Remove to see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1dot1();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The computation of a Newton iteration requires\n",
    "\n",
    "1. evaluation of $\\mf(\\vx_n)$ and a test for termination,\n",
    "2. approximate solution of the equation\n",
    "$\\mf'(\\vx_n) \\vs = - \\mf(\\vx_n)$\n",
    "for the Newton step $\\vs$, and\n",
    "3. construction of $\\vx_{n+1} = \\vx_n + \\lambda \\vs$, where the\n",
    "step length $\\lambda$ is selected to guarantee decrease in $\\| \\mf \\|$.\n",
    "\n",
    "\n",
    "The computation of the Newton step,\n",
    "consumes most of the work, and the variations\n",
    "in Newton's method that we discuss in this book differ most significantly\n",
    "in how the Newton step is approximated. Computing the\n",
    "step may require evaluation and factorization of the Jacobian matrix\n",
    "or the solution of the linear equation by an iterative method.\n",
    "Not all methods for computing\n",
    "the Newton step require the complete Jacobian matrix, which, as we\n",
    "will see in [Chapter 2](SIAMFANLCh2.ipynb)\n",
    "can be very expensive.\n",
    "\n",
    "In the example from Figure 1.1, the step $s$\n",
    "in  was\n",
    "satisfactory, and we can use $\\lambda=1$ in step 3. The reader\n",
    "should be warned that attention to the step length is generally very\n",
    "important. One should not write one's own nonlinear solver without\n",
    "step-length control (see [Section 1.6](#Section-1.10:-Global_Convergence_and_the_Armijo_Rule)).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 1.2.1: Local Convergence Theory\n",
    "\n",
    "The convergence theory for Newton's method \n",
    "<cite data-cite=\"dens\"><a href=\"siamfa.html#dens\">(DS96)</cite>,\n",
    "<cite data-cite=\"ctk:roots\"><a href=\"siamfa.html#ctk:roots\">(Kel95)</cite>,\n",
    "<cite data-cite=\"ortega\"><a href=\"siamfa.html#ortega\">(OR70)</cite>,\n",
    "that is most often seen in an elementary course in numerical methods\n",
    "is __local__. This means\n",
    "that one assumes that the __initial iterate__ $\\vx_0$ is near a solution.\n",
    "The local convergence\n",
    "theory from\n",
    "<cite data-cite=\"dens\"><a href=\"siamfa.html#dens\">(DS96)</cite>,\n",
    "<cite data-cite=\"ctk:roots\"><a href=\"siamfa.html#ctk:roots\">(Kel95)</cite>,\n",
    "<cite data-cite=\"ortega\"><a href=\"siamfa.html#ortega\">(OR70)</cite>,\n",
    "requires the __standard assumptions__.\n",
    "    \n",
    "---\n",
    "__Assumption:__\n",
    "\n",
    "1. The equation $\\mf(\\vx) = 0$ has a solution $\\vx^*$.\n",
    "2. $\\mf': \\Omega \\to R^{N \\times N}$ is Lipschitz continuous near $\\vx^*$.}\n",
    "3. $\\mf'(\\vx^*)$ is nonsingular.\n",
    "---\n",
    "\n",
    "Recall that Lipschitz continuity near $\\vx^*$\n",
    "means that there is $\\gamma > 0$\n",
    "(the __Lipschitz constant__) such that\n",
    "    \n",
    "$$\n",
    "\\| \\mf'(\\vx) - \\mf'(\\vy) \\| \\le \\gamma \\| \\vx - \\vy \\|\n",
    "$$\n",
    "for all $\\vx, \\vy$ sufficiently near $\\vx^*$.\n",
    "\n",
    "We state the classic local convergence theorem.\n",
    "\n",
    "___\n",
    "__Theorem 1.1__ Let the standard assumptions hold. If $\\vx_0$ is sufficiently\n",
    "near $\\vx^*$, then the Newton sequence exists\n",
    "(i.e., $\\mf'(\\vx_n)$ is nonsingular\n",
    "for all $n \\ge 0$) and converges to $\\vx^*$ and there is $K > 0$ such that\n",
    "    \n",
    "$$\n",
    "\\| \\ve_{n+1} \\| \\le K \\| \\ve_n \\|^2\n",
    "$$\n",
    "for $n$ sufficiently large.\n",
    "___\n",
    "\n",
    "\n",
    "The convergence described by the theorem, in which\n",
    "the error in the solution will be roughly squared with each iteration,\n",
    "is called  __q-quadratic__. \n",
    "Squaring the error roughly means that the number of significant\n",
    "figures in the result doubles with each iteration.  Of course,\n",
    "one cannot examine the error without knowing the solution. However,\n",
    "we can observe the quadratic reduction in the error computationally,\n",
    "if $\\mf'(\\vx*)$ is well conditioned,\n",
    "because the nonlinear residual\n",
    "will also be roughly squared with each iteration.\n",
    "Therefore, we should see the\n",
    "exponent field of the norm of the nonlinear residual roughly double\n",
    "with each iteration.\n",
    "    \n",
    "In Table 1.1 we report the Newton iteration\n",
    "for the scalar ($N=1$) nonlinear equation\n",
    "    \n",
    "$$\n",
    "f(x) = \\tan(x) - x = 0, \\, x_0 = 4.5.\n",
    "$$\n",
    "The solution is $x^* \\approx 4.493$.\n",
    "\n",
    "The decrease in the function\n",
    "is as the theory predicts for the first three iterations, then progress\n",
    "slows down for iteration 4 and stops completely after that. The\n",
    "reason for this __stagnation__\n",
    "is clear: one cannot evaluate the function\n",
    "to higher precision than (roughly) machine unit roundoff, which in\n",
    "the IEEE\n",
    "<cite data-cite=\"ieee\"><a href=\"siamfa.html#ieee\">(IEE85)</cite>,\n",
    "<cite data-cite=\"IEEEnew\"><a href=\"siamfa.html#IEEEnew\">(IEE19)</cite>,\n",
    "<cite data-cite=\"overtonbook\"><a href=\"siamfa.html#overtonbook\">(Ove01)</cite>\n",
    "floating point system is about $10^{-16}$.\n",
    "\n",
    "We make Table 1.1 with __tab1dot1.jl__.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tab1dot1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stagnation is not affected by the accuracy in the derivative. The\n",
    "results reported in Table 1.1 used a\n",
    "forward difference approximation to the derivative with a difference\n",
    "increment of $10^{-6}$. With this choice of difference increment, the\n",
    "convergence speed of the nonlinear iteration is as fast as\n",
    "that for Newton's method, at least for this example, until stagnation\n",
    "takes over.\n",
    "The reader should be aware that difference approximations to derivatives,\n",
    "while usually reliable, are often expensive and can be very inaccurate.\n",
    "An inaccurate Jacobian can\n",
    "cause many problems (see \n",
    "[Section 1.9](#Section-1.9:-What_Can_Go_Wrong) )\n",
    "\n",
    "An analytic Jacobian\n",
    "can require some human effort, but can be worth it in terms of\n",
    "computer time and robustness when a difference Jacobian performs poorly.\n",
    "\n",
    "One can quantify this stagnation by adding the errors in the function\n",
    "evaluation and derivative evaluations to __Theorem 1.1__\n",
    "The messages of __Theorem 1.2__ are:\n",
    "\n",
    "---\n",
    "1. Small errors, for example, machine roundoff, in the function evaluation can lead to stagnation. This type of stagnation is usually benign and, if the Jacobian is well conditioned (see the estimate in [Section 1.5](#Section1.5:-Termination_of_the_Iteration) ),the results will be as accurate as the evaluation of $\\mf$\n",
    "\n",
    "2. Errors in the Jacobian and in the solution of the linear equation for the Newton step \\eqnok{nstep} will affect the speed of the nonlinear iteration, but not the limit of the sequence.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "__Theorem 1.2__ \n",
    "Let the standard assumptions hold. Let a matrix-valued\n",
    "function $\\Delta(\\vx)$ and a vector-valued function $\\epsilon(\\vx)$\n",
    "be such that\n",
    "$$\n",
    "\\| \\Delta(\\vx) \\| < \\delta_J \\mbox{ and }\n",
    "\\| \\epsilon(\\vx) \\| < \\delta_F\n",
    "$$\n",
    "for all $\\vx$ near $\\vx^*$.\n",
    "Then, if $\\vx_0$ is sufficiently near $\\vx^*$ and $\\delta_J$ and\n",
    "$\\delta_F$ are sufficiently small, the sequence\n",
    "$$\n",
    "\\vx_{n+1} = \\vx_n -\n",
    "( \\mf'(\\vx_n) + \\Delta(\\vx_n))^{-1} (\\mf(\\vx_n) + \\epsilon(\\vx_n))\n",
    "$$\n",
    "is defined (i.e., $\\mf'(\\vx_n) + \\Delta(\\vx_n)$ is nonsingular for all $n$)\n",
    "and satisfies\n",
    "$$\n",
    "\\| \\ve_{n+1} \\| \\le {\\bar K} ( \\| \\ve_n \\|^2 + \\| \\Delta(\\vx_n) \\|\n",
    "\\| \\ve_n \\|\n",
    "+ \\| \\epsilon(\\vx_n)\\| )\n",
    "$$\n",
    "for some ${\\bar K} > 0$.\n",
    "\n",
    "***\n",
    "\n",
    "We will ignore the errors in the function in the rest of this book,\n",
    "but one needs to be aware that stagnation of the nonlinear iteration\n",
    "is all but certain in finite-precision arithmetic. However, the\n",
    "asymptotic convergence results for exact arithmetic\n",
    "describe the observations well for most problems.\n",
    "\n",
    "While Table 1.1\n",
    "gives a clear picture of quadratic convergence, it's\n",
    "easier to appreciate a graph.\n",
    "Figure 1.2 is a semilog plot of\n",
    "__residual history}__ i.e.,\n",
    "the norm of the nonlinear residual against the iteration number.\n",
    "The concavity of the plot is the signature of superlinear convergence.\n",
    "One uses the __semilogy__ command from the __PyPlot__ package\n",
    "in Julia for this. See the\n",
    "file __fig1dot2.jl__, which generated Figure 1.2\n",
    "for an example.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1dot2();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1.3: Approximating the Jacobian\n",
    "\n",
    "As we will see in the subsequent chapters, it is usually most efficient\n",
    "to approximate the Newton step in some way.\n",
    "One way to do this is\n",
    "to approximate $\\mf'(\\vx_n)$ in a way that not only avoids computation\n",
    "of the derivative, but also saves linear algebra work and matrix storage.\n",
    "\n",
    "The price for such an approximation is that the nonlinear iteration\n",
    "converges more slowly; i.e., more nonlinear iterations\n",
    "are needed to solve the problem.\n",
    "However, the overall cost of the solve\n",
    "is usually significantly less, because the computation of the\n",
    "Newton step is less expensive.\n",
    "\n",
    "One way to approximate the Jacobian is to\n",
    "compute $\\mf'(\\vx_0)$ and use that as an approximation to\n",
    "$\\mf'(\\vx_n)$\n",
    "throughout the iteration.\n",
    "This is the __chord method__\n",
    "or __modified Newton method__.\n",
    "The convergence\n",
    "of the chord iteration is not as fast as Newton's method. Assuming\n",
    "that the initial iteration is near enough to $x^*$, the convergence\n",
    "is __q-linear__.\n",
    "This means that there is $\\rho \\in (0,1)$ such that\n",
    "$$\n",
    "\\| \\ve_{n+1} \\| \\le \\rho \\| \\ve_{n} \\|\n",
    "$$\n",
    "for $n$ sufficiently large.\n",
    "We can apply __Theorem: (Approximate)__ to the chord method with\n",
    "$\\epsilon=0$ and\n",
    "$\\| \\Delta(\\vx_n) \\| = O(\\| \\ve_0 \\|)$ and conclude that $\\rho$ is\n",
    "proportional to the initial error.\n",
    "The constant $\\rho$ is called the  __q-factor__.\n",
    "The formal definition of q-linear convergence allows for faster\n",
    "convergence.\n",
    "Q-quadratic convergence is also q-linear, as you can see from the\n",
    "definition.\n",
    "In many cases of q-linear convergence, one observes that\n",
    "$$\n",
    "\\| \\ve_{n+1} \\| \\approx \\rho \\| \\ve_{n} \\|\n",
    "\\mbox{ or }\n",
    "\\| \\mf(\\vx_{n+1}) \\| \\approx \\rho \\| \\mf(\\vx_{n}) \\|.\n",
    "$$\n",
    "In these cases,\n",
    "q-linear convergence is usually easy to see on a semilog plot of the\n",
    "residual norms against the iteration number. The curve appears to\n",
    "be a line with slope $\\approx \\log(\\rho)$.\n",
    "\n",
    "The __secant method__ \n",
    "for scalar equations approximates the\n",
    "derivative using a finite difference, but, rather than a forward\n",
    "difference, uses the most recent two iterations to form\n",
    "the difference quotient. So\n",
    "$$\n",
    "x_{n+1} = x_n - \\frac{f(x_n)(x_n - x_{n-1})}{f(x_n)- f(x_{n-1})},\n",
    "$$\n",
    "where $x_n$ is the current iteration and $x_{n-1}$ is the\n",
    "iteration before that. The secant method must be initialized\n",
    "with two points. One way to do that is to let\n",
    "$x_{-1} = 0.99 x_0$. This is what we do in our Julia code __nsolsc.jl__, which\n",
    "includes the Newton, chord, and secant methods as options.\n",
    "\n",
    "The formula for the secant method does not extend to\n",
    "systems of equations ($N > 1$) because the denominator in the fraction\n",
    "would be a difference of vectors. We discuss one of the\n",
    "many generalizations of the secant method for systems of equations\n",
    "in \n",
    "[Chapter 5](SIAMFANLCh5.ipynb).\n",
    "\n",
    "The secant method's approximation to $f'(x_n)$ converges to $f'(x^*)$\n",
    "as the iteration progresses. __Theorem 1.2__, with\n",
    "$\\epsilon=0$ and $\\| \\Delta(x_n) \\| = O(\\| e_{n-1} \\|)$, implies that\n",
    "the iteration\n",
    "converges __q-superlinearly__. This means that either $x_n = x^*$\n",
    "for some finite $n$ or\n",
    "$$\n",
    "\\lim_{n \\to \\infty} \\dfrac{\\| e_{n+1} \\|}{\\| e_{n} \\|} = 0.\n",
    "$$\n",
    "Q-superlinear convergence is hard to distinguish from q-quadratic\n",
    "convergence by visual inspection of the semilog plot of the residual history.\n",
    "The residual curve for q-superlinear convergence is concave down\n",
    "but drops less rapidly than the one for Newton's method.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1.4: Inexact Newton Methods\n",
    "\n",
    "Rather than approximate the Jacobian, one could instead solve\n",
    "the equation for the Newton step approximately. An\n",
    "__inexact Newton method__\n",
    "<cite data-cite=\"demboes\"><a href=\"siamfa.html#demboes\">(Des82)</cite>\n",
    "uses as a Newton step\n",
    "a vector $\\vs$ that satisfies the\n",
    "__inexact Newton condition__\n",
    "\n",
    "$$\n",
    "\\| \\mf'(\\vx_n) \\vs + \\mf(\\vx_n) \\| \\le \\eta \\| \\mf(\\vx_n) \\|.\n",
    "$$\n",
    "    \n",
    "The parameter $\\eta$ (the __forcing term__)\n",
    "can be varied as the Newton iteration progresses.\n",
    "Choosing a small value of $\\eta$ will make the iteration more like\n",
    "Newton's method, therefore leading to convergence in fewer iterations.\n",
    "However, a small value of $\\eta$ may make computing a step\n",
    "that satisfies \\eqnok{inexact} very expensive. The local convergence\n",
    "theory\n",
    "<cite data-cite=\"demboes\"><a href=\"siamfa.html#demboes\">(Des82)</cite>,\n",
    "<cite data-cite=\"ctk:roots\"><a href=\"siamfa.html#ctk:roots\">(Kel95)</cite>\n",
    "for inexact Newton methods reflects the intuitive idea that a small\n",
    "value of $\\eta$ leads to fewer iterations. __Theorem 1.3__\n",
    "is a typical example of such a convergence result.\n",
    "    \n",
    "    \n",
    "---\n",
    "__Theorem 1.3__\n",
    "Let the standard assumptions hold. Then there are $\\delta$ and $\\bar \\eta$\n",
    "such that, if $\\| \\ve_0 \\| \\le \\delta$ and\n",
    "$\\{ \\eta_n \\} \\subset [0, \\bar \\eta]$,\n",
    "then the inexact Newton iteration\n",
    "\\[\n",
    "\\vx_{n+1} = \\vx_n + \\vs_n,\n",
    "\\]\n",
    "where\n",
    "$$\n",
    "\\| \\mf'(\\vx_n) \\vs_n + \\mf(\\vx_n) \\| \\le \\eta_n \\| \\mf(\\vx_n) \\|,\n",
    "$$\n",
    "converges q-linearly to $\\vx^*$. Moreover,\n",
    "\n",
    "- if $\\eta_n \\to 0$, the convergence is q-superlinear, and\n",
    "- if $\\eta_n \\le K_\\eta \\| \\mf(\\vx_n) \\|^p$ for some\n",
    "$K_\\eta > 0$, the convergence is q-superlinear with q-order $1+p$.\n",
    "    \n",
    "___\n",
    "\n",
    "Errors in the function evaluation will, in general, lead to\n",
    "stagnation of the iteration.\n",
    "\n",
    "One can use __Theorem 1.3__ to analyze the chord method or\n",
    "the secant method. In the case of the chord method, the steps\n",
    "satisfy the inexact Newton condition with\n",
    "$$\n",
    "\\eta_n = O(\\| \\ve_0 \\|),\n",
    "$$\n",
    "which implies q-linear convergence if $\\| \\ve_0 \\|$ is sufficiently small.\n",
    "For the secant method, $\\eta_n = O(\\| \\ve_{n-1} \\|)$, implying\n",
    "q-superlinear convergence. \n",
    "    \n",
    "Iterative methods (such as GMRES\n",
    "<cite data-cite=\"gmres\"><a href=\"siamfa.html#gmres\">(SS86)</cite>) for solving\n",
    "the equation for the Newton step would typically use\n",
    "the inexact Newton condition as a termination criterion. In this case, the\n",
    "overall nonlinear solver is called a\n",
    "__Newton iterative method__.\n",
    "Newton iterative methods are named by the\n",
    "particular iterative method used for the linear equation. For\n",
    "example, the __nsoli.jl__ code,\n",
    "which we describe in [Chapter 3](SIAMFANLCh3.ipynb), is an implementation\n",
    "of some __Newton-Krylov__ methods.\n",
    "\n",
    "An unfortunate choice of the forcing term $\\eta$ can lead to\n",
    "very poor results. The reader is invited to try the two\n",
    "choices $\\eta = 10^{-6}$ and $\\eta = .9$ in __nsoli.jl__\n",
    "to see this. Better choices of $\\eta$ include $\\eta = 0.1$,\n",
    "the author's personal favorite, and a more complex approach\n",
    "(see [Chapter 3](SIAMFANLCh3.ipynb))\n",
    "from \n",
    "<cite data-cite=\"homerstan\"><a href=\"siamfa.html#homerstan\">(EW94)</cite>\n",
    "and\n",
    "<cite data-cite=\"ctk:roots\"><a href=\"siamfa.html#ctk:roots\">(Kel95)</cite>\n",
    "that is the\n",
    "default in __nsoli.jl__. Either of these  usually leads to\n",
    "rapid convergence near the solution, but at a much lower cost\n",
    "for the linear solver than a very small forcing term such as\n",
    "$\\eta = 10^{-4}$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1.5: Termination of the Iteration\n",
    "\n",
    "While one cannot know the error without\n",
    "knowing the solution, in most cases the norm of $\\mf(\\vx)$ can be used\n",
    "as a reliable indicator of the rate of decay in $\\| \\ve \\|$ as the\n",
    "iteration progresses \\cite{ctk:roots}. Based on this heuristic, we\n",
    "terminate the iteration in our codes when\n",
    "\n",
    "$$\n",
    "\\| \\mf(\\vx) \\| \\le \\tau_r \\| \\mf(\\vx_0) \\| + \\tau_a.\n",
    "$$\n",
    "\n",
    "The relative $\\tau_r$\n",
    "and absolute $\\tau_a$ error tolerances are\n",
    "both important. Using only the relative reduction in the\n",
    "nonlinear residual as a basis for termination (i.e., setting $\\tau_a = 0$)\n",
    "is a poor idea because an initial iterate that is near the solution\n",
    "may make the criterion impossible to satisfy with $\\tau_a = 0$.\n",
    "\n",
    "One way to quantify the utility of termination when\n",
    "$\\| \\mf(\\vx) \\|$\n",
    "is small is to compare a relative reduction in the norm of the\n",
    "error with a relative reduction in the norm of the nonlinear residual.\n",
    "If the standard assumptions hold and\n",
    "$x_0$ and $x$ are sufficiently near the root, then\n",
    "<cite data-cite=\"ctk:roots\"><a href=\"siamfa.html#ctk:roots\">(Kel95)</cite>\n",
    "\n",
    "$$\n",
    "\\frac{\\| \\ve \\|}{4 \\| \\ve_0 \\| \\kappa(\\mf'(\\vx^*))} \\le\n",
    "\\frac{\\| \\mf(\\vx) \\|}{\\| \\mf(\\vx_0) \\|}\n",
    "\\le \\frac{4 \\kappa(\\mf'(x^*)) \\| \\ve \\|}{\\| \\ve_0 \\|},\n",
    "$$\n",
    "    \n",
    "where\n",
    "    \n",
    "$$\n",
    "\\kappa(\\mf'(\\vx^*)) = \\| \\mf'(\\vx^*)\\| \\| \\mf'(\\vx^*)^{-1} \\|\n",
    "$$\n",
    "    \n",
    "is the\n",
    "__condition number__ of $\\mf'(\\vx^*)$ relative to the norm\n",
    "$\\| \\cdot \\|$. From the estimate above which compares relative error and\n",
    "relative resiual, we conclude that, if the\n",
    "Jacobian is well conditioned (i.e., $\\kappa(\\mf'(\\vx^*))$ is not\n",
    "very large), then our termination criterion useful.\n",
    "This is analogous to the linear case, where\n",
    "a small residual implies a small error if the matrix is well conditioned.\n",
    "    \n",
    "Another approach, which is supported by theory only for superlinearly\n",
    "convergent methods,\n",
    "is to exploit the fast convergence to estimate the error in\n",
    "terms of the step. If the iteration is converging superlinearly, then\n",
    "    \n",
    "$$\n",
    "\\ve_{n+1} = \\ve_n + \\vs_n = o(\\| \\ve_n \\|)\n",
    "$$\n",
    "    \n",
    "and hence\n",
    "    \n",
    "$$\n",
    "\\vs_n = -\\ve_n + o(\\| \\ve_n \\|).\n",
    "$$\n",
    "    \n",
    "Therefore, when the iteration is converging superlinearly, one may\n",
    "use $\\| \\vs_n \\|$ as an estimate of $\\| \\ve_n \\|$. One can estimate the\n",
    "current rate of convergence from above by\n",
    "    \n",
    "$$\n",
    "\\rho_n = \\| \\vs_n \\|/\\| \\vs_{n-1} \\| \\approx \\| \\ve_n \\|/\\| \\ve_{n-1} \\|\n",
    "\\ge \\| \\ve_{n+1} \\|/\\| \\ve_n \\|.\n",
    "$$\n",
    "    \n",
    "Hence, for $n$ sufficiently large,\n",
    "    \n",
    "$$\n",
    "\\| \\ve_{n+1} \\| \\le \\rho_n \\| \\ve_n \\| \\approx \\| \\vs_n \\|^2/\\| \\vs_{n-1}\\|.\n",
    "$$\n",
    "    \n",
    "So, for a superlinearly convergent method, terminating the iteration\n",
    "with $\\vx_{n+1}$ as soon as\n",
    "\n",
    "$$\n",
    "\\| \\vs_n \\|^2/\\|\\vs_{n-1}\\| < \\tau\n",
    "$$\n",
    "will imply that $\\| \\ve_{n+1} \\| < \\tau$. This is __termination on small steps__.\n",
    "    \n",
    "Termination on small steps\n",
    "is only supported by theory for superlinearly convergent\n",
    "methods, but is used for linearly convergent methods in some\n",
    "initial value problem solvers \n",
    "<cite data-cite=\"slc\"><a href=\"siamfa.html#slc\">(BCP96)</cite>,\n",
    "<cite data-cite=\"lsode\"><a href=\"siamfa.html#lsode\">(RH93)</cite>.\n",
    "The trick is\n",
    "to estimate the q-factor $\\rho$, say, by\n",
    "    \n",
    "$$\n",
    "\\rho \\approx \\| \\vs_n \\|/\\| \\vs_{n-1} \\|\n",
    "\\mbox{ or }\n",
    "\\rho \\approx (\\| \\vs_n \\|/\\| \\vs_0 \\|)^{1/n}.\n",
    "$$\n",
    "    \n",
    "Assuming that the estimate of $\\rho$ is reasonable, then\n",
    "    \n",
    "$$\n",
    "\\| \\ve_n \\| - \\| \\vs_n \\| \\le \\| \\ve_{n+1} \\| \\approx \\rho \\| \\ve_n \\|\n",
    "$$\n",
    "    \n",
    "implies that\n",
    "    \n",
    "$$\n",
    "\\| \\ve_{n+1} \\|/\\rho \\approx \\| \\ve_n \\| \\le \\| \\vs_n \\|/(1 - \\rho).\n",
    "$$\n",
    "    \n",
    "Hence, if we terminate the iteration when\n",
    "\n",
    "$$\n",
    "\\| \\vs_n \\| \\le \\tau (1 - \\rho)/\\rho\n",
    "$$\n",
    "    \n",
    "and the estimate of $\\rho$ is an __overestimate__, then\n",
    "the termination on small steps criterion will imply that\n",
    "    \n",
    "$$\n",
    "\\| \\ve_{n+1} \\| \\le \\rho \\| \\vs_n \\|/(1-\\rho)  \\le \\tau.\n",
    "$$\n",
    "    \n",
    "In practice, a safety factor is used on the left side of these criteria\n",
    "to guard against an underestimate.\n",
    "\n",
    "If, however, the estimate of $\\rho$ is much smaller than the actual\n",
    "q-factor, the iteration can terminate too soon. This can happen\n",
    "in practice if the Jacobian is ill conditioned and the initial iterate\n",
    "is far from the solution\n",
    "<cite data-cite=\"ctk:mike2\"><a href=\"siamfa.html#ctk:mike2\">(KMT98)</cite>.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1.6: Global Convergence and the Armijo Rule\n",
    "\n",
    "The requirement in the local convergence theory\n",
    "that the initial iterate be near the solution is\n",
    "more than mathematical pedantry. To see this, we apply\n",
    "Newton's method to find the root $x^* = 0$ of the\n",
    "function $f(x) = \\arctan(x)$ with initial iterate $x_0 = 10$. This\n",
    "initial iterate is too far from the root for the local convergence\n",
    "theory to hold. In fact, the step\n",
    "\n",
    "$$\n",
    "s = \\frac{f(x_0)}{f'(x_0)} \\approx \\frac{1.5}{-0.01} \\approx -150,\n",
    "$$\n",
    "\n",
    "while in the correct direction, is far too large in magnitude.\n",
    "\n",
    "The initial iterate and the four subsequent iterates are\n",
    "\n",
    "$$\n",
    "10, -138, 2.9 \\times 10^4, -1.5 \\times 10^9, 9.9 \\times 10^{17}.\n",
    "$$\n",
    "\n",
    "As you can see, the Newton step points in the correct direction,\n",
    "i.e., toward $x^* = 0$, but overshoots by larger and larger amounts.\n",
    "The simple artifice of reducing the step by half\n",
    "until $\\|\\mf(\\vx)\\|$ has been reduced will usually solve this problem.\n",
    "\n",
    "In order to clearly describe this, we will now\n",
    "make a distinction between the\n",
    "__Newton direction__\n",
    "$\\vd = -\\mf'(x)^{-1} \\mf(x)$ and the\n",
    "__Newton step__ when we discuss global\n",
    "convergence. For the methods in this book, the Newton step\n",
    "will be a positive scalar multiple of the Newton direction.\n",
    "When we talk about local convergence and are taking\n",
    "full steps ($\\lambda = 1$ and $\\vs = \\vd$), we will not make this\n",
    "distinction and only refer to the step, as we have been doing\n",
    "up to now.\n",
    "\n",
    "A rigorous convergence analysis requires a bit more detail. We\n",
    "begin by computing the __Newton direction__\n",
    "$$\n",
    "\\vd = - \\mf'(\\vx_n)^{-1} \\mf(\\vx_n).\n",
    "$$\n",
    "\n",
    "To keep the step from going too far, we find the smallest integer $m \\ge 0$\n",
    "such that the __sufficient decrease condition__\n",
    "\n",
    "$$\n",
    "\\|\\mf(\\vx_n + 2^{-m} \\vd) \\| < (1 - \\alpha 2^{-m} ) \\| \\mf(\\vx_n) \\|\n",
    "$$\n",
    "\n",
    "holds. We let the step be $\\vs = 2^{-m} \\vd$ and\n",
    "$\\vx_{n+1} = \\vx_n + 2^{-m} \\vd$.\n",
    "\n",
    "The parameter $\\alpha \\in (0,1)$ is a small\n",
    "number intended to make the sufficient decrease condition as easy as possible\n",
    "to satisfy.\n",
    "$\\alpha = 10^{-4}$ is typical and used in our codes.\n",
    "\n",
    "In the figure (Figure 1.4 in the print book) \n",
    "created by __fig1dot4.jl__,\n",
    "we show how this approach, called the\n",
    "__Armijo rule__ \n",
    "<cite data-cite=\"armijo\"><a href=\"siamfa.html#armijo\">(Arm66)</cite>,\n",
    "succeeds. The\n",
    "circled points are iterations for which $m > 1$ and the value of\n",
    "$m$ is above the circle.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1dot4();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Methods like the Armijo rule are called\n",
    "__line search__ methods because one searches\n",
    "for a decrease in $\\| \\mf \\|$ along the line segment\n",
    "$[\\vx_n, \\vx_n + \\vd]$.\n",
    "\n",
    "The line search in our codes manages the reduction in the step size\n",
    "with more sophistication than simply halving an unsuccessful step.\n",
    "The motivation\n",
    "for this is that some problems respond well to one or two reductions\n",
    "in the step length by modest amounts (such as 1/2)\n",
    "and others require many such reductions, but might do much\n",
    "better if a more\n",
    "aggressive step-length reduction (by factors of 1/10, say) is used.\n",
    "To address this possibility, after two reductions by halving do\n",
    "not lead to sufficient decrease, we build a quadratic polynomial model of\n",
    "$$\n",
    "\\phi(\\lambda) = \\|F(\\vx_n + \\lambda \\vd) \\|^2\n",
    "$$\n",
    "based on interpolation of $\\phi$ at the three most\n",
    "recent values of $\\lambda$. The next $\\lambda$ is the\n",
    "minimizer of the quadratic model, subject to the\n",
    "__safeguard__ that the reduction in $\\lambda$ be at least\n",
    "a factor of two and at most a factor of ten. So\n",
    "the algorithm generates a sequence of candidate step-length factors\n",
    "$\\{ \\lambda_m \\}$ with $\\lambda_0 = 1$ and\n",
    "$$\n",
    "1/10 \\le \\lambda_{m+1}/\\lambda_m \\le 1/2.\n",
    "$$\n",
    "The norm in this equation is, at least for theory, understood to be the\n",
    "$\\ell^2$ norm and is squared to make $\\phi$ a smooth\n",
    "function that can be accurately modeled by a quadratic over\n",
    "small ranges of $\\lambda$.\n",
    "\n",
    "In the advanced codes from the subsequent chapters,\n",
    "we use the three-point parabolic model from \n",
    "<cite data-cite=\"ctk:roots\"><a href=\"siamfa.html#ctk:roots\">(Kel95)</cite>. In\n",
    "this approach, $\\lambda_1 = 1/2$. To compute $\\lambda_m$ for\n",
    "$m > 1$, a parabola is fitted to the data $\\phi(0)$,\n",
    "$\\phi(\\lambda_m)$, and\n",
    "$\\phi(\\lambda_{m-1})$. $\\lambda_m$ is the minimum of this parabola on the\n",
    "interval $[\\lambda_{m-1}/10,\\lambda_{m-1}/2]$. We refer the reader\n",
    "to \n",
    "<cite data-cite=\"ctk:roots\"><a href=\"siamfa.html#ctk:roots\">(Kel95)</cite> for the details and\n",
    "to \n",
    "<cite data-cite=\"ctk:roots\"><a href=\"siamfa.html#ctk:roots\">(Kel95)</cite>,\n",
    "<cite data-cite=\"ortega\"><a href=\"siamfa.html#ortega\">(OR70)</cite>,\n",
    "<cite data-cite=\"dens\"><a href=\"siamfa.html#dens\">(DS96)</cite>,\n",
    "<cite data-cite=\"homerstan\"><a href=\"siamfa.html#homerstan\">(EW94)</cite>\n",
    "for a discussion of other ways\n",
    "to implement a line search.\n",
    "\n",
    "In Figure 1.5 we apply the \n",
    "parabolic line search to the problem from Figure 1.4.\n",
    "As you can see iteration with the polynomial line search avoids the \n",
    "repeated stepsize reductions early in the iteration and finds the \n",
    "solution in significantly fewer function evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1dot5();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 1.6.1:  A Basic Algorithm\n",
    "\n",
    "Algorithm __nsolg__ is a general formulation of an inexact\n",
    "Newton--Armijo iteration. The methods in \n",
    "[Chapter 2](SIAMFANLCh2.ipynb) and\n",
    "[Chapter 3](SIAMFANLCh3.ipynb)\n",
    "are special cases of __nsolg__. There is\n",
    "a lot of freedom in Algorithm __nsolg__. The essential input\n",
    "arguments are the initial iterate $\\vx$, the function $\\mf$, and\n",
    "the relative and absolute termination tolerances $\\tau_a$\n",
    "and $\\tau_r$. If __nsolg__ terminates successfully,\n",
    "$\\vx$ will be the approximate solution on output.\n",
    "\n",
    "Within the algorithm, the computation of the\n",
    "Newton direction $\\vd$ can be done with direct or iterative\n",
    "linear solvers, using either the Jacobian $\\mf'(\\vx)$ or an\n",
    "approximation of it. If you use a direct solver, then\n",
    "the forcing term $\\eta$ is determined implicitly; you do\n",
    "not need to provide one. For example, if you solve the\n",
    "equation for the Newton step with a direct method, then\n",
    "$\\eta = 0$ in exact arithmetic. If you use an approximate\n",
    "Jacobian and solve with a direct method, then $\\eta$ is\n",
    "proportional to the error in the Jacobian. Knowing about\n",
    "$\\eta$ helps you understand and apply the theory, but is not\n",
    "necessary in practice if you use direct solvers.\n",
    "\n",
    "If you use an iterative linear solver, then usually\n",
    "the inexact Newton condition is the termination criterion for that\n",
    "linear solver. You'll need to make a decision about the forcing term\n",
    "in that case (or accept the defaults from a code like\n",
    "__nsoli.jl__, which we describe in [Chapter 3](SIAMFANLCh3.ipynb).\n",
    "The theoretical\n",
    "requirements on the forcing term $\\eta$ are that it be safely\n",
    "bounded away from one.\n",
    "\n",
    "Having computed the Newton direction, we compute a step length\n",
    "$\\lambda$ and a step $\\vs = \\lambda \\vd$ so that the sufficient\n",
    "decrease condition holds. It's standard in line search\n",
    "implementations to use a polynomial model like the one we described\n",
    "above.\n",
    "\n",
    "The algorithm does not cover all aspects of a useful implementation.\n",
    "The number of nonlinear iterations, linear iterations, and\n",
    "changes in the step length all should be limited. Failure of any\n",
    "of these loops to terminate reasonably rapidly\n",
    "indicates that something is wrong. We list some of the potential\n",
    "causes of failure in the subsequent sections.\n",
    "\n",
    "---\n",
    "\n",
    "![Alg1.1](Images/Alg1dot1.png)\n",
    "\n",
    "\n",
    "The theory for Algorithm __nsolg__ is very satisfying.\n",
    "If $\\mf$ is sufficiently smooth, $\\eta$\n",
    "is safely bounded away from one,\n",
    "the Jacobians remain well conditioned throughout the iteration, and\n",
    "the sequence $\\{ \\vx_n \\}$\n",
    "remains bounded, then the iteration converges to a solution\n",
    "and, when near the solution, the convergence is as fast as the quality of\n",
    "the linear solver permits. T__heorem 1.4__ states this precisely,\n",
    "but not as generally as the results in \n",
    "<cite data-cite=\"ctk:roots\"><a href=\"siamfa.html#ctk:roots\">(Kel95)</cite>,\n",
    "<cite data-cite=\"dens\"><a href=\"siamfa.html#dens\">(DS96)</cite>,\n",
    "<cite data-cite=\"ortega\"><a href=\"siamfa.html#ortega\">(OR70)</cite>.\n",
    "The important thing that you should remember is that, for smooth $F$,\n",
    "there are only three possibilities for the iteration of\n",
    "Algorithm __nsolg__:\n",
    "\n",
    "- $\\{ \\vx_n \\}$ will converge to a solution $x^*$, at which the standard\n",
    "assumptions hold,\n",
    "- $\\{ \\vx_n \\}$ will be unbounded, or\n",
    "- $\\mf'(\\vx_n)$ will become singular.\n",
    "\n",
    "\n",
    "While the line search paradigm is the simplest way to find a solution\n",
    "if the initial iterate is far from a root, other methods are available\n",
    "and can sometimes overcome stagnation or, in the case of\n",
    "many solutions, find the solution that\n",
    "is appropriate to a physical problem.\n",
    "\n",
    "__Trust region__ globalization \n",
    "<cite data-cite=\"dens\"><a href=\"siamfa.html#dens\">(DS96)</cite>,\n",
    "<cite data-cite=\"powelltreq\"><a href=\"siamfa.html#powelltreq\">(Pow70)</cite>\n",
    "__pseudotransient continuation__ \n",
    "<cite data-cite=\"ctk:pst\"><a href=\"siamfa.html#ctk:pst\">(KK98)</cite>,\n",
    "<cite data-cite=\"ctk:ptc2\"><a href=\"siamfa.html#ctk:ptc2\">(CKK03)</cite>,\n",
    "<cite data-cite=\"highamptc\"><a href=\"siamfa.html#highamptc\">(Hig99)</cite>,\n",
    "<cite data-cite=\"deufptc\"><a href=\"siamfa.html#deufptc\">(Deu02)</cite>\n",
    "and \n",
    "__homotopy__ methods \n",
    "<cite data-cite=\"bertini\"><a href=\"siamfa.html#bertini\">(BHSW13)</cite>,\n",
    "<cite data-cite=\"hompack\"><a href=\"siamfa.html#hompack\">(WBM87)</cite>\n",
    "are three such alternatives. \n",
    "We will cover pseudotransient continuation in some detail the next section.\n",
    "\n",
    "---\n",
    "    \n",
    "__Theorem 1.4:__\n",
    "Let $\\vx_0 \\in R^N$ and $\\alpha \\in (0,1)$ be given.\n",
    "Assume that $\\{ \\vx_n \\}$ is given by Algorithm __nsolg__,\n",
    "$\\mf$ is Lipschitz continuously differentiable,\n",
    "    \n",
    "$$\n",
    "\\{ \\eta_n \\} \\subset (0, \\bar \\eta] \\subset (0,1-\\alpha),\n",
    "$$\n",
    "    \n",
    "and $\\{ \\vx_n \\}$ and $\\{\\| \\mf'(\\vx_n)^{-1} \\|\\}$ are bounded.\n",
    "Then $\\{ \\vx_n \\}$ converges to a root $\\vx^*$ of\n",
    "$\\mf$ at which the standard assumptions hold, full steps\n",
    "($\\lambda = 1$) are taken for\n",
    "$n$ sufficiently large, and the convergence behavior in the final phase\n",
    "of the iteration is that given by the local theory for inexact\n",
    "Newton methods (__Theorem 1.3__).\n",
    "    \n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 1.6.2: Warning!\n",
    "\n",
    "The theory for global convergence of the inexact Newton--Armijo iteration\n",
    "is only valid if $\\mf'(\\vx_n)$, or a very good approximation\n",
    "(forward difference, for example), is used to compute the step.\n",
    "A poor approximation to the Jacobian will cause the Newton step\n",
    "to be inaccurate. While this can result in slow convergence when\n",
    "the iterations are near the root, the outcome can be much worse\n",
    "when far from a solution. The reason for this is that the success\n",
    "of the line search is very sensitive to the direction.\n",
    "In particular, if\n",
    "$\\vx_0$ is far from $\\vx^*$ there is __no reason__ to expect the\n",
    "secant or chord method to converge. Sometimes methods like\n",
    "the secant and chord methods work fine with a line search when\n",
    "the initial iterate is far from a solution, but users of\n",
    "nonlinear solvers should be aware that the line search can fail.\n",
    "A good code will watch for this failure and respond by\n",
    "using a more accurate Jacobian or Jacobian-vector product.\n",
    "\n",
    "Difference approximations to the Jacobian are usually sufficiently\n",
    "accurate. However, there are particularly hard problems\n",
    "<cite data-cite=\"kerksaad\"><a href=\"siamfa.html#kerksaad\">(KS92)</cite>\n",
    "for which differentiation in the coordinate\n",
    "directions is very inaccurate, whereas differentiation in\n",
    "the directions of the iterations, residuals, and steps,\n",
    "which are natural directions\n",
    "for the problem, is very accurate.\n",
    "The inexact Newton methods, such as the\n",
    "Newton-Krylov methods in \n",
    "[Chapter 3](SIAMFANLCh3.ipynb), use a forward\n",
    "difference approximation for Jacobian-vector products\n",
    "(with vectors that are natural for the problem) and, therefore,\n",
    "will usually (but not always) work well when far from a solution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1.7: Pseudo-Transient Continuation\n",
    "\n",
    "Nonlinear equations can have multiple solutions and even if the Newton-Armijo iteration converges to a solution, that may not be the solution you want. __Pseudo-Transient Continuation__ ($\\ptc$) is one way to differentiate solutions you want from the ones you don't."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1.8: Things to Consider"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1.9: What Can Go Wrong?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Section 1.10: Scalar Equation Solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.4.1",
   "language": "julia",
   "name": "julia-1.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
