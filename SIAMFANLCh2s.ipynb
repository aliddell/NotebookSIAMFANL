{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\newcommand{\\calf}{{\\cal F}}\n",
    "\\newcommand{\\dnu}{d \\nu}\n",
    "\\newcommand{\\mf}{{\\bf F}}\n",
    "\\newcommand{\\vu}{{\\bf u}}\n",
    "\\newcommand{\\vx}{{\\bf x}}\n",
    "\\newcommand{\\vy}{{\\bf y}}\n",
    "\\newcommand{\\vs}{{\\bf s}}\n",
    "\\newcommand{\\ve}{{\\bf e}}\n",
    "\\newcommand{\\vd}{{\\bf d}}\n",
    "\\newcommand{\\mg}{{\\bf G}}\n",
    "\\newcommand{\\ml}{{\\bf L}}\n",
    "\\newcommand{\\mg}{{\\bf G}}\n",
    "\\newcommand{\\mv}{{\\bf V}}\n",
    "\\newcommand{\\ma}{{\\bf A}}\n",
    "\\newcommand{\\mi}{{\\bf I}}\n",
    "\\newcommand{\\mm}{{\\bf M}}\n",
    "\\newcommand{\\ball}{{\\cal B}}\n",
    "\\newcommand{\\ptc}{{\\Psi TC}}\n",
    "\\newcommand{\\diag}{\\mbox{diag}}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "include(\"fanote_init.jl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2.9 Solvers for Chapter 2\n",
    "\n",
    "Contents for Section 2.10\n",
    "\n",
    "[Overview](#Overview)\n",
    "\n",
    "[nsol.jl](#nsol.jl)\n",
    "\n",
    "- [H-equation Revisited](#H-Equation-revisited)\n",
    "\n",
    "- [More on the Two-Point BVP](#More-on-the-Two--Point-BVP)\n",
    "\n",
    "[ptcsol.jl](#ptcsol.jl)\n",
    "\n",
    "- [More on the Buckling Beam](#Benchmarking-the-Buckling-Beam)\n",
    "\n",
    "[Section 2.10: Projects](#Section-2.10-Projects)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will follow the pattern of Chapter 1 and present two sovlers, a Newton code and a $\\ptc$ code. Both codes are for systems of equations and use direct methods to compute the step. We returned the solution history for the simple two dimensional example in Section 2.6, but will not do that again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nsol.jl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__nsol.jl__ solves systems of nonlinear equations and computes the Newton step with direct linear solvers. Let's look at the docstrings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search: \u001b[0m\u001b[1mn\u001b[22m\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1ml\u001b[22m \u001b[0m\u001b[1mn\u001b[22m\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1ml\u001b[22msc \u001b[0m\u001b[1mn\u001b[22m\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1ml\u001b[22mheq tra\u001b[0m\u001b[1mn\u001b[22m\u001b[0m\u001b[1ms\u001b[22mc\u001b[0m\u001b[1mo\u001b[22mde tra\u001b[0m\u001b[1mn\u001b[22m\u001b[0m\u001b[1ms\u001b[22mp\u001b[0m\u001b[1mo\u001b[22mse Tra\u001b[0m\u001b[1mn\u001b[22m\u001b[0m\u001b[1ms\u001b[22mp\u001b[0m\u001b[1mo\u001b[22mse tra\u001b[0m\u001b[1mn\u001b[22m\u001b[0m\u001b[1ms\u001b[22mp\u001b[0m\u001b[1mo\u001b[22mse!\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "nsol(F!, x0, FS, FPS, J!=diffjac!; rtol=1.e-6, atol=1.e-12,\n",
       "           maxit=20, solver=\"newton\", sham=5, armmax=10, resdec=.1,\n",
       "           dx = 1.e-7, armfix=false, \n",
       "           pdata = nothing, jfact = klfact,\n",
       "           printerr = true, keepsolhist = false, stagnationok=false)\n",
       "\\end{verbatim}\n",
       ")\n",
       "\n",
       "C. T. Kelley, 2020\n",
       "\n",
       "Julia versions of the nonlinear solvers from my SIAM books.  Herewith: nsol\n",
       "\n",
       "You must allocate storage for the function and Jacobian in advance –> in the calling program <– ie. in FS and FPS\n",
       "\n",
       "Inputs:\n",
       "\n",
       "\\begin{itemize}\n",
       "\\item F!: function evaluation, the ! indicates that F! overwrites FS, your   preallocated storage for the function.\n",
       "\n",
       "So FS=F!(FS,x) or FS=F!(FS,x,pdata) returns FS=F(x)\n",
       "\n",
       "\\end{itemize}\n",
       "\\begin{itemize}\n",
       "\\item x0: initial iterate\n",
       "\n",
       "\\end{itemize}\n",
       "\\begin{itemize}\n",
       "\\item FS: Preallcoated storage for function. It is an N x 1 column vector\n",
       "\n",
       "\\end{itemize}\n",
       "\\begin{itemize}\n",
       "\\item FPS: preallcoated storage for Jacobian. It is an N x N matrix\n",
       "\n",
       "\\end{itemize}\n",
       "\\begin{itemize}\n",
       "\\item J!: Jacobian evaluation, the ! indicates that J! overwrites FPS, your   preallocated storage for the Jacobian. If you leave this out the   default is a finite difference Jacobian.\n",
       "\n",
       "So, FP=J!(FP,FS,x) or FP=J!(FP,FS,x,pdata) returns FP=F'(x). \n",
       "\n",
       "(FP,FS, x) must be the argument list, even if FP does not need FS.   One reason for this is that the finite-difference Jacobian   does and that is the default in the solver.\n",
       "\n",
       "\n",
       "\\item Precision: Lemme tell ya 'bout precision. I designed this code for    full precision functions and linear algebra in any precision you want.    You can declare   FPS as Float64, Float32, or Float16 and nsol will do the right thing if   YOU do not destroy the declaration in your J! function. I'm amazed   that this works so easily. If the Jacobian is reasonably well    conditioned, I can see no reason to do linear algebra in    double precision for anything other than horribly ill-conditioned   problems.\n",
       "\n",
       "BUT ... There is very limited support for direct sparse solvers in   anything other than Float64. I recommend that you only use Float64   with direct sparse solvers unless you really know what you're doing.\n",
       "\n",
       "\\end{itemize}\n",
       "\\rule{\\textwidth}{1pt}\n",
       "Keyword Arguments (kwargs):\n",
       "\n",
       "\\begin{itemize}\n",
       "\\item rtol and atol: relative and absolute error tolerances\n",
       "\n",
       "\\end{itemize}\n",
       "\\begin{itemize}\n",
       "\\item maxit: limit on nonlinear iterations\n",
       "\n",
       "\\end{itemize}\n",
       "solver: default = \"newton\"\n",
       "\n",
       "Your choices are \"newton\" or \"chord\". However, you have sham at your disposal only if you chose newton. \"chord\" will keep using the initial derivative until the iterate converges, uses the iteration budget, or the line search fails. It is not the same as sham=Inf, which is smarter.\n",
       "\n",
       "sham: default = 5 (ie Newton)\n",
       "\n",
       "This is the Shamanskii method. If sham=1, you have Newton. The iteration updates the derivative every sham iterations. The convergence rate has local q-order sham+1 if you only count iterations where you update the derivative. You need not provide your own derivative function to use this option. sham=Inf is chord only if chord is converging well.\n",
       "\n",
       "I made sham=1 the default for scalar equations. For systems I'm more aggressive and want to invest as little energy in linear algebra as possible. So the default is sham=5.\n",
       "\n",
       "armmax: upper bound on stepsize reductions in linesearch\n",
       "\n",
       "resdec: default = .1\n",
       "\n",
       "This is the target value for residual reduction. The default value is .1. In the old MATLAB codes it was .5. I only turn Shamanskii on if the residuals are decreasing rapidly, at least a factor of resdec, and the line search is quiescent. If you want to eliminate resdec from the method ( you don't ) then set resdec = 1.0 and you will never hear from it again.\n",
       "\n",
       "dx: default = 1.e-7\n",
       "\n",
       "difference increment in finite-difference derivatives       h=dx*norm(x,Inf)+1.e-8\n",
       "\n",
       "armfix: default = false\n",
       "\n",
       "The default is a parabolic line search (ie false). Set to true and the stepsize will be fixed at .5. Don't do this unless you are doing experiments for research.\n",
       "\n",
       "pdata:\n",
       "\n",
       "precomputed data for the function/Jacobian.  Things will go better if you use this rather than hide the data  in global variables within the module for your function/Jacobian\n",
       "\n",
       "jfact: default = klfact (tries to figure out best choice) \n",
       "\n",
       "If your Jacobian has any special structure, please set jfact to the correct choice for a factorization.\n",
       "\n",
       "I use jfact when I call PrepareJac! to evaluate the Jacobian (using your J!) and factor it. The default is to use klfact (an internal function) to do something reasonable. For general matrices, klfact picks lu! to compute an LU factorization and share storage with the Jacobian.  You may change LU to something else by, for example, setting jfact = cholseky! if your Jacobian is spd.\n",
       "\n",
       "klfact knows about banded matrices and picks qr. You should, however RTFM, allocate the extra two upper bands, and use jfact=qr! to override klfact.\n",
       "\n",
       "If you give me something that klfact does not know how to dispatch on, then nothing happens. I just return the original Jacobian matrix and  nsol will use backslash to compute the Newton step. I know that this is probably not optimal in your situation, so it is  good to pick something else, like jfact = lu.\n",
       "\n",
       "Please do not mess with the line that calls PrepareJac!. \n",
       "\n",
       "\\begin{verbatim}\n",
       "    FPF = PrepareJac!(FPS, FS, x, ItRules)\n",
       "\\end{verbatim}\n",
       "FPF is not the same as FPS (the storage you allocate for the Jacobian) for a reason. FPF and FPS do not have the same type, even though they share storage. So, FPS=PrepareJac!(FPS, FS, ...) will break things.\n",
       "\n",
       "printerr: default = true\n",
       "\n",
       "I print a helpful message when the solver fails. To supress that message set printerr to false.\n",
       "\n",
       "keepsolhist: default = false\n",
       "\n",
       "Set this to true to get the history of the iteration in the output tuple. This is on by default for scalar equations and off for systems. Only turn it on if you have use for the data, which can get REALLY LARGE.\n",
       "\n",
       "stagnationok: default = false\n",
       "\n",
       "Set this to true if you want to disable the line search and either observe divergence or stagnation. This is only useful for research or writing a book.\n",
       "\n",
       "Output:\n",
       "\n",
       "A named tuple (solution, functionval, history, stats, idid,                errcode, solhist) where\n",
       "\n",
       "solution = converged result functionval = F(solution) history = the vector of residual norms (||F(x)||) for the iteration stats = named tuple of the history of (ifun, ijac, iarm), the number of functions/derivatives/steplength reductions at each iteration.\n",
       "\n",
       "I do not count the function values for a finite-difference derivative because they count toward a Jacobian evaluation. \n",
       "\n",
       "idid=true if the iteration succeeded and false if not.\n",
       "\n",
       "errcode = 0 if if the iteration succeeded         = -1 if the initial iterate satisifies the termination criteria         = 10 if no convergence after maxit iterations         = 1  if the line search failed\n",
       "\n",
       "solhist:\n",
       "\n",
       "This is the entire history of the iteration if you've set keepsolhist=true\n",
       "\n",
       "solhist is an N x K array where N is the length of x and K is the number of iteration + 1. So, for scalar equations, it's a row vector.\n",
       "\n",
       "\\rule{\\textwidth}{1pt}\n",
       "\\section{Examples}\n",
       "\\paragraph{World's easiest problem example.}\n",
       "\\begin{verbatim}\n",
       " julia> function f!(fv,x)\n",
       "       fv[1]=x[1] + sin(x[2])\n",
       "       fv[2]=cos(x[1]+x[2])\n",
       "       end\n",
       "f (generic function with 1 method)\n",
       "\n",
       "julia> x=ones(2,); fv=zeros(2,); jv=zeros(2,2);\n",
       "julia> nout=nsol(f!,x,fv,jv; sham=1);\n",
       "julia> nout.history\n",
       "5-element Array{Float64,1}:\n",
       " 1.88791e+00\n",
       " 2.43119e-01\n",
       " 1.19231e-02\n",
       " 1.03266e-05\n",
       " 1.46416e-11\n",
       "\n",
       "julia> nout.solution\n",
       "2-element Array{Float64,1}:\n",
       " -7.39085e-01\n",
       "  2.30988e+00\n",
       "\n",
       "\\end{verbatim}\n",
       "\\paragraph{H-equation example. I'm taking the sham=5 default here, so the convergence is not quadratic. The good news is that we evaluate the Jacobian only once.}\n",
       "\\begin{verbatim}\n",
       "julia> n=16; x0=ones(n,); FV=ones(n,); JV=ones(n,n);\n",
       "julia> hdata=heqinit(x0, .5);\n",
       "julia> hout=nsol(heqf!,x0,FV,JV;pdata=hdata);\n",
       "julia> hout.history\n",
       "4-element Array{Float64,1}:\n",
       " 6.17376e-01\n",
       " 3.17810e-03\n",
       " 2.75227e-05\n",
       " 2.35817e-07\n",
       "\\end{verbatim}\n"
      ],
      "text/markdown": [
       "```\n",
       "nsol(F!, x0, FS, FPS, J!=diffjac!; rtol=1.e-6, atol=1.e-12,\n",
       "           maxit=20, solver=\"newton\", sham=5, armmax=10, resdec=.1,\n",
       "           dx = 1.e-7, armfix=false, \n",
       "           pdata = nothing, jfact = klfact,\n",
       "           printerr = true, keepsolhist = false, stagnationok=false)\n",
       "```\n",
       "\n",
       ")\n",
       "\n",
       "C. T. Kelley, 2020\n",
       "\n",
       "Julia versions of the nonlinear solvers from my SIAM books.  Herewith: nsol\n",
       "\n",
       "You must allocate storage for the function and Jacobian in advance –> in the calling program <– ie. in FS and FPS\n",
       "\n",
       "Inputs:\n",
       "\n",
       "  * F!: function evaluation, the ! indicates that F! overwrites FS, your   preallocated storage for the function.\n",
       "\n",
       "    So FS=F!(FS,x) or FS=F!(FS,x,pdata) returns FS=F(x)\n",
       "\n",
       "  * x0: initial iterate\n",
       "\n",
       "  * FS: Preallcoated storage for function. It is an N x 1 column vector\n",
       "\n",
       "  * FPS: preallcoated storage for Jacobian. It is an N x N matrix\n",
       "\n",
       "  * J!: Jacobian evaluation, the ! indicates that J! overwrites FPS, your   preallocated storage for the Jacobian. If you leave this out the   default is a finite difference Jacobian.\n",
       "\n",
       "    So, FP=J!(FP,FS,x) or FP=J!(FP,FS,x,pdata) returns FP=F'(x). \n",
       "\n",
       "    (FP,FS, x) must be the argument list, even if FP does not need FS.   One reason for this is that the finite-difference Jacobian   does and that is the default in the solver.\n",
       "  * Precision: Lemme tell ya 'bout precision. I designed this code for    full precision functions and linear algebra in any precision you want.    You can declare   FPS as Float64, Float32, or Float16 and nsol will do the right thing if   YOU do not destroy the declaration in your J! function. I'm amazed   that this works so easily. If the Jacobian is reasonably well    conditioned, I can see no reason to do linear algebra in    double precision for anything other than horribly ill-conditioned   problems.\n",
       "\n",
       "    BUT ... There is very limited support for direct sparse solvers in   anything other than Float64. I recommend that you only use Float64   with direct sparse solvers unless you really know what you're doing.\n",
       "\n",
       "---\n",
       "\n",
       "Keyword Arguments (kwargs):\n",
       "\n",
       "  * rtol and atol: relative and absolute error tolerances\n",
       "\n",
       "  * maxit: limit on nonlinear iterations\n",
       "\n",
       "solver: default = \"newton\"\n",
       "\n",
       "Your choices are \"newton\" or \"chord\". However, you have sham at your disposal only if you chose newton. \"chord\" will keep using the initial derivative until the iterate converges, uses the iteration budget, or the line search fails. It is not the same as sham=Inf, which is smarter.\n",
       "\n",
       "sham: default = 5 (ie Newton)\n",
       "\n",
       "This is the Shamanskii method. If sham=1, you have Newton. The iteration updates the derivative every sham iterations. The convergence rate has local q-order sham+1 if you only count iterations where you update the derivative. You need not provide your own derivative function to use this option. sham=Inf is chord only if chord is converging well.\n",
       "\n",
       "I made sham=1 the default for scalar equations. For systems I'm more aggressive and want to invest as little energy in linear algebra as possible. So the default is sham=5.\n",
       "\n",
       "armmax: upper bound on stepsize reductions in linesearch\n",
       "\n",
       "resdec: default = .1\n",
       "\n",
       "This is the target value for residual reduction. The default value is .1. In the old MATLAB codes it was .5. I only turn Shamanskii on if the residuals are decreasing rapidly, at least a factor of resdec, and the line search is quiescent. If you want to eliminate resdec from the method ( you don't ) then set resdec = 1.0 and you will never hear from it again.\n",
       "\n",
       "dx: default = 1.e-7\n",
       "\n",
       "difference increment in finite-difference derivatives       h=dx*norm(x,Inf)+1.e-8\n",
       "\n",
       "armfix: default = false\n",
       "\n",
       "The default is a parabolic line search (ie false). Set to true and the stepsize will be fixed at .5. Don't do this unless you are doing experiments for research.\n",
       "\n",
       "pdata:\n",
       "\n",
       "precomputed data for the function/Jacobian.  Things will go better if you use this rather than hide the data  in global variables within the module for your function/Jacobian\n",
       "\n",
       "jfact: default = klfact (tries to figure out best choice) \n",
       "\n",
       "If your Jacobian has any special structure, please set jfact to the correct choice for a factorization.\n",
       "\n",
       "I use jfact when I call PrepareJac! to evaluate the Jacobian (using your J!) and factor it. The default is to use klfact (an internal function) to do something reasonable. For general matrices, klfact picks lu! to compute an LU factorization and share storage with the Jacobian.  You may change LU to something else by, for example, setting jfact = cholseky! if your Jacobian is spd.\n",
       "\n",
       "klfact knows about banded matrices and picks qr. You should, however RTFM, allocate the extra two upper bands, and use jfact=qr! to override klfact.\n",
       "\n",
       "If you give me something that klfact does not know how to dispatch on, then nothing happens. I just return the original Jacobian matrix and  nsol will use backslash to compute the Newton step. I know that this is probably not optimal in your situation, so it is  good to pick something else, like jfact = lu.\n",
       "\n",
       "Please do not mess with the line that calls PrepareJac!. \n",
       "\n",
       "```\n",
       "    FPF = PrepareJac!(FPS, FS, x, ItRules)\n",
       "```\n",
       "\n",
       "FPF is not the same as FPS (the storage you allocate for the Jacobian) for a reason. FPF and FPS do not have the same type, even though they share storage. So, FPS=PrepareJac!(FPS, FS, ...) will break things.\n",
       "\n",
       "printerr: default = true\n",
       "\n",
       "I print a helpful message when the solver fails. To supress that message set printerr to false.\n",
       "\n",
       "keepsolhist: default = false\n",
       "\n",
       "Set this to true to get the history of the iteration in the output tuple. This is on by default for scalar equations and off for systems. Only turn it on if you have use for the data, which can get REALLY LARGE.\n",
       "\n",
       "stagnationok: default = false\n",
       "\n",
       "Set this to true if you want to disable the line search and either observe divergence or stagnation. This is only useful for research or writing a book.\n",
       "\n",
       "Output:\n",
       "\n",
       "A named tuple (solution, functionval, history, stats, idid,                errcode, solhist) where\n",
       "\n",
       "solution = converged result functionval = F(solution) history = the vector of residual norms (||F(x)||) for the iteration stats = named tuple of the history of (ifun, ijac, iarm), the number of functions/derivatives/steplength reductions at each iteration.\n",
       "\n",
       "I do not count the function values for a finite-difference derivative because they count toward a Jacobian evaluation. \n",
       "\n",
       "idid=true if the iteration succeeded and false if not.\n",
       "\n",
       "errcode = 0 if if the iteration succeeded         = -1 if the initial iterate satisifies the termination criteria         = 10 if no convergence after maxit iterations         = 1  if the line search failed\n",
       "\n",
       "solhist:\n",
       "\n",
       "This is the entire history of the iteration if you've set keepsolhist=true\n",
       "\n",
       "solhist is an N x K array where N is the length of x and K is the number of iteration + 1. So, for scalar equations, it's a row vector.\n",
       "\n",
       "---\n",
       "\n",
       "# Examples\n",
       "\n",
       "#### World's easiest problem example.\n",
       "\n",
       "```jldoctest\n",
       " julia> function f!(fv,x)\n",
       "       fv[1]=x[1] + sin(x[2])\n",
       "       fv[2]=cos(x[1]+x[2])\n",
       "       end\n",
       "f (generic function with 1 method)\n",
       "\n",
       "julia> x=ones(2,); fv=zeros(2,); jv=zeros(2,2);\n",
       "julia> nout=nsol(f!,x,fv,jv; sham=1);\n",
       "julia> nout.history\n",
       "5-element Array{Float64,1}:\n",
       " 1.88791e+00\n",
       " 2.43119e-01\n",
       " 1.19231e-02\n",
       " 1.03266e-05\n",
       " 1.46416e-11\n",
       "\n",
       "julia> nout.solution\n",
       "2-element Array{Float64,1}:\n",
       " -7.39085e-01\n",
       "  2.30988e+00\n",
       "\n",
       "```\n",
       "\n",
       "#### H-equation example. I'm taking the sham=5 default here, so the convergence is not quadratic. The good news is that we evaluate the Jacobian only once.\n",
       "\n",
       "```jldoctest\n",
       "julia> n=16; x0=ones(n,); FV=ones(n,); JV=ones(n,n);\n",
       "julia> hdata=heqinit(x0, .5);\n",
       "julia> hout=nsol(heqf!,x0,FV,JV;pdata=hdata);\n",
       "julia> hout.history\n",
       "4-element Array{Float64,1}:\n",
       " 6.17376e-01\n",
       " 3.17810e-03\n",
       " 2.75227e-05\n",
       " 2.35817e-07\n",
       "```\n"
      ],
      "text/plain": [
       "\u001b[36m  nsol(F!, x0, FS, FPS, J!=diffjac!; rtol=1.e-6, atol=1.e-12,\u001b[39m\n",
       "\u001b[36m             maxit=20, solver=\"newton\", sham=5, armmax=10, resdec=.1,\u001b[39m\n",
       "\u001b[36m             dx = 1.e-7, armfix=false, \u001b[39m\n",
       "\u001b[36m             pdata = nothing, jfact = klfact,\u001b[39m\n",
       "\u001b[36m             printerr = true, keepsolhist = false, stagnationok=false)\u001b[39m\n",
       "\n",
       "  )\n",
       "\n",
       "  C. T. Kelley, 2020\n",
       "\n",
       "  Julia versions of the nonlinear solvers from my SIAM books. Herewith: nsol\n",
       "\n",
       "  You must allocate storage for the function and Jacobian in advance –> in the\n",
       "  calling program <– ie. in FS and FPS\n",
       "\n",
       "  Inputs:\n",
       "\n",
       "    •    F!: function evaluation, the ! indicates that F! overwrites FS,\n",
       "        your preallocated storage for the function.\n",
       "      \n",
       "        So FS=F!(FS,x) or FS=F!(FS,x,pdata) returns FS=F(x)\n",
       "\n",
       "    •    x0: initial iterate\n",
       "\n",
       "    •    FS: Preallcoated storage for function. It is an N x 1 column\n",
       "        vector\n",
       "\n",
       "    •    FPS: preallcoated storage for Jacobian. It is an N x N matrix\n",
       "\n",
       "    •    J!: Jacobian evaluation, the ! indicates that J! overwrites FPS,\n",
       "        your preallocated storage for the Jacobian. If you leave this out\n",
       "        the default is a finite difference Jacobian.\n",
       "      \n",
       "        So, FP=J!(FP,FS,x) or FP=J!(FP,FS,x,pdata) returns FP=F'(x). \n",
       "      \n",
       "        (FP,FS, x) must be the argument list, even if FP does not need FS.\n",
       "        One reason for this is that the finite-difference Jacobian does\n",
       "        and that is the default in the solver.\n",
       "\n",
       "    •    Precision: Lemme tell ya 'bout precision. I designed this code for\n",
       "        full precision functions and linear algebra in any precision you\n",
       "        want. You can declare FPS as Float64, Float32, or Float16 and nsol\n",
       "        will do the right thing if YOU do not destroy the declaration in\n",
       "        your J! function. I'm amazed that this works so easily. If the\n",
       "        Jacobian is reasonably well conditioned, I can see no reason to do\n",
       "        linear algebra in double precision for anything other than\n",
       "        horribly ill-conditioned problems.\n",
       "      \n",
       "        BUT ... There is very limited support for direct sparse solvers in\n",
       "        anything other than Float64. I recommend that you only use Float64\n",
       "        with direct sparse solvers unless you really know what you're\n",
       "        doing.\n",
       "\n",
       "  ────────────────────────────────────────────────────────────────────────────\n",
       "\n",
       "  Keyword Arguments (kwargs):\n",
       "\n",
       "    •    rtol and atol: relative and absolute error tolerances\n",
       "\n",
       "    •    maxit: limit on nonlinear iterations\n",
       "\n",
       "  solver: default = \"newton\"\n",
       "\n",
       "  Your choices are \"newton\" or \"chord\". However, you have sham at your\n",
       "  disposal only if you chose newton. \"chord\" will keep using the initial\n",
       "  derivative until the iterate converges, uses the iteration budget, or the\n",
       "  line search fails. It is not the same as sham=Inf, which is smarter.\n",
       "\n",
       "  sham: default = 5 (ie Newton)\n",
       "\n",
       "  This is the Shamanskii method. If sham=1, you have Newton. The iteration\n",
       "  updates the derivative every sham iterations. The convergence rate has local\n",
       "  q-order sham+1 if you only count iterations where you update the derivative.\n",
       "  You need not provide your own derivative function to use this option.\n",
       "  sham=Inf is chord only if chord is converging well.\n",
       "\n",
       "  I made sham=1 the default for scalar equations. For systems I'm more\n",
       "  aggressive and want to invest as little energy in linear algebra as\n",
       "  possible. So the default is sham=5.\n",
       "\n",
       "  armmax: upper bound on stepsize reductions in linesearch\n",
       "\n",
       "  resdec: default = .1\n",
       "\n",
       "  This is the target value for residual reduction. The default value is .1. In\n",
       "  the old MATLAB codes it was .5. I only turn Shamanskii on if the residuals\n",
       "  are decreasing rapidly, at least a factor of resdec, and the line search is\n",
       "  quiescent. If you want to eliminate resdec from the method ( you don't )\n",
       "  then set resdec = 1.0 and you will never hear from it again.\n",
       "\n",
       "  dx: default = 1.e-7\n",
       "\n",
       "  difference increment in finite-difference derivatives h=dx*norm(x,Inf)+1.e-8\n",
       "\n",
       "  armfix: default = false\n",
       "\n",
       "  The default is a parabolic line search (ie false). Set to true and the\n",
       "  stepsize will be fixed at .5. Don't do this unless you are doing experiments\n",
       "  for research.\n",
       "\n",
       "  pdata:\n",
       "\n",
       "  precomputed data for the function/Jacobian. Things will go better if you use\n",
       "  this rather than hide the data in global variables within the module for\n",
       "  your function/Jacobian\n",
       "\n",
       "  jfact: default = klfact (tries to figure out best choice) \n",
       "\n",
       "  If your Jacobian has any special structure, please set jfact to the correct\n",
       "  choice for a factorization.\n",
       "\n",
       "  I use jfact when I call PrepareJac! to evaluate the Jacobian (using your J!)\n",
       "  and factor it. The default is to use klfact (an internal function) to do\n",
       "  something reasonable. For general matrices, klfact picks lu! to compute an\n",
       "  LU factorization and share storage with the Jacobian. You may change LU to\n",
       "  something else by, for example, setting jfact = cholseky! if your Jacobian\n",
       "  is spd.\n",
       "\n",
       "  klfact knows about banded matrices and picks qr. You should, however RTFM,\n",
       "  allocate the extra two upper bands, and use jfact=qr! to override klfact.\n",
       "\n",
       "  If you give me something that klfact does not know how to dispatch on, then\n",
       "  nothing happens. I just return the original Jacobian matrix and nsol will\n",
       "  use backslash to compute the Newton step. I know that this is probably not\n",
       "  optimal in your situation, so it is good to pick something else, like jfact\n",
       "  = lu.\n",
       "\n",
       "  Please do not mess with the line that calls PrepareJac!. \n",
       "\n",
       "\u001b[36m      FPF = PrepareJac!(FPS, FS, x, ItRules)\u001b[39m\n",
       "\n",
       "  FPF is not the same as FPS (the storage you allocate for the Jacobian) for a\n",
       "  reason. FPF and FPS do not have the same type, even though they share\n",
       "  storage. So, FPS=PrepareJac!(FPS, FS, ...) will break things.\n",
       "\n",
       "  printerr: default = true\n",
       "\n",
       "  I print a helpful message when the solver fails. To supress that message set\n",
       "  printerr to false.\n",
       "\n",
       "  keepsolhist: default = false\n",
       "\n",
       "  Set this to true to get the history of the iteration in the output tuple.\n",
       "  This is on by default for scalar equations and off for systems. Only turn it\n",
       "  on if you have use for the data, which can get REALLY LARGE.\n",
       "\n",
       "  stagnationok: default = false\n",
       "\n",
       "  Set this to true if you want to disable the line search and either observe\n",
       "  divergence or stagnation. This is only useful for research or writing a\n",
       "  book.\n",
       "\n",
       "  Output:\n",
       "\n",
       "  A named tuple (solution, functionval, history, stats, idid, errcode,\n",
       "  solhist) where\n",
       "\n",
       "  solution = converged result functionval = F(solution) history = the vector\n",
       "  of residual norms (||F(x)||) for the iteration stats = named tuple of the\n",
       "  history of (ifun, ijac, iarm), the number of\n",
       "  functions/derivatives/steplength reductions at each iteration.\n",
       "\n",
       "  I do not count the function values for a finite-difference derivative\n",
       "  because they count toward a Jacobian evaluation. \n",
       "\n",
       "  idid=true if the iteration succeeded and false if not.\n",
       "\n",
       "  errcode = 0 if if the iteration succeeded = -1 if the initial iterate\n",
       "  satisifies the termination criteria = 10 if no convergence after maxit\n",
       "  iterations = 1 if the line search failed\n",
       "\n",
       "  solhist:\n",
       "\n",
       "  This is the entire history of the iteration if you've set keepsolhist=true\n",
       "\n",
       "  solhist is an N x K array where N is the length of x and K is the number of\n",
       "  iteration + 1. So, for scalar equations, it's a row vector.\n",
       "\n",
       "  ────────────────────────────────────────────────────────────────────────────\n",
       "\n",
       "\u001b[1m  Examples\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡≡≡≡\u001b[22m\n",
       "\n",
       "\u001b[1m  World's easiest problem example.\u001b[22m\n",
       "\u001b[1m  ----------------------------------\u001b[22m\n",
       "\n",
       "\u001b[36m   julia> function f!(fv,x)\u001b[39m\n",
       "\u001b[36m         fv[1]=x[1] + sin(x[2])\u001b[39m\n",
       "\u001b[36m         fv[2]=cos(x[1]+x[2])\u001b[39m\n",
       "\u001b[36m         end\u001b[39m\n",
       "\u001b[36m  f (generic function with 1 method)\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> x=ones(2,); fv=zeros(2,); jv=zeros(2,2);\u001b[39m\n",
       "\u001b[36m  julia> nout=nsol(f!,x,fv,jv; sham=1);\u001b[39m\n",
       "\u001b[36m  julia> nout.history\u001b[39m\n",
       "\u001b[36m  5-element Array{Float64,1}:\u001b[39m\n",
       "\u001b[36m   1.88791e+00\u001b[39m\n",
       "\u001b[36m   2.43119e-01\u001b[39m\n",
       "\u001b[36m   1.19231e-02\u001b[39m\n",
       "\u001b[36m   1.03266e-05\u001b[39m\n",
       "\u001b[36m   1.46416e-11\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> nout.solution\u001b[39m\n",
       "\u001b[36m  2-element Array{Float64,1}:\u001b[39m\n",
       "\u001b[36m   -7.39085e-01\u001b[39m\n",
       "\u001b[36m    2.30988e+00\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\n",
       "\u001b[1m  H-equation example. I'm taking the sham=5 default here, so the\u001b[22m\n",
       "\u001b[1m convergence is not quadratic. The good news is that we evaluate the\u001b[22m\n",
       "\u001b[1m Jacobian only once.\u001b[22m\n",
       "\u001b[1m  --------------------------\u001b[22m\n",
       "\n",
       "\u001b[36m  julia> n=16; x0=ones(n,); FV=ones(n,); JV=ones(n,n);\u001b[39m\n",
       "\u001b[36m  julia> hdata=heqinit(x0, .5);\u001b[39m\n",
       "\u001b[36m  julia> hout=nsol(heqf!,x0,FV,JV;pdata=hdata);\u001b[39m\n",
       "\u001b[36m  julia> hout.history\u001b[39m\n",
       "\u001b[36m  4-element Array{Float64,1}:\u001b[39m\n",
       "\u001b[36m   6.17376e-01\u001b[39m\n",
       "\u001b[36m   3.17810e-03\u001b[39m\n",
       "\u001b[36m   2.75227e-05\u001b[39m\n",
       "\u001b[36m   2.35817e-07\u001b[39m"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?nsol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input\n",
    "\n",
    "The calling sequence for the Newton solvers in this book are similar, differing mostly in the management of the linear solver and memory allocation. The calling sequence for __nsold.jl__ is\n",
    "\n",
    "```julia\n",
    "function nsold(\n",
    "    F!,\n",
    "    x0,\n",
    "    FS,\n",
    "    FPS,\n",
    "    J! = diffjac!;\n",
    "    rtol = 1.e-6,\n",
    "    atol = 1.e-12,\n",
    "    maxit = 20,\n",
    "    solver = \"newton\",\n",
    "    sham = 1,\n",
    "    armmax = 10,\n",
    "    resdec = 0.1,\n",
    "    dx = 1.e-7,\n",
    "    armfix = false,\n",
    "    pdata = nothing,\n",
    "    jfact = lu!,\n",
    "    printerr = true,\n",
    "    keepsolhist = false,\n",
    "    stagnationok = false,\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we said earlier in the chapter, the calling sequence has some new things which are not in __nsolsc.jl__. The most significant are the arrays __FS__ and __FPS__, which preallocate\n",
    "storage for the function and Jacobian. As we have pointed out earlier, the farther upstream one allocates memory the better, so __nsold.jl__ insists that you allocate an vector __FS__ of the same size as the initial iterate and a matrix \n",
    "__FPS__ for the Jacobian. \n",
    "\n",
    "The other major new feature is the keyword argument __pdata__. This is the data structure for you to store any precomuted or preallocated data your function evaluation needs. You will almost surely need __pdata__ for any but the most trivial problems. The H-equation example from section 2.6 uses __pdata__ in a serious manner.\n",
    "\n",
    "You may dimension __x0__ either as $(N,1)$ or $(N,)$, but you must be consistent and dimension __FS__ the same way. __nsold.jl__ expects vectors to be in double precision (Float64). \n",
    "\n",
    "The __!__ in the function evaluation __F!__ is to indicate that __nsold__ expects __F__  to overwrite its imput. So,\n",
    "the way to call __F!__ is to preallocate the storage for the function value in an array __FS__ and then call the function as\n",
    "```Julia\n",
    "F!(FS,x)\n",
    "```\n",
    "or\n",
    "```Julia\n",
    "F!(FS,x,pdata)\n",
    "```\n",
    "__nsold.jl__ will figure out if you have populated __pdata__ or left it alone as the default value of __nothing__.\n",
    "\n",
    "\n",
    "And now for the Jacobian. __nsold.jl__ uses direct methods for linear algebra. If your matrix is dense, the default is to use Julia's __lu!__ function to do an LU factorization. If your matrix is symmetric or symmetric positive definite you can use the __factorization__ keyword to change __lu!__ to __ldlt!__ or __cholesky!__ for example. __nsold.jl__ assumes that the factorization you ask for will overwrite the matrix. Hence, the __factorize__ function in Julia is not what you want for this application.\n",
    "\n",
    "You will also need to preallocate storage for the Jacobian in the array __FPS__. You may use any legal real precision for __FPS__. Float64 is the default. If you use Float32 you cut the storage for the matrix and the time for the factorization in half. We recommend that you do this if your Jacobian is dense. If you are using the __Sparsesuite__ sparse solvers, then you must store the Jacobian in double precision. __Sparsesuite__ does not support lower precision.\n",
    "\n",
    "Your Jacobian computation __J!__ must also overwrite it's input. The call looks like\n",
    "```julia\n",
    "J!(FV,FP,x)\n",
    "```\n",
    "or \n",
    "```julia\n",
    "J!(FV,FP,x,pdata)\n",
    "\n",
    "```\n",
    "returns FP=F'(x). The input FP=F(x), which __nsold.jl__ has already compouted, has to be there.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  H-Equation revisited"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we do several experiments to illustrage the advantages of infrequent evaluation and factorization of the Jacobian and mixed precision computation. We will begin with a function that will support our testing. We want to investigate combinations of Newton's method/Shamanskii with ```sham=5``` (the default), storing and factoring the Jacobian in double/single precision, and analytic/forward difference Jacobians. To make this easy we write a function that solves the H-equation with __nsol.jl__ and lets us vary these cases. \n",
    "\n",
    "The functions for the residual __heqf!.jl__, the Jacobian __heqJ!.jl__, and the precomputed data \n",
    "__heqinit.jl__ are in the large file \n",
    "[src/TestProblems/Systems/Hequation.jl](https://github.com/ctkelley/SIAMFANLEquations.jl/blob/master/src/TestProblems/Systems/Hequation.jl)\n",
    "in the \n",
    "[SIAMFANLEquations.jl](https://github.com/ctkelley/SIAMFANLEquations.jl)\n",
    "repository. \n",
    "\n",
    "I'm passing the precomputed data to the function rather than computing it within. This keeps the cost of the precomputed data out of the benchmarking I'll do later.\n",
    "\n",
    "I'm also using __splat__. I populate a named tuple ```bargs``` to keep the keyword arguments in a convenient place and then, when it's time to give it to __nsol.jl__, the call looks like ```bargs...```. The three dots are the __splat__ and tell __nsol__ to expand bargs and harvest the keyword arguments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "htest (generic function with 1 method)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function htest(x0, FS, FPS, hdata; analytic=false, hsham=5)\n",
    "    n=length(FS)\n",
    "    #\n",
    "    # I've preallocaed x0, FS, and FPS. But they may have been changed by previous runs.\n",
    "    # The cost of resetting their entries to 1.0 is insignificant. \n",
    "    #\n",
    "    FS.=1.0\n",
    "    FPS.=1.0\n",
    "    bargs=(atol = 1.e-10, rtol = 1.e-10, sham = hsham, resdec = .1, pdata=hdata)\n",
    "    if analytic\n",
    "        nout=nsol( heqf!, x0, FS, FPS, heqJ!; bargs...)\n",
    "    else\n",
    "        nout=nsol( heqf!, x0, FS, FPS; bargs...)\n",
    "    end\n",
    "    return nout\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin we will compare the iteration histories for four cases. We will consider analytic and forward difference Jacobians with the storage and factorization of the Jacobian done in double and single precision. __Theorem 1.2__ says the results should be almost indistinguishable. We will begin with Newton's method.\n",
    "\n",
    "All we need to do to store and factor Jacobians is to allocate the storage in single precision. That allocation is the line ```FPS32=ones(Float32,n,n);```. Note that we must reset ```FS``` and ```FPS``` after each call to __nsol.jl__ because the solver uses the storage for residuals and Jacobians for the entire iteration. We do this with __broadcast__ after the initial allocaton ```.=1.0``` instead of ```=ones(n,n)``` to avoid reallocation of the Jacobian.\n",
    "\n",
    "We will print all the residual histories in an array. The history vectors are the same length and are very hard to tell apart until the residual norm is one iteration from stagnation. This is just what the theory predicts. The theory \n",
    "(see <cite data-cite=\"ctk:sirev20\"><a href=\"siamfa.html#ctk:sirev20\">(Kel20a)</cite> ) also predicts that there will be little difference between double precision linear algebra and single precistion. We observe this as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4×4 Array{Float64,2}:\n",
       " 4.94274e+00  4.94274e+00  4.94274e+00  4.94274e+00\n",
       " 2.54130e-02  2.54130e-02  2.54130e-02  2.54119e-02\n",
       " 4.96643e-07  4.97414e-07  4.96643e-07  5.01844e-07\n",
       " 9.74217e-15  5.18031e-14  9.74217e-15  9.49256e-13"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n=1024; FS=ones(n,); FPS=ones(n,n); FPS32=ones(Float32,n,n); x0=ones(n,); c=.5; hdata = heqinit(x0, c);\n",
    "nouta64=htest(x0, FS, FPS, hdata;analytic=true, hsham=1);\n",
    "nouta32=htest(x0, FS, FPS32, hdata; analytic=true, hsham=1);\n",
    "noutfd64=htest(x0, FS, FPS, hdata; analytic=true, hsham=1);\n",
    "noutfd32=htest(x0, FS, FPS32,hdata; analytic=false, hsham=1);\n",
    "[nouta64.history nouta32.history noutfd64.history noutfd32.history]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now do the same thing with the default setting of ```sham=5```. The theory correctly predicts that we will see slower convergence. We will be using BenchmarkTools to compare the costs later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6×4 Array{Float64,2}:\n",
       " 4.94274e+00  4.94274e+00  4.94274e+00  4.94274e+00\n",
       " 2.54130e-02  2.54130e-02  2.54119e-02  2.54119e-02\n",
       " 2.19929e-04  2.19929e-04  2.19912e-04  2.19913e-04\n",
       " 1.88328e-06  1.88328e-06  1.88308e-06  1.88309e-06\n",
       " 1.61169e-08  1.61169e-08  1.61147e-08  1.61147e-08\n",
       " 1.37919e-10  1.37938e-10  1.37890e-10  1.37898e-10"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nouta64=htest(x0, FS, FPS, hdata; analytic=true);\n",
    "noutfd64=htest(x0, FS, FPS, hdata; analytic=false);\n",
    "nouta32=htest(x0, FS, FPS32, hdata; analytic=true);\n",
    "noutfd32=htest(x0, FS, FPS32, hdata; analytic=false);\n",
    "[nouta64.history nouta32.history noutfd64.history noutfd32.history]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is interesting. We need more iterations, but we evaluate the Jacobian only once for Shamanskii. We can see this by looking at the ```stats``` field of the output tuple. They are all the same, so we will use ```nouta64```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(ifun = [1, 1, 1, 1, 1, 1], ijac = [0, 1, 0, 0, 0, 0], iarm = [0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nouta64.stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The interpretation is that we do a function evaluation at all iterations and a single Jacobian evaluation to compute the $\\vx_1$. We do no Jacobian work after that. The default in __nsol.jl__ is to reevaluate the Jacobian if the reduction in the residual norm larger than ```resdec = .1```. You can change ```resdec``` in the keywork arguments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the [BenchmarkTools.jl](https://github.com/JuliaCI/BenchmarkTools.jl) package to look at performance. \n",
    "The ```@btime``` command will show compute time and memory allocations for an average of several runs. The averaging will mitigage the effects of the compile time for the first run. \n",
    "\n",
    "To begin, we will compare the four versions of Newton's method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analytic, double\n",
      "  27.119 ms (6198 allocations: 468.92 KiB)\n",
      "finite difference, double\n",
      "  94.759 ms (9264 allocations: 924.27 KiB)\n",
      "analytic, single\n",
      "  33.621 ms (6198 allocations: 468.92 KiB)\n",
      "finite difference, single\n",
      "  101.863 ms (9264 allocations: 924.27 KiB)\n"
     ]
    }
   ],
   "source": [
    "println(\"analytic, double\"); @btime htest($x0, $FS, $FPS, hdata; analytic=true, hsham=1);\n",
    "println(\"finite difference, double\"); @btime htest($x0, $FS, $FPS, hdata; analytic=false, hsham=1);\n",
    "println(\"analytic, single\"); @btime htest($x0, $FS, $FPS32, hdata; analytic=true, hsham=1);\n",
    "println(\"finite difference, single\"); @btime htest($x0, $FS, $FPS32, hdata; analytic=false, hsham=1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even for a problem of only modest size, the differences between the analytic Jacobian and the forward difference are significant. The differences between single and double precision linear algebra are less so, but would become more dramatic as the dimension of the problem grows. \n",
    "\n",
    "Next, we look at the default ```sham=5``` from __nsol.jl__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analytic, double\n",
      "  9.890 ms (2098 allocations: 244.58 KiB)\n",
      "finite difference, double\n",
      "  32.476 ms (3120 allocations: 396.36 KiB)\n",
      "analytic, single\n",
      "  12.439 ms (2098 allocations: 244.58 KiB)\n",
      "finite difference, single\n",
      "  35.105 ms (3120 allocations: 396.36 KiB)\n"
     ]
    }
   ],
   "source": [
    "println(\"analytic, double\"); @btime htest($x0, $FS, $FPS, hdata; analytic=true);\n",
    "println(\"finite difference, double\"); @btime htest($x0, $FS, $FPS, hdata; analytic=false);\n",
    "println(\"analytic, single\"); @btime htest($x0, $FS, $FPS32, hdata; analytic=true);\n",
    "println(\"finite difference, single\"); @btime htest($x0, $FS, $FPS32, hdata; analytic=false);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The moral here is pretty clear. We see that compute time is cut by a factor of at least two in all cases. The result for an analytic Jacobian with linear algebra in single precision and ```sham=5``` is 17 times faster than our slowest computation (Newton + finite difference Jacobian + double precision linear algebra). So, do less linear algebra and do it in single precision.\n",
    "\n",
    "Finally, we will increase the dimenson. As we do that the computation becomes more burdensome, so we will only do two cases, both with an analytic Jacobian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analytic, double, Newton\n",
      "  88.581 ms (12342 allocations: 932.92 KiB)\n",
      "analytic, single, sham=5\n",
      "  26.873 ms (4146 allocations: 484.58 KiB)\n"
     ]
    }
   ],
   "source": [
    "n=2048; FS=ones(n,); FPS=ones(n,n); FPS32=ones(Float32,n,n); x0=ones(n,); c=.5; hdata = heqinit(x0, c);\n",
    "println(\"analytic, double, Newton\"); @btime htest($x0, $FS, $FPS, hdata; analytic=true, hsham=1);\n",
    "println(\"analytic, single, sham=5\"); @btime htest($x0, $FS, $FPS32, hdata; analytic=true);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For sufficiently large dimension, the linear algebra cost will dominate the computation. Time should increase by roughly a factor of 8 as the dimension doubles because our LU factorization takes $O(N^3)$ operations. We are not in that regime yet. One of the projects at the end of this chapter challenges you to get there. Remember that we allocated storage for the Jacobian when we defined ```FPS```, so @btime is not measuring the allocation for the Jacobian."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### More on the Two-Point BVP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the Jacobian is sparse and you use the solvers from __SuiteSparse__ then you cannot use single preciscion. Even if the structure of the Jacobian allows you to use the LAPACK solvers or a special-purpose package, there is less benefit in using single precision for linear algebra than in the dense case. We will explore that for the boundary value problem, where we can use [BandedMatrices.jl](https://github.com/JuliaMatrices/BandedMatrices.jl) and __qr!__ for the linear solver.\n",
    "\n",
    "The we will set up the problem for a very fine mesh, far finer than one needs to get a useful result, to illustrate the performance. The band solver takes $O(N)$ work, so we would expect the solve to be fast. To set things up we mimic __bvp_solve.jl__ ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set it up\n",
    "    n=10^6;\n",
    "    bdata = bvpinit(n, Float64);\n",
    "#\n",
    "    U0 = zeros(2n);\n",
    "    FV = zeros(2n);\n",
    "# Banded matrix with the correct number of bands\n",
    "# Make double and single precision copies\n",
    "    FPV = BandedMatrix{Float64}(Zeros(2n, 2n), (2, 4));\n",
    "#\n",
    "# Build the initial iterate\n",
    "#\n",
    "    tv = bdata.tv;\n",
    "    sv = -.1 * tv .* tv;\n",
    "    view(U0,1:2:2n-1) .= exp.(-.1 .* tv .* tv);\n",
    "    view(U0,2:2:2n).= -.2 .* view(U0,1:2:2n-1) .* tv;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will begin by comparing the default ```sham=5``` with Newton's method. As we did with the H-equation, we will write a test function that uses the data we allocated above. We only use an analytic Jacobian for this and other examples with sparse Jacobians. The reader might want to look at the project in this chapter on [sparse differencing](#Sparse Differencing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bvptest (generic function with 1 method)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function bvptest(U0, FS, FPS, bdata; bsham=5, bfact=qr!)\n",
    "    FS .*= 0.0\n",
    "    FPS .*= 0.\n",
    "        bvpout = nsol(Fbvp!, U0, FS, FPS, Jbvp!; atol=1.e-8, rtol = 1.e-8, sham=bsham,\n",
    "             pdata = bdata, jfact=bfact)\n",
    "    return bvpout\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will look at the convergence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "Figure(PyObject <Figure size 640x480 with 1 Axes>)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bvpoutn64=bvptest(U0, FV, FPV, bdata;  bsham=1);\n",
    "bvpouts64=bvptest(U0, FV, FPV, bdata; bsham=5);\n",
    "newtonhist=bvpoutn64.history;\n",
    "shamhist=bvpouts64.history;\n",
    "nn=length(newtonhist);\n",
    "ns=length(shamhist);\n",
    "semilogy(0:nn-1,newtonhist,\"k-\",0:ns-1,shamhist,\"k--\")\n",
    "legend([\"newton\",\"sham=5\"]);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1×9 Adjoint{Int64,Array{Int64,1}}:\n",
       " 0  0  0  2  2  1  0  0  0"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bvpoutn64.stats.iarm'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Updating the Jacobian less frequently has a smaller impact than in the case of the H-equation. You should expect this because the linear solver costs $O(N)$ work rather than $O(N^3)$ and the Jacobian is updated for the early iterations because the residual is not decreasing fast enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analytic, double, Newton\n",
      "  1.607 s (138 allocations: 427.25 MiB)\n",
      "analytic, double, sham=5\n",
      "  1.448 s (137 allocations: 442.51 MiB)\n"
     ]
    }
   ],
   "source": [
    "println(\"analytic, double, Newton\"); @btime bvptest($U0, $FV, $FPV, $bdata;  bsham=1);\n",
    "println(\"analytic, double, sham=5\"); @btime bvptest($U0, $FV, $FPV, $bdata;  bsham=5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ptcsol.jl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ptcsol.jl__ is our $\\ptc$ solve. As usual, we begin with the docstrings ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search: \u001b[0m\u001b[1mp\u001b[22m\u001b[0m\u001b[1mt\u001b[22m\u001b[0m\u001b[1mc\u001b[22m\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1ml\u001b[22m \u001b[0m\u001b[1mp\u001b[22m\u001b[0m\u001b[1mt\u001b[22m\u001b[0m\u001b[1mc\u001b[22m\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1ml\u001b[22msc \u001b[0m\u001b[1mP\u001b[22mar\u001b[0m\u001b[1mt\u001b[22mialQui\u001b[0m\u001b[1mc\u001b[22mk\u001b[0m\u001b[1mS\u001b[22m\u001b[0m\u001b[1mo\u001b[22mrt\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "ptcsol(F!, x0, FS, FPS, J! = diffjac!; rtol=1.e-6, atol=1.e-12,                maxit=20, dt0=1.e-6, dx=1.e-7, pdata = nothing, jfact = klfact,                printerr = true, keepsolhist = false)\n",
       "\n",
       "C. T. Kelley, 2020\n",
       "\n",
       "Julia versions of the nonlinear solvers from my SIAM books.  Herewith: some new stuff ==> ptcsol\n",
       "\n",
       "You must allocate storage for the function and Jacobian in advance –> in the calling program <– ie. in FS and FPS\n",
       "\n",
       "Inputs:\n",
       "\n",
       "\\begin{itemize}\n",
       "\\item F!: function evaluation, the ! indicates that F! overwrites FS, your   preallocated storage for the function.\n",
       "\n",
       "So, FV=F!(FV,x) or FV=F!(FV,x,pdata) returns FV=F(x)\n",
       "\n",
       "\n",
       "\\item x0: initial iterate\n",
       "\n",
       "\\end{itemize}\n",
       "\\begin{itemize}\n",
       "\\item FS: Preallcoated storage for function. It is an N x 1 column vector.\n",
       "\n",
       "\\end{itemize}\n",
       "You may dimension it as (n,) or (n,1). (n,) is best, but the solvers can deal with it either way.\n",
       "\n",
       "\\begin{itemize}\n",
       "\\item FPS: preallcoated storage for Jacobian. It is an N x N matrix\n",
       "\n",
       "\\end{itemize}\n",
       "\\begin{itemize}\n",
       "\\item J!: Jacobian evaluation, the ! indicates that J! overwrites FPS, your   preallocated storage for the Jacobian. If you leave this out the   default is a finite difference Jacobian.\n",
       "\n",
       "So, FP=J!(FP,FV,x) or FP=J!(FP,FV,x,pdata) returns FP=F'(x);   (FP,FV, x) must be the argument list, even if FP does not need FV.   One reason for this is that the finite-difference Jacobian   does and that is the default in the solver.\n",
       "\n",
       "\n",
       "\\item Precision: Lemme tell ya 'bout precision. I designed this code for    full precision   functions and linear algebra in any precision you want. You can declare   FPS as Float64, Float32, or Float16 and ptcsol will do the right thing if   YOU do not destroy the declaration in your J! function. I'm amazed   that this works so easily. If the Jacobian is reasonably well    conditioned, I can see no reason to do linear algebra in    double precision I can see no reason to do linear algebra in   double precision for anything other than horribly ill-conditioned   problems.\n",
       "\n",
       "BUT ... There is very limited support for direct sparse solvers in   anything other than Float64. I recommend that you only use Float64   with direct sparse solvers unless you really know what you're doing.\n",
       "\n",
       "\\end{itemize}\n",
       "\\rule{\\textwidth}{1pt}\n",
       "Keyword Arguments (kwargs):\n",
       "\n",
       "rtol and atol: relative and absolute error tolerances\n",
       "\n",
       "dt0: initial time step. The default value of 1.e-3 is a bit conservative and is one option you really should play with. Look at the example where I set it to 1.0!\n",
       "\n",
       "maxit: limit on nonlinear iterations, default=100. \n",
       "\n",
       "This is coupled to dt0. If your choice of dt0 is too small (conservative) then you'll need many iterations to converge and will need a larger value of maxit\n",
       "\n",
       "For PTC you'll need more iterations than for a stright-up nonlinear solve. This is part of the price for finding the  stable solution.\n",
       "\n",
       "dx: default = 1.e-7\n",
       "\n",
       "difference increment in finite-difference derivatives       h=dx*norm(x)+1.e-6\n",
       "\n",
       "pdata:\n",
       "\n",
       "precomputed data for the function/Jacobian.  Things will go better if you use this rather than hide the data  in global variables within the module for your function/Jacobian\n",
       "\n",
       "jfact: default = klfact (tries to figure out best choice) \n",
       "\n",
       "If your Jacobian has any special structure, please set jfact to the correct choice for a factorization.\n",
       "\n",
       "I use jfact when I call PTCUpdate to evaluate the Jacobian (using your J!) and factor it. The default is to use klfact (an internal function) to do something reasonable. For general matrices, klfact picks lu! to compute an LU factorization and share storage with the Jacobian.  You may change LU to something else by, for example, setting jfact = cholseky! if your Jacobian is spd.\n",
       "\n",
       "klfact knows about banded matrices and picks qr. You should, however RTFM, allocate the extra two upper bands, and use jfact=qr! to override klfact.\n",
       "\n",
       "If you give me something that klfact does not know how to dispatch on, then nothing happens. I just return the original Jacobian matrix and  ptcsol will use backslash to compute the Newton step.\n",
       "\n",
       "I know that this is probably not optimal in your situation, so it is  good to pick something else, like jfact = lu.\n",
       "\n",
       "printerr: default = true\n",
       "\n",
       "I print a helpful message when the solver fails. To supress that message set printerr to false.\n",
       "\n",
       "keepsolhist: default = false\n",
       "\n",
       "Set this to true to get the history of the iteration in the output tuple. This is on by default for scalar equations and off for systems. Only turn it on if you have use for the data, which can get REALLY LARGE.\n",
       "\n",
       "Output:\n",
       "\n",
       "A named tuple (solution, functionval, history, stats, idid,                errcode, solhist) where\n",
       "\n",
       "solution = converged result functionval = F(solution) history = the vector of residual norms (||F(x)||) for the iteration stats = named tuple of the history of (ifun, ijac, iarm), the number of functions/derivatives/steplength reductions at each iteration.\n",
       "\n",
       "I do not count the function values for a finite-difference derivative because they count toward a Jacobian evaluation. \n",
       "\n",
       "idid=true if the iteration succeeded and false if not.\n",
       "\n",
       "errcode = 0 if if the iteration succeeded         = -1 if the initial iterate satisifies the termination criteria         = 10 if no convergence after maxit iterations         = 1  if the line search failed\n",
       "\n",
       "solhist:\n",
       "\n",
       "This is the entire history of the iteration if you've set keepsolhist=true\n",
       "\n",
       "solhist is an N x K array where N is the length of x and K is the number of iteration + 1. So, for scalar equations, it's a row vector.\n",
       "\n"
      ],
      "text/markdown": [
       "ptcsol(F!, x0, FS, FPS, J! = diffjac!; rtol=1.e-6, atol=1.e-12,                maxit=20, dt0=1.e-6, dx=1.e-7, pdata = nothing, jfact = klfact,                printerr = true, keepsolhist = false)\n",
       "\n",
       "C. T. Kelley, 2020\n",
       "\n",
       "Julia versions of the nonlinear solvers from my SIAM books.  Herewith: some new stuff ==> ptcsol\n",
       "\n",
       "You must allocate storage for the function and Jacobian in advance –> in the calling program <– ie. in FS and FPS\n",
       "\n",
       "Inputs:\n",
       "\n",
       "  * F!: function evaluation, the ! indicates that F! overwrites FS, your   preallocated storage for the function.\n",
       "\n",
       "    So, FV=F!(FV,x) or FV=F!(FV,x,pdata) returns FV=F(x)\n",
       "  * x0: initial iterate\n",
       "\n",
       "  * FS: Preallcoated storage for function. It is an N x 1 column vector.\n",
       "\n",
       "You may dimension it as (n,) or (n,1). (n,) is best, but the solvers can deal with it either way.\n",
       "\n",
       "  * FPS: preallcoated storage for Jacobian. It is an N x N matrix\n",
       "\n",
       "  * J!: Jacobian evaluation, the ! indicates that J! overwrites FPS, your   preallocated storage for the Jacobian. If you leave this out the   default is a finite difference Jacobian.\n",
       "\n",
       "    So, FP=J!(FP,FV,x) or FP=J!(FP,FV,x,pdata) returns FP=F'(x);   (FP,FV, x) must be the argument list, even if FP does not need FV.   One reason for this is that the finite-difference Jacobian   does and that is the default in the solver.\n",
       "  * Precision: Lemme tell ya 'bout precision. I designed this code for    full precision   functions and linear algebra in any precision you want. You can declare   FPS as Float64, Float32, or Float16 and ptcsol will do the right thing if   YOU do not destroy the declaration in your J! function. I'm amazed   that this works so easily. If the Jacobian is reasonably well    conditioned, I can see no reason to do linear algebra in    double precision I can see no reason to do linear algebra in   double precision for anything other than horribly ill-conditioned   problems.\n",
       "\n",
       "    BUT ... There is very limited support for direct sparse solvers in   anything other than Float64. I recommend that you only use Float64   with direct sparse solvers unless you really know what you're doing.\n",
       "\n",
       "---\n",
       "\n",
       "Keyword Arguments (kwargs):\n",
       "\n",
       "rtol and atol: relative and absolute error tolerances\n",
       "\n",
       "dt0: initial time step. The default value of 1.e-3 is a bit conservative and is one option you really should play with. Look at the example where I set it to 1.0!\n",
       "\n",
       "maxit: limit on nonlinear iterations, default=100. \n",
       "\n",
       "This is coupled to dt0. If your choice of dt0 is too small (conservative) then you'll need many iterations to converge and will need a larger value of maxit\n",
       "\n",
       "For PTC you'll need more iterations than for a stright-up nonlinear solve. This is part of the price for finding the  stable solution.\n",
       "\n",
       "dx: default = 1.e-7\n",
       "\n",
       "difference increment in finite-difference derivatives       h=dx*norm(x)+1.e-6\n",
       "\n",
       "pdata:\n",
       "\n",
       "precomputed data for the function/Jacobian.  Things will go better if you use this rather than hide the data  in global variables within the module for your function/Jacobian\n",
       "\n",
       "jfact: default = klfact (tries to figure out best choice) \n",
       "\n",
       "If your Jacobian has any special structure, please set jfact to the correct choice for a factorization.\n",
       "\n",
       "I use jfact when I call PTCUpdate to evaluate the Jacobian (using your J!) and factor it. The default is to use klfact (an internal function) to do something reasonable. For general matrices, klfact picks lu! to compute an LU factorization and share storage with the Jacobian.  You may change LU to something else by, for example, setting jfact = cholseky! if your Jacobian is spd.\n",
       "\n",
       "klfact knows about banded matrices and picks qr. You should, however RTFM, allocate the extra two upper bands, and use jfact=qr! to override klfact.\n",
       "\n",
       "If you give me something that klfact does not know how to dispatch on, then nothing happens. I just return the original Jacobian matrix and  ptcsol will use backslash to compute the Newton step.\n",
       "\n",
       "I know that this is probably not optimal in your situation, so it is  good to pick something else, like jfact = lu.\n",
       "\n",
       "printerr: default = true\n",
       "\n",
       "I print a helpful message when the solver fails. To supress that message set printerr to false.\n",
       "\n",
       "keepsolhist: default = false\n",
       "\n",
       "Set this to true to get the history of the iteration in the output tuple. This is on by default for scalar equations and off for systems. Only turn it on if you have use for the data, which can get REALLY LARGE.\n",
       "\n",
       "Output:\n",
       "\n",
       "A named tuple (solution, functionval, history, stats, idid,                errcode, solhist) where\n",
       "\n",
       "solution = converged result functionval = F(solution) history = the vector of residual norms (||F(x)||) for the iteration stats = named tuple of the history of (ifun, ijac, iarm), the number of functions/derivatives/steplength reductions at each iteration.\n",
       "\n",
       "I do not count the function values for a finite-difference derivative because they count toward a Jacobian evaluation. \n",
       "\n",
       "idid=true if the iteration succeeded and false if not.\n",
       "\n",
       "errcode = 0 if if the iteration succeeded         = -1 if the initial iterate satisifies the termination criteria         = 10 if no convergence after maxit iterations         = 1  if the line search failed\n",
       "\n",
       "solhist:\n",
       "\n",
       "This is the entire history of the iteration if you've set keepsolhist=true\n",
       "\n",
       "solhist is an N x K array where N is the length of x and K is the number of iteration + 1. So, for scalar equations, it's a row vector.\n"
      ],
      "text/plain": [
       "  ptcsol(F!, x0, FS, FPS, J! = diffjac!; rtol=1.e-6, atol=1.e-12, maxit=20,\n",
       "  dt0=1.e-6, dx=1.e-7, pdata = nothing, jfact = klfact, printerr = true,\n",
       "  keepsolhist = false)\n",
       "\n",
       "  C. T. Kelley, 2020\n",
       "\n",
       "  Julia versions of the nonlinear solvers from my SIAM books. Herewith: some\n",
       "  new stuff ==> ptcsol\n",
       "\n",
       "  You must allocate storage for the function and Jacobian in advance –> in the\n",
       "  calling program <– ie. in FS and FPS\n",
       "\n",
       "  Inputs:\n",
       "\n",
       "    •    F!: function evaluation, the ! indicates that F! overwrites FS,\n",
       "        your preallocated storage for the function.\n",
       "      \n",
       "        So, FV=F!(FV,x) or FV=F!(FV,x,pdata) returns FV=F(x)\n",
       "\n",
       "    •    x0: initial iterate\n",
       "\n",
       "    •    FS: Preallcoated storage for function. It is an N x 1 column\n",
       "        vector.\n",
       "\n",
       "  You may dimension it as (n,) or (n,1). (n,) is best, but the solvers can\n",
       "  deal with it either way.\n",
       "\n",
       "    •    FPS: preallcoated storage for Jacobian. It is an N x N matrix\n",
       "\n",
       "    •    J!: Jacobian evaluation, the ! indicates that J! overwrites FPS,\n",
       "        your preallocated storage for the Jacobian. If you leave this out\n",
       "        the default is a finite difference Jacobian.\n",
       "      \n",
       "        So, FP=J!(FP,FV,x) or FP=J!(FP,FV,x,pdata) returns FP=F'(x);\n",
       "        (FP,FV, x) must be the argument list, even if FP does not need FV.\n",
       "        One reason for this is that the finite-difference Jacobian does\n",
       "        and that is the default in the solver.\n",
       "\n",
       "    •    Precision: Lemme tell ya 'bout precision. I designed this code for\n",
       "        full precision functions and linear algebra in any precision you\n",
       "        want. You can declare FPS as Float64, Float32, or Float16 and\n",
       "        ptcsol will do the right thing if YOU do not destroy the\n",
       "        declaration in your J! function. I'm amazed that this works so\n",
       "        easily. If the Jacobian is reasonably well conditioned, I can see\n",
       "        no reason to do linear algebra in double precision I can see no\n",
       "        reason to do linear algebra in double precision for anything other\n",
       "        than horribly ill-conditioned problems.\n",
       "      \n",
       "        BUT ... There is very limited support for direct sparse solvers in\n",
       "        anything other than Float64. I recommend that you only use Float64\n",
       "        with direct sparse solvers unless you really know what you're\n",
       "        doing.\n",
       "\n",
       "  ────────────────────────────────────────────────────────────────────────────\n",
       "\n",
       "  Keyword Arguments (kwargs):\n",
       "\n",
       "  rtol and atol: relative and absolute error tolerances\n",
       "\n",
       "  dt0: initial time step. The default value of 1.e-3 is a bit conservative and\n",
       "  is one option you really should play with. Look at the example where I set\n",
       "  it to 1.0!\n",
       "\n",
       "  maxit: limit on nonlinear iterations, default=100. \n",
       "\n",
       "  This is coupled to dt0. If your choice of dt0 is too small (conservative)\n",
       "  then you'll need many iterations to converge and will need a larger value of\n",
       "  maxit\n",
       "\n",
       "  For PTC you'll need more iterations than for a stright-up nonlinear solve.\n",
       "  This is part of the price for finding the stable solution.\n",
       "\n",
       "  dx: default = 1.e-7\n",
       "\n",
       "  difference increment in finite-difference derivatives h=dx*norm(x)+1.e-6\n",
       "\n",
       "  pdata:\n",
       "\n",
       "  precomputed data for the function/Jacobian. Things will go better if you use\n",
       "  this rather than hide the data in global variables within the module for\n",
       "  your function/Jacobian\n",
       "\n",
       "  jfact: default = klfact (tries to figure out best choice) \n",
       "\n",
       "  If your Jacobian has any special structure, please set jfact to the correct\n",
       "  choice for a factorization.\n",
       "\n",
       "  I use jfact when I call PTCUpdate to evaluate the Jacobian (using your J!)\n",
       "  and factor it. The default is to use klfact (an internal function) to do\n",
       "  something reasonable. For general matrices, klfact picks lu! to compute an\n",
       "  LU factorization and share storage with the Jacobian. You may change LU to\n",
       "  something else by, for example, setting jfact = cholseky! if your Jacobian\n",
       "  is spd.\n",
       "\n",
       "  klfact knows about banded matrices and picks qr. You should, however RTFM,\n",
       "  allocate the extra two upper bands, and use jfact=qr! to override klfact.\n",
       "\n",
       "  If you give me something that klfact does not know how to dispatch on, then\n",
       "  nothing happens. I just return the original Jacobian matrix and ptcsol will\n",
       "  use backslash to compute the Newton step.\n",
       "\n",
       "  I know that this is probably not optimal in your situation, so it is good to\n",
       "  pick something else, like jfact = lu.\n",
       "\n",
       "  printerr: default = true\n",
       "\n",
       "  I print a helpful message when the solver fails. To supress that message set\n",
       "  printerr to false.\n",
       "\n",
       "  keepsolhist: default = false\n",
       "\n",
       "  Set this to true to get the history of the iteration in the output tuple.\n",
       "  This is on by default for scalar equations and off for systems. Only turn it\n",
       "  on if you have use for the data, which can get REALLY LARGE.\n",
       "\n",
       "  Output:\n",
       "\n",
       "  A named tuple (solution, functionval, history, stats, idid, errcode,\n",
       "  solhist) where\n",
       "\n",
       "  solution = converged result functionval = F(solution) history = the vector\n",
       "  of residual norms (||F(x)||) for the iteration stats = named tuple of the\n",
       "  history of (ifun, ijac, iarm), the number of\n",
       "  functions/derivatives/steplength reductions at each iteration.\n",
       "\n",
       "  I do not count the function values for a finite-difference derivative\n",
       "  because they count toward a Jacobian evaluation. \n",
       "\n",
       "  idid=true if the iteration succeeded and false if not.\n",
       "\n",
       "  errcode = 0 if if the iteration succeeded = -1 if the initial iterate\n",
       "  satisifies the termination criteria = 10 if no convergence after maxit\n",
       "  iterations = 1 if the line search failed\n",
       "\n",
       "  solhist:\n",
       "\n",
       "  This is the entire history of the iteration if you've set keepsolhist=true\n",
       "\n",
       "  solhist is an N x K array where N is the length of x and K is the number of\n",
       "  iteration + 1. So, for scalar equations, it's a row vector."
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?ptcsol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### More on the Buckling Beam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2.10 Projects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More on the H-equation\n",
    "\n",
    "- Run the H-equation tests with $c=.99$ and larger dimensions ($2^k$ for $k=12, 13, 14$). Are the values of ```sham``` and ```resdec``` optimal for this case? How, for example, is ```resdec = Inf``` (your author's favorite value). To see why I like ```sham=Inf``` have a look at\n",
    "<cite data-cite=\"brent\"><a href=\"siamfa.html#brent\">(Bre73)</cite>.\n",
    "\n",
    "- What happens with $c=1$? Is the chord method a good idea? \n",
    "Don't look at\n",
    "<cite data-cite=\"ctk:sirev20\"><a href=\"siamfa.html#ctk:sirev20\">(Kel20a)</cite>,\n",
    "<cite data-cite=\"ctk:n1\"><a href=\"siamfa.html#ctk:n1\">(DK80)</cite>, or\n",
    "<cite data-cite=\"ctk:chord\"><a href=\"siamfa.html#ctk:chord\">(DK83)</cite> before trying to figure things out on your own.\n",
    "\n",
    "- Increase the dimension as much as you can and compare single and double precision linear algebra.\n",
    "    \n",
    "- Duplicate the table on page 125 of <cite data-cite=\"chand\"><a href=\"siamfa.html#chand\">(Cha60)</cite>.    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic Differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(not trivial!)__ Try using __automatic differention__ to compute the Jacobian for the examples in this chapter and use the results to write a Jacobian evaluation function to pass to __nsol.jl__.\n",
    "You will first have to decide how you want to do that. Two of mature packages are [ForwardDiff.jl](#https://github.com/JuliaDiff/ForwardDiff.jl)\n",
    "<cite data-cite=\"forwarddiff\"><a href=\"siamfa.html#forwarddiff\">(RPL16)</cite>\n",
    "and [Zygote.jl](#https://github.com/FluxML/Zygote.jl) \n",
    "<cite data-cite=\"zygote\"><a href=\"siamfa.html#zygote\">(CoRR)</cite>.\n",
    "Books like\n",
    "<cite data-cite=\"grautodiff\"><a href=\"siamfa.html#grautodiff\">(Gri00)</cite> explain the\n",
    "algorithmic differences between these packages.\n",
    "    \n",
    "How does the perfomance compate to an analytic or forward difference Jacobian? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nested Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solving a differential or integral equation\n",
    "by __nested iteration__ or __grid sequencing__ means resolving\n",
    "the rough features of the solution of a differential or\n",
    "integral equation on a coarse mesh, interpolating the\n",
    "solution to a finer mesh, resolving on the finer mesh, and\n",
    "then repeating the process until you have a solution on a target\n",
    "mesh.\n",
    "\n",
    "Apply this idea to some of the examples in the text, using piecewise\n",
    "linear interpolation to move from coarse to fine meshes. If the\n",
    "discretization is second-order accurate and you halve the mesh\n",
    "width at each level, how should you terminate the solver at\n",
    "each level? What kind of iteration statistics would tell you that\n",
    "you've done a satisfactory job?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse Differencing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write Jacobian evaluation functions using sparse differencing for the boundary value problem and the buckling beam examples. You can use the literature, for exampe the methods from \n",
    "<cite data-cite=\"curtispr\"><a href=\"siamfa.html#curtisor\">(CPR74)</cite> or\n",
    "<cite data-cite=\"colmore\"><a href=\"siamfa.html#colmore\">(CM83)</cite>.  <cite data-cite=\"ctk:newton\"><a href=\"siamfa.html#ctk:newton\">(Kel03)</cite> has a Matlab code for banded matrices that, while convertable to Julila, needs some work to explicltly store the Jacobian as a BandedMatrix so\n",
    "[BandedMatrices.jl](https://github.com/JuliaMatrices/BandedMatrices.jl) will solve the linear system efficiently.\n",
    "Another alternative is to use __sparsefdiff__ \n",
    "<cite data-cite=\"sparsediff\"><a href=\"siamfa.html#sparsediff\">(RMGH20)</cite> from\n",
    "[SparseDiffTools.jl](#https://github.com/JuliaDiff/SparseDiffTools.jl). \n",
    "    \n",
    "No matter what you do, you'll need to think about fill-in, symbolic factorization in the general case, and storage. \n",
    "    \n",
    "Have fun!    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.5.2",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
