{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\newcommand{\\calf}{{\\cal F}}\n",
    "\\newcommand{\\dnu}{d \\nu}\n",
    "\\newcommand{\\mf}{{\\bf F}}\n",
    "\\newcommand{\\vu}{{\\bf u}}\n",
    "\\newcommand{\\vx}{{\\bf x}}\n",
    "\\newcommand{\\vy}{{\\bf y}}\n",
    "\\newcommand{\\vs}{{\\bf s}}\n",
    "\\newcommand{\\ve}{{\\bf e}}\n",
    "\\newcommand{\\vd}{{\\bf d}}\n",
    "\\newcommand{\\mg}{{\\bf G}}\n",
    "\\newcommand{\\ml}{{\\bf L}}\n",
    "\\newcommand{\\mg}{{\\bf G}}\n",
    "\\newcommand{\\mv}{{\\bf V}}\n",
    "\\newcommand{\\ma}{{\\bf A}}\n",
    "\\newcommand{\\mi}{{\\bf I}}\n",
    "\\newcommand{\\mm}{{\\bf M}}\n",
    "\\newcommand{\\ball}{{\\cal B}}\n",
    "\\newcommand{\\ptc}{{\\Psi TC}}\n",
    "\\newcommand{\\diag}{\\mbox{diag}}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "include(\"fanote_init.jl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2.8 Solvers for Chapter 2\n",
    "\n",
    "Contents for Section 2.8\n",
    "\n",
    "[Overview](#Overview)\n",
    "\n",
    "[nsol.jl](#nsol.jl)\n",
    "\n",
    "- [H-equation Revisited](#H-Equation-revisited)\n",
    "\n",
    "- [More on the Two-Point BVP](#More-on-the-Two--Point-BVP)\n",
    "\n",
    "[ptcsol.jl](#ptcsol.jl)\n",
    "\n",
    "- [More on the Buckling Beam](#Benchmarking-the-Buckling-Beam)\n",
    "\n",
    "[Section 2.9: Projects](#Section-2.9-Projects)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will follow the pattern of Chapter 1 and present two solvers, a Newton code and a $\\ptc$ code. Both codes are for systems of equations and use direct methods to compute the step. We returned the solution history for the simple two dimensional example in Section 2.6, but will not do that again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nsol.jl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__nsol.jl__ solves systems of nonlinear equations and computes the Newton step with direct linear solvers. Let's look at the docstrings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search: \u001b[0m\u001b[1mn\u001b[22m\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1ml\u001b[22m \u001b[0m\u001b[1mn\u001b[22m\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1ml\u001b[22msc \u001b[0m\u001b[1mn\u001b[22m\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1ml\u001b[22mheq tra\u001b[0m\u001b[1mn\u001b[22m\u001b[0m\u001b[1ms\u001b[22mc\u001b[0m\u001b[1mo\u001b[22mde tra\u001b[0m\u001b[1mn\u001b[22m\u001b[0m\u001b[1ms\u001b[22mp\u001b[0m\u001b[1mo\u001b[22mse Tra\u001b[0m\u001b[1mn\u001b[22m\u001b[0m\u001b[1ms\u001b[22mp\u001b[0m\u001b[1mo\u001b[22mse tra\u001b[0m\u001b[1mn\u001b[22m\u001b[0m\u001b[1ms\u001b[22mp\u001b[0m\u001b[1mo\u001b[22mse!\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "nsol(F!, x0, FS, FPS, J!=diffjac!; rtol=1.e-6, atol=1.e-12,\n",
       "           maxit=20, solver=\"newton\", sham=5, armmax=10, resdec=.1,\n",
       "           dx = 1.e-7, armfix=false, \n",
       "           pdata = nothing, jfact = klfact,\n",
       "           printerr = true, keepsolhist = false, stagnationok=false)\n",
       "\\end{verbatim}\n",
       ")\n",
       "\n",
       "C. T. Kelley, 2020\n",
       "\n",
       "Julia versions of the nonlinear solvers from my SIAM books.  Herewith: nsol\n",
       "\n",
       "You must allocate storage for the function and Jacobian in advance –> in the calling program <– ie. in FS and FPS\n",
       "\n",
       "Inputs:\n",
       "\n",
       "\\begin{itemize}\n",
       "\\item F!: function evaluation, the ! indicates that F! overwrites FS, your   preallocated storage for the function.\n",
       "\n",
       "So FS=F!(FS,x) or FS=F!(FS,x,pdata) returns FS=F(x)\n",
       "\n",
       "\\end{itemize}\n",
       "\\begin{itemize}\n",
       "\\item x0: initial iterate\n",
       "\n",
       "\\end{itemize}\n",
       "\\begin{itemize}\n",
       "\\item FS: Preallocated storage for function. It is an N x 1 column vector\n",
       "\n",
       "\\end{itemize}\n",
       "\\begin{itemize}\n",
       "\\item FPS: preallocated storage for Jacobian. It is an N x N matrix\n",
       "\n",
       "\\end{itemize}\n",
       "\\begin{itemize}\n",
       "\\item J!: Jacobian evaluation, the ! indicates that J! overwrites FPS, your   preallocated storage for the Jacobian. If you leave this out the   default is a finite difference Jacobian.\n",
       "\n",
       "So, FP=J!(FP,FS,x) or FP=J!(FP,FS,x,pdata) returns FP=F'(x). \n",
       "\n",
       "(FP,FS, x) must be the argument list, even if FP does not need FS.   One reason for this is that the finite-difference Jacobian   does and that is the default in the solver.\n",
       "\n",
       "\n",
       "\\item Precision: Lemme tell ya 'bout precision. I designed this code for    full precision functions and linear algebra in any precision you want.    You can declare   FPS as Float64, Float32, or Float16 and nsol will do the right thing if   YOU do not destroy the declaration in your J! function. I'm amazed   that this works so easily. If the Jacobian is reasonably well    conditioned, you can cut the cost of Jacobian factorization and   storage in half with no loss. For large dense Jacobians and inexpensive   functions, this is a good deal.\n",
       "\n",
       "BUT ... There is very limited support for direct sparse solvers in   anything other than Float64. I recommend that you only use Float64   with direct sparse solvers unless you really know what you're doing. I   have a couple examples in the notebook, but watch out.\n",
       "\n",
       "\\end{itemize}\n",
       "\\rule{\\textwidth}{1pt}\n",
       "Keyword Arguments (kwargs):\n",
       "\n",
       "\\begin{itemize}\n",
       "\\item rtol and atol: relative and absolute error tolerances\n",
       "\n",
       "\\end{itemize}\n",
       "\\begin{itemize}\n",
       "\\item maxit: limit on nonlinear iterations\n",
       "\n",
       "\\end{itemize}\n",
       "solver: default = \"newton\"\n",
       "\n",
       "Your choices are \"newton\" or \"chord\". However, you have sham at your disposal only if you chose newton. \"chord\" will keep using the initial derivative until the iterate converges, uses the iteration budget, or the line search fails. It is not the same as sham=Inf, which is smarter.\n",
       "\n",
       "sham: default = 5 (ie Newton)\n",
       "\n",
       "This is the Shamanskii method. If sham=1, you have Newton. The iteration updates the derivative every sham iterations. The convergence rate has local q-order sham+1 if you only count iterations where you update the derivative. You need not provide your own derivative function to use this option. sham=Inf is chord only if chord is converging well.\n",
       "\n",
       "I made sham=1 the default for scalar equations. For systems I'm more aggressive and want to invest as little energy in linear algebra as possible. So the default is sham=5.\n",
       "\n",
       "armmax: upper bound on step size reductions in line search\n",
       "\n",
       "resdec: default = .1\n",
       "\n",
       "This is the target value for residual reduction. The default value is .1. In the old MATLAB codes it was .5. I only turn Shamanskii on if the residuals are decreasing rapidly, at least a factor of resdec, and the line search is quiescent. If you want to eliminate resdec from the method ( you don't ) then set resdec = 1.0 and you will never hear from it again.\n",
       "\n",
       "dx: default = 1.e-7\n",
       "\n",
       "difference increment in finite-difference derivatives       h=dx*norm(x,Inf)+1.e-8\n",
       "\n",
       "armfix: default = false\n",
       "\n",
       "The default is a parabolic line search (ie false). Set to true and the step size will be fixed at .5. Don't do this unless you are doing experiments for research.\n",
       "\n",
       "pdata:\n",
       "\n",
       "precomputed data for the function/Jacobian.  Things will go better if you use this rather than hide the data  in global variables within the module for your function/Jacobian\n",
       "\n",
       "jfact: default = klfact (tries to figure out best choice) \n",
       "\n",
       "If your Jacobian has any special structure, please set jfact to the correct choice for a factorization.\n",
       "\n",
       "I use jfact when I call PrepareJac! to evaluate the Jacobian (using your J!) and factor it. The default is to use klfact (an internal function) to do something reasonable. For general matrices, klfact picks lu! to compute an LU factorization and share storage with the Jacobian.  You may change LU to something else by, for example, setting jfact = cholseky! if your Jacobian is spd.\n",
       "\n",
       "klfact knows about banded matrices and picks qr. You should, however RTFM, allocate the extra two upper bands, and use jfact=qr! to override klfact.\n",
       "\n",
       "If you give me something that klfact does not know how to dispatch on, then nothing happens. I just return the original Jacobian matrix and  nsol will use backslash to compute the Newton step. I know that this is probably not optimal in your situation, so it is  good to pick something else, like jfact = lu.\n",
       "\n",
       "Please do not mess with the line that calls PrepareJac!. \n",
       "\n",
       "\\begin{verbatim}\n",
       "    FPF = PrepareJac!(FPS, FS, x, ItRules)\n",
       "\\end{verbatim}\n",
       "FPF is not the same as FPS (the storage you allocate for the Jacobian) for a reason. FPF and FPS do not have the same type, even though they share storage. So, FPS=PrepareJac!(FPS, FS, ...) will break things.\n",
       "\n",
       "printerr: default = true\n",
       "\n",
       "I print a helpful message when the solver fails. To suppress that message set printerr to false.\n",
       "\n",
       "keepsolhist: default = false\n",
       "\n",
       "Set this to true to get the history of the iteration in the output tuple. This is on by default for scalar equations and off for systems. Only turn it on if you have use for the data, which can get REALLY LARGE.\n",
       "\n",
       "stagnationok: default = false\n",
       "\n",
       "Set this to true if you want to disable the line search and either observe divergence or stagnation. This is only useful for research or writing a book.\n",
       "\n",
       "Output:\n",
       "\n",
       "A named tuple (solution, functionval, history, stats, idid,                errcode, solhist) where\n",
       "\n",
       "solution = converged result functionval = F(solution) history = the vector of residual norms (||F(x)||) for the iteration stats = named tuple of the history of (ifun, ijac, iarm), the number of functions/derivatives/steplength reductions at each iteration.\n",
       "\n",
       "I do not count the function values for a finite-difference derivative because they count toward a Jacobian evaluation. \n",
       "\n",
       "idid=true if the iteration succeeded and false if not.\n",
       "\n",
       "errcode = 0 if if the iteration succeeded         = -1 if the initial iterate satisfies the termination criteria         = 10 if no convergence after maxit iterations         = 1  if the line search failed\n",
       "\n",
       "solhist:\n",
       "\n",
       "This is the entire history of the iteration if you've set keepsolhist=true\n",
       "\n",
       "solhist is an N x K array where N is the length of x and K is the number of iteration + 1. So, for scalar equations, it's a row vector.\n",
       "\n",
       "\\rule{\\textwidth}{1pt}\n",
       "\\section{Examples}\n",
       "\\paragraph{World's easiest problem example. Test 64 and 32 bit Jacobians. No meaningful difference in the residual histories or the converged solutions.}\n",
       "\\begin{verbatim}\n",
       " julia> function f!(fv,x)\n",
       "       fv[1]=x[1] + sin(x[2])\n",
       "       fv[2]=cos(x[1]+x[2])\n",
       "       end\n",
       "f (generic function with 1 method)\n",
       "\n",
       "julia> x=ones(2,); fv=zeros(2,); jv=zeros(2,2); jv32=zeros(Float32,2,2);\n",
       "julia> nout=nsol(f!,x,fv,jv; sham=1);\n",
       "julia> nout32=nsol(f!,x,fv,jv32; sham=1);\n",
       "julia> [nout.history nout32.history]\n",
       "5×2 Array{Float64,2}:\n",
       " 1.88791e+00  1.88791e+00\n",
       " 2.43119e-01  2.43119e-01\n",
       " 1.19231e-02  1.19231e-02\n",
       " 1.03266e-05  1.03262e-05\n",
       " 1.46416e-11  1.43548e-11\n",
       "\n",
       "julia> [nout.solution nout.solution - nout32.solution]\n",
       "2×2 Array{Float64,2}:\n",
       " -7.39085e-01  -5.42899e-14\n",
       "  2.30988e+00   3.49498e-13\n",
       "\\end{verbatim}\n",
       "\\paragraph{H-equation example. I'm taking the sham=5 default here, so the convergence is not quadratic. The good news is that we evaluate the Jacobian only once.}\n",
       "\\begin{verbatim}\n",
       "julia> n=16; x0=ones(n,); FV=ones(n,); JV=ones(n,n);\n",
       "julia> hdata=heqinit(x0, .5);\n",
       "julia> hout=nsol(heqf!,x0,FV,JV;pdata=hdata);\n",
       "julia> hout.history\n",
       "4-element Array{Float64,1}:\n",
       " 6.17376e-01\n",
       " 3.17810e-03\n",
       " 2.75227e-05\n",
       " 2.35817e-07\n",
       "\\end{verbatim}\n"
      ],
      "text/markdown": [
       "```\n",
       "nsol(F!, x0, FS, FPS, J!=diffjac!; rtol=1.e-6, atol=1.e-12,\n",
       "           maxit=20, solver=\"newton\", sham=5, armmax=10, resdec=.1,\n",
       "           dx = 1.e-7, armfix=false, \n",
       "           pdata = nothing, jfact = klfact,\n",
       "           printerr = true, keepsolhist = false, stagnationok=false)\n",
       "```\n",
       "\n",
       ")\n",
       "\n",
       "C. T. Kelley, 2020\n",
       "\n",
       "Julia versions of the nonlinear solvers from my SIAM books.  Herewith: nsol\n",
       "\n",
       "You must allocate storage for the function and Jacobian in advance –> in the calling program <– ie. in FS and FPS\n",
       "\n",
       "Inputs:\n",
       "\n",
       "  * F!: function evaluation, the ! indicates that F! overwrites FS, your   preallocated storage for the function.\n",
       "\n",
       "    So FS=F!(FS,x) or FS=F!(FS,x,pdata) returns FS=F(x)\n",
       "\n",
       "  * x0: initial iterate\n",
       "\n",
       "  * FS: Preallocated storage for function. It is an N x 1 column vector\n",
       "\n",
       "  * FPS: preallocated storage for Jacobian. It is an N x N matrix\n",
       "\n",
       "  * J!: Jacobian evaluation, the ! indicates that J! overwrites FPS, your   preallocated storage for the Jacobian. If you leave this out the   default is a finite difference Jacobian.\n",
       "\n",
       "    So, FP=J!(FP,FS,x) or FP=J!(FP,FS,x,pdata) returns FP=F'(x). \n",
       "\n",
       "    (FP,FS, x) must be the argument list, even if FP does not need FS.   One reason for this is that the finite-difference Jacobian   does and that is the default in the solver.\n",
       "  * Precision: Lemme tell ya 'bout precision. I designed this code for    full precision functions and linear algebra in any precision you want.    You can declare   FPS as Float64, Float32, or Float16 and nsol will do the right thing if   YOU do not destroy the declaration in your J! function. I'm amazed   that this works so easily. If the Jacobian is reasonably well    conditioned, you can cut the cost of Jacobian factorization and   storage in half with no loss. For large dense Jacobians and inexpensive   functions, this is a good deal.\n",
       "\n",
       "    BUT ... There is very limited support for direct sparse solvers in   anything other than Float64. I recommend that you only use Float64   with direct sparse solvers unless you really know what you're doing. I   have a couple examples in the notebook, but watch out.\n",
       "\n",
       "---\n",
       "\n",
       "Keyword Arguments (kwargs):\n",
       "\n",
       "  * rtol and atol: relative and absolute error tolerances\n",
       "\n",
       "  * maxit: limit on nonlinear iterations\n",
       "\n",
       "solver: default = \"newton\"\n",
       "\n",
       "Your choices are \"newton\" or \"chord\". However, you have sham at your disposal only if you chose newton. \"chord\" will keep using the initial derivative until the iterate converges, uses the iteration budget, or the line search fails. It is not the same as sham=Inf, which is smarter.\n",
       "\n",
       "sham: default = 5 (ie Newton)\n",
       "\n",
       "This is the Shamanskii method. If sham=1, you have Newton. The iteration updates the derivative every sham iterations. The convergence rate has local q-order sham+1 if you only count iterations where you update the derivative. You need not provide your own derivative function to use this option. sham=Inf is chord only if chord is converging well.\n",
       "\n",
       "I made sham=1 the default for scalar equations. For systems I'm more aggressive and want to invest as little energy in linear algebra as possible. So the default is sham=5.\n",
       "\n",
       "armmax: upper bound on step size reductions in line search\n",
       "\n",
       "resdec: default = .1\n",
       "\n",
       "This is the target value for residual reduction. The default value is .1. In the old MATLAB codes it was .5. I only turn Shamanskii on if the residuals are decreasing rapidly, at least a factor of resdec, and the line search is quiescent. If you want to eliminate resdec from the method ( you don't ) then set resdec = 1.0 and you will never hear from it again.\n",
       "\n",
       "dx: default = 1.e-7\n",
       "\n",
       "difference increment in finite-difference derivatives       h=dx*norm(x,Inf)+1.e-8\n",
       "\n",
       "armfix: default = false\n",
       "\n",
       "The default is a parabolic line search (ie false). Set to true and the step size will be fixed at .5. Don't do this unless you are doing experiments for research.\n",
       "\n",
       "pdata:\n",
       "\n",
       "precomputed data for the function/Jacobian.  Things will go better if you use this rather than hide the data  in global variables within the module for your function/Jacobian\n",
       "\n",
       "jfact: default = klfact (tries to figure out best choice) \n",
       "\n",
       "If your Jacobian has any special structure, please set jfact to the correct choice for a factorization.\n",
       "\n",
       "I use jfact when I call PrepareJac! to evaluate the Jacobian (using your J!) and factor it. The default is to use klfact (an internal function) to do something reasonable. For general matrices, klfact picks lu! to compute an LU factorization and share storage with the Jacobian.  You may change LU to something else by, for example, setting jfact = cholseky! if your Jacobian is spd.\n",
       "\n",
       "klfact knows about banded matrices and picks qr. You should, however RTFM, allocate the extra two upper bands, and use jfact=qr! to override klfact.\n",
       "\n",
       "If you give me something that klfact does not know how to dispatch on, then nothing happens. I just return the original Jacobian matrix and  nsol will use backslash to compute the Newton step. I know that this is probably not optimal in your situation, so it is  good to pick something else, like jfact = lu.\n",
       "\n",
       "Please do not mess with the line that calls PrepareJac!. \n",
       "\n",
       "```\n",
       "    FPF = PrepareJac!(FPS, FS, x, ItRules)\n",
       "```\n",
       "\n",
       "FPF is not the same as FPS (the storage you allocate for the Jacobian) for a reason. FPF and FPS do not have the same type, even though they share storage. So, FPS=PrepareJac!(FPS, FS, ...) will break things.\n",
       "\n",
       "printerr: default = true\n",
       "\n",
       "I print a helpful message when the solver fails. To suppress that message set printerr to false.\n",
       "\n",
       "keepsolhist: default = false\n",
       "\n",
       "Set this to true to get the history of the iteration in the output tuple. This is on by default for scalar equations and off for systems. Only turn it on if you have use for the data, which can get REALLY LARGE.\n",
       "\n",
       "stagnationok: default = false\n",
       "\n",
       "Set this to true if you want to disable the line search and either observe divergence or stagnation. This is only useful for research or writing a book.\n",
       "\n",
       "Output:\n",
       "\n",
       "A named tuple (solution, functionval, history, stats, idid,                errcode, solhist) where\n",
       "\n",
       "solution = converged result functionval = F(solution) history = the vector of residual norms (||F(x)||) for the iteration stats = named tuple of the history of (ifun, ijac, iarm), the number of functions/derivatives/steplength reductions at each iteration.\n",
       "\n",
       "I do not count the function values for a finite-difference derivative because they count toward a Jacobian evaluation. \n",
       "\n",
       "idid=true if the iteration succeeded and false if not.\n",
       "\n",
       "errcode = 0 if if the iteration succeeded         = -1 if the initial iterate satisfies the termination criteria         = 10 if no convergence after maxit iterations         = 1  if the line search failed\n",
       "\n",
       "solhist:\n",
       "\n",
       "This is the entire history of the iteration if you've set keepsolhist=true\n",
       "\n",
       "solhist is an N x K array where N is the length of x and K is the number of iteration + 1. So, for scalar equations, it's a row vector.\n",
       "\n",
       "---\n",
       "\n",
       "# Examples\n",
       "\n",
       "#### World's easiest problem example. Test 64 and 32 bit Jacobians. No meaningful difference in the residual histories or the converged solutions.\n",
       "\n",
       "```jldoctest\n",
       " julia> function f!(fv,x)\n",
       "       fv[1]=x[1] + sin(x[2])\n",
       "       fv[2]=cos(x[1]+x[2])\n",
       "       end\n",
       "f (generic function with 1 method)\n",
       "\n",
       "julia> x=ones(2,); fv=zeros(2,); jv=zeros(2,2); jv32=zeros(Float32,2,2);\n",
       "julia> nout=nsol(f!,x,fv,jv; sham=1);\n",
       "julia> nout32=nsol(f!,x,fv,jv32; sham=1);\n",
       "julia> [nout.history nout32.history]\n",
       "5×2 Array{Float64,2}:\n",
       " 1.88791e+00  1.88791e+00\n",
       " 2.43119e-01  2.43119e-01\n",
       " 1.19231e-02  1.19231e-02\n",
       " 1.03266e-05  1.03262e-05\n",
       " 1.46416e-11  1.43548e-11\n",
       "\n",
       "julia> [nout.solution nout.solution - nout32.solution]\n",
       "2×2 Array{Float64,2}:\n",
       " -7.39085e-01  -5.42899e-14\n",
       "  2.30988e+00   3.49498e-13\n",
       "```\n",
       "\n",
       "#### H-equation example. I'm taking the sham=5 default here, so the convergence is not quadratic. The good news is that we evaluate the Jacobian only once.\n",
       "\n",
       "```jldoctest\n",
       "julia> n=16; x0=ones(n,); FV=ones(n,); JV=ones(n,n);\n",
       "julia> hdata=heqinit(x0, .5);\n",
       "julia> hout=nsol(heqf!,x0,FV,JV;pdata=hdata);\n",
       "julia> hout.history\n",
       "4-element Array{Float64,1}:\n",
       " 6.17376e-01\n",
       " 3.17810e-03\n",
       " 2.75227e-05\n",
       " 2.35817e-07\n",
       "```\n"
      ],
      "text/plain": [
       "\u001b[36m  nsol(F!, x0, FS, FPS, J!=diffjac!; rtol=1.e-6, atol=1.e-12,\u001b[39m\n",
       "\u001b[36m             maxit=20, solver=\"newton\", sham=5, armmax=10, resdec=.1,\u001b[39m\n",
       "\u001b[36m             dx = 1.e-7, armfix=false, \u001b[39m\n",
       "\u001b[36m             pdata = nothing, jfact = klfact,\u001b[39m\n",
       "\u001b[36m             printerr = true, keepsolhist = false, stagnationok=false)\u001b[39m\n",
       "\n",
       "  )\n",
       "\n",
       "  C. T. Kelley, 2020\n",
       "\n",
       "  Julia versions of the nonlinear solvers from my SIAM books. Herewith: nsol\n",
       "\n",
       "  You must allocate storage for the function and Jacobian in advance –> in the\n",
       "  calling program <– ie. in FS and FPS\n",
       "\n",
       "  Inputs:\n",
       "\n",
       "    •    F!: function evaluation, the ! indicates that F! overwrites FS,\n",
       "        your preallocated storage for the function.\n",
       "      \n",
       "        So FS=F!(FS,x) or FS=F!(FS,x,pdata) returns FS=F(x)\n",
       "\n",
       "    •    x0: initial iterate\n",
       "\n",
       "    •    FS: Preallocated storage for function. It is an N x 1 column\n",
       "        vector\n",
       "\n",
       "    •    FPS: preallocated storage for Jacobian. It is an N x N matrix\n",
       "\n",
       "    •    J!: Jacobian evaluation, the ! indicates that J! overwrites FPS,\n",
       "        your preallocated storage for the Jacobian. If you leave this out\n",
       "        the default is a finite difference Jacobian.\n",
       "      \n",
       "        So, FP=J!(FP,FS,x) or FP=J!(FP,FS,x,pdata) returns FP=F'(x). \n",
       "      \n",
       "        (FP,FS, x) must be the argument list, even if FP does not need FS.\n",
       "        One reason for this is that the finite-difference Jacobian does\n",
       "        and that is the default in the solver.\n",
       "\n",
       "    •    Precision: Lemme tell ya 'bout precision. I designed this code for\n",
       "        full precision functions and linear algebra in any precision you\n",
       "        want. You can declare FPS as Float64, Float32, or Float16 and nsol\n",
       "        will do the right thing if YOU do not destroy the declaration in\n",
       "        your J! function. I'm amazed that this works so easily. If the\n",
       "        Jacobian is reasonably well conditioned, you can cut the cost of\n",
       "        Jacobian factorization and storage in half with no loss. For large\n",
       "        dense Jacobians and inexpensive functions, this is a good deal.\n",
       "      \n",
       "        BUT ... There is very limited support for direct sparse solvers in\n",
       "        anything other than Float64. I recommend that you only use Float64\n",
       "        with direct sparse solvers unless you really know what you're\n",
       "        doing. I have a couple examples in the notebook, but watch out.\n",
       "\n",
       "  ────────────────────────────────────────────────────────────────────────────\n",
       "\n",
       "  Keyword Arguments (kwargs):\n",
       "\n",
       "    •    rtol and atol: relative and absolute error tolerances\n",
       "\n",
       "    •    maxit: limit on nonlinear iterations\n",
       "\n",
       "  solver: default = \"newton\"\n",
       "\n",
       "  Your choices are \"newton\" or \"chord\". However, you have sham at your\n",
       "  disposal only if you chose newton. \"chord\" will keep using the initial\n",
       "  derivative until the iterate converges, uses the iteration budget, or the\n",
       "  line search fails. It is not the same as sham=Inf, which is smarter.\n",
       "\n",
       "  sham: default = 5 (ie Newton)\n",
       "\n",
       "  This is the Shamanskii method. If sham=1, you have Newton. The iteration\n",
       "  updates the derivative every sham iterations. The convergence rate has local\n",
       "  q-order sham+1 if you only count iterations where you update the derivative.\n",
       "  You need not provide your own derivative function to use this option.\n",
       "  sham=Inf is chord only if chord is converging well.\n",
       "\n",
       "  I made sham=1 the default for scalar equations. For systems I'm more\n",
       "  aggressive and want to invest as little energy in linear algebra as\n",
       "  possible. So the default is sham=5.\n",
       "\n",
       "  armmax: upper bound on step size reductions in line search\n",
       "\n",
       "  resdec: default = .1\n",
       "\n",
       "  This is the target value for residual reduction. The default value is .1. In\n",
       "  the old MATLAB codes it was .5. I only turn Shamanskii on if the residuals\n",
       "  are decreasing rapidly, at least a factor of resdec, and the line search is\n",
       "  quiescent. If you want to eliminate resdec from the method ( you don't )\n",
       "  then set resdec = 1.0 and you will never hear from it again.\n",
       "\n",
       "  dx: default = 1.e-7\n",
       "\n",
       "  difference increment in finite-difference derivatives h=dx*norm(x,Inf)+1.e-8\n",
       "\n",
       "  armfix: default = false\n",
       "\n",
       "  The default is a parabolic line search (ie false). Set to true and the step\n",
       "  size will be fixed at .5. Don't do this unless you are doing experiments for\n",
       "  research.\n",
       "\n",
       "  pdata:\n",
       "\n",
       "  precomputed data for the function/Jacobian. Things will go better if you use\n",
       "  this rather than hide the data in global variables within the module for\n",
       "  your function/Jacobian\n",
       "\n",
       "  jfact: default = klfact (tries to figure out best choice) \n",
       "\n",
       "  If your Jacobian has any special structure, please set jfact to the correct\n",
       "  choice for a factorization.\n",
       "\n",
       "  I use jfact when I call PrepareJac! to evaluate the Jacobian (using your J!)\n",
       "  and factor it. The default is to use klfact (an internal function) to do\n",
       "  something reasonable. For general matrices, klfact picks lu! to compute an\n",
       "  LU factorization and share storage with the Jacobian. You may change LU to\n",
       "  something else by, for example, setting jfact = cholseky! if your Jacobian\n",
       "  is spd.\n",
       "\n",
       "  klfact knows about banded matrices and picks qr. You should, however RTFM,\n",
       "  allocate the extra two upper bands, and use jfact=qr! to override klfact.\n",
       "\n",
       "  If you give me something that klfact does not know how to dispatch on, then\n",
       "  nothing happens. I just return the original Jacobian matrix and nsol will\n",
       "  use backslash to compute the Newton step. I know that this is probably not\n",
       "  optimal in your situation, so it is good to pick something else, like jfact\n",
       "  = lu.\n",
       "\n",
       "  Please do not mess with the line that calls PrepareJac!. \n",
       "\n",
       "\u001b[36m      FPF = PrepareJac!(FPS, FS, x, ItRules)\u001b[39m\n",
       "\n",
       "  FPF is not the same as FPS (the storage you allocate for the Jacobian) for a\n",
       "  reason. FPF and FPS do not have the same type, even though they share\n",
       "  storage. So, FPS=PrepareJac!(FPS, FS, ...) will break things.\n",
       "\n",
       "  printerr: default = true\n",
       "\n",
       "  I print a helpful message when the solver fails. To suppress that message\n",
       "  set printerr to false.\n",
       "\n",
       "  keepsolhist: default = false\n",
       "\n",
       "  Set this to true to get the history of the iteration in the output tuple.\n",
       "  This is on by default for scalar equations and off for systems. Only turn it\n",
       "  on if you have use for the data, which can get REALLY LARGE.\n",
       "\n",
       "  stagnationok: default = false\n",
       "\n",
       "  Set this to true if you want to disable the line search and either observe\n",
       "  divergence or stagnation. This is only useful for research or writing a\n",
       "  book.\n",
       "\n",
       "  Output:\n",
       "\n",
       "  A named tuple (solution, functionval, history, stats, idid, errcode,\n",
       "  solhist) where\n",
       "\n",
       "  solution = converged result functionval = F(solution) history = the vector\n",
       "  of residual norms (||F(x)||) for the iteration stats = named tuple of the\n",
       "  history of (ifun, ijac, iarm), the number of\n",
       "  functions/derivatives/steplength reductions at each iteration.\n",
       "\n",
       "  I do not count the function values for a finite-difference derivative\n",
       "  because they count toward a Jacobian evaluation. \n",
       "\n",
       "  idid=true if the iteration succeeded and false if not.\n",
       "\n",
       "  errcode = 0 if if the iteration succeeded = -1 if the initial iterate\n",
       "  satisfies the termination criteria = 10 if no convergence after maxit\n",
       "  iterations = 1 if the line search failed\n",
       "\n",
       "  solhist:\n",
       "\n",
       "  This is the entire history of the iteration if you've set keepsolhist=true\n",
       "\n",
       "  solhist is an N x K array where N is the length of x and K is the number of\n",
       "  iteration + 1. So, for scalar equations, it's a row vector.\n",
       "\n",
       "  ────────────────────────────────────────────────────────────────────────────\n",
       "\n",
       "\u001b[1m  Examples\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡≡≡≡\u001b[22m\n",
       "\n",
       "\u001b[1m  World's easiest problem example. Test 64 and 32 bit Jacobians. No\u001b[22m\n",
       "\u001b[1m meaningful difference in the residual histories or the converged\u001b[22m\n",
       "\u001b[1m solutions.\u001b[22m\n",
       "\u001b[1m  --------------------------\u001b[22m\n",
       "\n",
       "\u001b[36m   julia> function f!(fv,x)\u001b[39m\n",
       "\u001b[36m         fv[1]=x[1] + sin(x[2])\u001b[39m\n",
       "\u001b[36m         fv[2]=cos(x[1]+x[2])\u001b[39m\n",
       "\u001b[36m         end\u001b[39m\n",
       "\u001b[36m  f (generic function with 1 method)\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> x=ones(2,); fv=zeros(2,); jv=zeros(2,2); jv32=zeros(Float32,2,2);\u001b[39m\n",
       "\u001b[36m  julia> nout=nsol(f!,x,fv,jv; sham=1);\u001b[39m\n",
       "\u001b[36m  julia> nout32=nsol(f!,x,fv,jv32; sham=1);\u001b[39m\n",
       "\u001b[36m  julia> [nout.history nout32.history]\u001b[39m\n",
       "\u001b[36m  5×2 Array{Float64,2}:\u001b[39m\n",
       "\u001b[36m   1.88791e+00  1.88791e+00\u001b[39m\n",
       "\u001b[36m   2.43119e-01  2.43119e-01\u001b[39m\n",
       "\u001b[36m   1.19231e-02  1.19231e-02\u001b[39m\n",
       "\u001b[36m   1.03266e-05  1.03262e-05\u001b[39m\n",
       "\u001b[36m   1.46416e-11  1.43548e-11\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> [nout.solution nout.solution - nout32.solution]\u001b[39m\n",
       "\u001b[36m  2×2 Array{Float64,2}:\u001b[39m\n",
       "\u001b[36m   -7.39085e-01  -5.42899e-14\u001b[39m\n",
       "\u001b[36m    2.30988e+00   3.49498e-13\u001b[39m\n",
       "\n",
       "\u001b[1m  H-equation example. I'm taking the sham=5 default here, so the\u001b[22m\n",
       "\u001b[1m convergence is not quadratic. The good news is that we evaluate the\u001b[22m\n",
       "\u001b[1m Jacobian only once.\u001b[22m\n",
       "\u001b[1m  --------------------------\u001b[22m\n",
       "\n",
       "\u001b[36m  julia> n=16; x0=ones(n,); FV=ones(n,); JV=ones(n,n);\u001b[39m\n",
       "\u001b[36m  julia> hdata=heqinit(x0, .5);\u001b[39m\n",
       "\u001b[36m  julia> hout=nsol(heqf!,x0,FV,JV;pdata=hdata);\u001b[39m\n",
       "\u001b[36m  julia> hout.history\u001b[39m\n",
       "\u001b[36m  4-element Array{Float64,1}:\u001b[39m\n",
       "\u001b[36m   6.17376e-01\u001b[39m\n",
       "\u001b[36m   3.17810e-03\u001b[39m\n",
       "\u001b[36m   2.75227e-05\u001b[39m\n",
       "\u001b[36m   2.35817e-07\u001b[39m"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?nsol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input\n",
    "\n",
    "The calling sequence for the Newton solvers in this book are similar, differing mostly in the management of the linear solver and memory allocation. The calling sequence for __nsold.jl__ is\n",
    "\n",
    "```julia\n",
    "function nsold(\n",
    "    F!,\n",
    "    x0,\n",
    "    FS,\n",
    "    FPS,\n",
    "    J! = diffjac!;\n",
    "    rtol = 1.e-6,\n",
    "    atol = 1.e-12,\n",
    "    maxit = 20,\n",
    "    solver = \"newton\",\n",
    "    sham = 1,\n",
    "    armmax = 10,\n",
    "    resdec = 0.1,\n",
    "    dx = 1.e-7,\n",
    "    armfix = false,\n",
    "    pdata = nothing,\n",
    "    jfact = lu!,\n",
    "    printerr = true,\n",
    "    keepsolhist = false,\n",
    "    stagnationok = false,\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we said earlier in the chapter, the calling sequence has some new things which are not in __nsolsc.jl__. The most significant are the arrays __FS__ and __FPS__, which preallocate\n",
    "storage for the function and Jacobian. As we have pointed out earlier, the farther upstream one allocates memory the better, so __nsold.jl__ insists that you allocate an vector __FS__ of the same size as the initial iterate and a matrix \n",
    "__FPS__ for the Jacobian. \n",
    "\n",
    "The other major new feature is the keyword argument __pdata__. This is the data structure for you to store any precomputed or preallocated data your function evaluation needs. You will almost surely need __pdata__ for any but the most trivial problems. The H-equation example from section 2.6 uses __pdata__ in a serious manner.\n",
    "\n",
    "You may dimension __x0__ either as $(N,1)$ or $(N,)$, but you must be consistent and dimension __FS__ the same way. __nsold.jl__ expects vectors to be in double precision (Float64). \n",
    "\n",
    "The __!__ in the function evaluation __F!__ is to indicate that __nsold__ expects __F__  to overwrite its input. So,\n",
    "the way to call __F!__ is to preallocate the storage for the function value in an array __FS__ and then call the function as\n",
    "```Julia\n",
    "F!(FS,x)\n",
    "```\n",
    "or\n",
    "```Julia\n",
    "F!(FS,x,pdata)\n",
    "```\n",
    "__nsold.jl__ will figure out if you have populated __pdata__ or left it alone as the default value of __nothing__.\n",
    "\n",
    "\n",
    "And now for the Jacobian. __nsold.jl__ uses direct methods for linear algebra. If your matrix is dense, the default is to use Julia's __lu!__ function to do an LU factorization. If your matrix is symmetric or symmetric positive definite you can use the __factorization__ keyword to change __lu!__ to __ldlt!__ or __cholesky!__ for example. __nsold.jl__ assumes that the factorization you ask for will overwrite the matrix. Hence, the __factorize__ function in Julia is not what you want for this application.\n",
    "\n",
    "You will also need to preallocate storage for the Jacobian in the array __FPS__. You may use any legal real precision for __FPS__. Float64 is the default. If you use Float32 you cut the storage for the matrix and the time for the factorization in half. We recommend that you do this if your Jacobian is dense. If you are using the __Sparsesuite__ sparse solvers, then you must store the Jacobian in double precision. __Sparsesuite__ does not support lower precision.\n",
    "\n",
    "Your Jacobian computation __J!__ must also overwrite it's input. The call looks like\n",
    "```julia\n",
    "J!(FV,FP,x)\n",
    "```\n",
    "or \n",
    "```julia\n",
    "J!(FV,FP,x,pdata)\n",
    "\n",
    "```\n",
    "returns FP=F'(x). The input FP=F(x), which __nsold.jl__ has already computed, has to be there.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  H-Equation revisited"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we do several experiments to illustrate the advantages of infrequent evaluation and factorization of the Jacobian and mixed precision computation. We will begin with a function that will support our testing. We want to investigate combinations of Newton's method/Shamanskii with ```sham=5``` (the default), storing and factoring the Jacobian in double/single precision, and analytic/forward difference Jacobians. To make this easy we write a function that solves the H-equation with __nsol.jl__ and lets us vary these cases. \n",
    "\n",
    "The functions for the residual __heqf!.jl__, the Jacobian __heqJ!.jl__, and the precomputed data \n",
    "__heqinit.jl__ are in the large file \n",
    "[src/TestProblems/Systems/Hequation.jl](https://github.com/ctkelley/SIAMFANLEquations.jl/blob/master/src/TestProblems/Systems/Hequation.jl)\n",
    "in the \n",
    "[SIAMFANLEquations.jl](https://github.com/ctkelley/SIAMFANLEquations.jl)\n",
    "repository. \n",
    "\n",
    "I'm passing the precomputed data to the function rather than computing it within. This keeps the cost of the precomputed data out of the benchmarking I'll do later.\n",
    "\n",
    "I'm also using __splat__. I populate a named tuple ```bargs``` to keep the keyword arguments in a convenient place and then, when it's time to give it to __nsol.jl__, the call looks like ```bargs...```. The three dots are the __splat__ and tell __nsol__ to expand bargs and harvest the keyword arguments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "htest (generic function with 1 method)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function htest(x0, FS, FPS, hdata; analytic=false, hsham=5)\n",
    "    n=length(FS)\n",
    "    #\n",
    "    # I've preallocaed x0, FS, and FPS. But they may have been changed by previous runs.\n",
    "    # The cost of resetting their entries to 1.0 is insignificant. \n",
    "    #\n",
    "    FS.=1.0\n",
    "    FPS.=1.0\n",
    "    bargs=(atol = 1.e-10, rtol = 1.e-10, sham = hsham, resdec = .1, pdata=hdata)\n",
    "    if analytic\n",
    "        nout=nsol( heqf!, x0, FS, FPS, heqJ!; bargs...)\n",
    "    else\n",
    "        nout=nsol( heqf!, x0, FS, FPS; bargs...)\n",
    "    end\n",
    "    return nout\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin we will compare the iteration histories for four cases. We will consider analytic and forward difference Jacobians with the storage and factorization of the Jacobian done in double and single precision. __Theorem 1.2__ says the results should be almost indistinguishable. We will begin with Newton's method.\n",
    "\n",
    "All we need to do to store and factor Jacobians is to allocate the storage in single precision. That allocation is the line ```FPS32=ones(Float32,n,n);```. Note that we must reset ```FS``` and ```FPS``` after each call to __nsol.jl__ because the solver uses the storage for residuals and Jacobians for the entire iteration. We do this with __broadcast__ after the initial allocation ```.=1.0``` instead of ```=ones(n,n)``` to avoid reallocation of the Jacobian.\n",
    "\n",
    "We will print all the residual histories in an array. The history vectors are the same length and are very hard to tell apart until the residual norm is one iteration from stagnation. This is just what the theory predicts. The theory \n",
    "(see <cite data-cite=\"ctk:sirev20\"><a href=\"siamfa.html#ctk:sirev20\">(Kel20a)</cite> ) also predicts that there will be little difference between double precision linear algebra and single precision. We observe this as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4×4 Array{Float64,2}:\n",
       " 1.00000e+00  1.00000e+00  1.00000e+00  1.00000e+00\n",
       " 5.14148e-03  5.14150e-03  5.14148e-03  5.14127e-03\n",
       " 1.00479e-07  1.00713e-07  1.00479e-07  9.61079e-08\n",
       " 1.97100e-15  3.73299e-14  1.97100e-15  1.57530e-13"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n=1024; FS=ones(n,); FPS=ones(n,n); FPS32=ones(Float32,n,n); x0=ones(n,); c=.5; hdata = heqinit(x0, c);\n",
    "nouta64=htest(x0, FS, FPS, hdata;analytic=true, hsham=1);\n",
    "nouta32=htest(x0, FS, FPS32, hdata; analytic=true, hsham=1);\n",
    "noutfd64=htest(x0, FS, FPS, hdata; analytic=true, hsham=1);\n",
    "noutfd32=htest(x0, FS, FPS32,hdata; analytic=false, hsham=1);\n",
    "[nouta64.history nouta32.history noutfd64.history noutfd32.history]./nouta64.history[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now do the same thing with the default setting of ```sham=5```. The theory correctly predicts that we will see slower convergence. We will be using BenchmarkTools to compare the costs later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6×4 Array{Float64,2}:\n",
       " 1.00000e+00  1.00000e+00  1.00000e+00  1.00000e+00\n",
       " 5.14148e-03  5.14150e-03  5.14126e-03  5.14127e-03\n",
       " 4.44954e-05  4.44961e-05  4.44920e-05  4.44925e-05\n",
       " 3.81019e-07  3.81025e-07  3.80979e-07  3.80983e-07\n",
       " 3.26071e-09  3.26076e-09  3.26027e-09  3.26031e-09\n",
       " 2.79033e-11  2.79028e-11  2.78975e-11  2.78996e-11"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nouta64=htest(x0, FS, FPS, hdata; analytic=true);\n",
    "noutfd64=htest(x0, FS, FPS, hdata; analytic=false);\n",
    "nouta32=htest(x0, FS, FPS32, hdata; analytic=true);\n",
    "noutfd32=htest(x0, FS, FPS32, hdata; analytic=false);\n",
    "[nouta64.history nouta32.history noutfd64.history noutfd32.history]./nouta64.history[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is interesting. We need more iterations, but we evaluate the Jacobian only once for Shamanskii. We can see this by looking at the ```stats``` field of the output tuple. They are all the same, so we will use ```nouta64```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(ifun = [1, 1, 1, 1, 1, 1], ijac = [0, 1, 0, 0, 0, 0], iarm = [0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nouta64.stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The interpretation is that we do a function evaluation at all iterations and a single Jacobian evaluation to compute the $\\vx_1$. We do no Jacobian work after that. The default in __nsol.jl__ is to reevaluate the Jacobian if the reduction in the residual norm larger than ```resdec = .1```. You can change ```resdec``` in the keyword arguments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the [BenchmarkTools.jl](https://github.com/JuliaCI/BenchmarkTools.jl) package to look at performance. \n",
    "The ```@btime``` command will show compute time and memory allocations for an average of several runs. The averaging will mitigate the effects of the compile time for the first run. \n",
    "\n",
    "To begin, we will compare the four versions of Newton's method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analytic, double\n",
      "  26.700 ms (6198 allocations: 468.92 KiB)\n",
      "finite difference, double\n",
      "  93.028 ms (9264 allocations: 924.27 KiB)\n",
      "analytic, single\n",
      "  33.038 ms (6201 allocations: 457.30 KiB)\n",
      "finite difference, single\n",
      "  99.802 ms (9267 allocations: 912.64 KiB)\n"
     ]
    }
   ],
   "source": [
    "println(\"analytic, double\"); @btime htest($x0, $FS, $FPS, hdata; analytic=true, hsham=1);\n",
    "println(\"finite difference, double\"); @btime htest($x0, $FS, $FPS, hdata; analytic=false, hsham=1);\n",
    "println(\"analytic, single\"); @btime htest($x0, $FS, $FPS32, hdata; analytic=true, hsham=1);\n",
    "println(\"finite difference, single\"); @btime htest($x0, $FS, $FPS32, hdata; analytic=false, hsham=1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even for a problem of only modest size, the differences between the analytic Jacobian and the forward difference are significant. The differences between single and double precision linear algebra are, at least for the analytic Jacobian, roughly the factor of two we'd expect if the matrix factorization dominated the computation. For the forward difference Jacobian, we see that the cost of the Jacobian evaluation dominates everything else.\n",
    "\n",
    "Next, we look at the default ```sham=5``` from __nsol.jl__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analytic, double\n",
      "  9.675 ms (2098 allocations: 244.58 KiB)\n",
      "finite difference, double\n",
      "  32.286 ms (3120 allocations: 396.36 KiB)\n",
      "analytic, single\n",
      "  11.183 ms (2103 allocations: 225.20 KiB)\n",
      "finite difference, single\n",
      "  34.991 ms (3125 allocations: 376.98 KiB)\n"
     ]
    }
   ],
   "source": [
    "println(\"analytic, double\"); @btime htest($x0, $FS, $FPS, hdata; analytic=true);\n",
    "println(\"finite difference, double\"); @btime htest($x0, $FS, $FPS, hdata; analytic=false);\n",
    "println(\"analytic, single\"); @btime htest($x0, $FS, $FPS32, hdata; analytic=true);\n",
    "println(\"finite difference, single\"); @btime htest($x0, $FS, $FPS32, hdata; analytic=false);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The moral here is pretty clear. We see that compute time is cut by a factor of at least two in all cases. The result for an analytic Jacobian with linear algebra in single precision and ```sham=5``` is 19 times faster than our slowest computation (Newton + finite difference Jacobian + double precision linear algebra). So, do less linear algebra and do it in single precision.\n",
    "\n",
    "Finally, we will increase the dimension. As we do that the computation becomes more burdensome, so we will only do two cases, both with an analytic Jacobian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analytic, double, Newton\n",
      "  510.149 ms (24652 allocations: 1.82 MiB)\n",
      "analytic, single, sham=5\n",
      "  98.503 ms (8255 allocations: 884.83 KiB)\n"
     ]
    }
   ],
   "source": [
    "n=4096; FS=ones(n,); FPS=ones(n,n); FPS32=ones(Float32,n,n); x0=ones(n,); c=.5; hdata = heqinit(x0, c);\n",
    "println(\"analytic, double, Newton\"); @btime htest($x0, $FS, $FPS, hdata; analytic=true, hsham=1);\n",
    "println(\"analytic, single, sham=5\"); @btime htest($x0, $FS, $FPS32, hdata; analytic=true);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For sufficiently large dimension, the linear algebra cost will dominate the computation. Time should increase by roughly a factor of 8 as the dimension doubles because our LU factorization takes $O(N^3)$ operations. We are not in that regime yet, but the single precision cost is now less than 6 times less than the double precision one. One of the projects at the end of this chapter challenges you to increase the dimension and compare the timings as you do that. Remember that we allocated storage for the Jacobian when we defined ```FPS```, so @btime is not measuring the allocation for the Jacobian."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### More on the Two-Point BVP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the Jacobian is sparse and you use the solvers from __SuiteSparse__ then you cannot use single precision. Even if the structure of the Jacobian allows you to use the LAPACK solvers or a special-purpose package, there is less benefit in using single precision for linear algebra than in the dense case. We will explore that for the boundary value problem, where we can use [BandedMatrices.jl](https://github.com/JuliaMatrices/BandedMatrices.jl) and __qr!__ for the linear solver. __qr!__ supports single precision linear algebra, so we will use that. __lu!__ does not, so the support is not consistent.\n",
    "\n",
    "The we will set up the problem for a very fine mesh, far finer than one needs to get a useful result, to illustrate the performance. The band solver takes $O(N)$ work, so we would expect the solve to be fast. To set things up we mimic __bvp_solve.jl__ ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set it up\n",
    "    n=10^5;\n",
    "    bdata = bvpinit(n, Float64);\n",
    "#\n",
    "    U0 = zeros(2n);\n",
    "    FV = zeros(2n);\n",
    "# Banded matrix with the correct number of bands\n",
    "# Make double and single precision copies\n",
    "    FPV = BandedMatrix{Float64}(Zeros(2n, 2n), (2, 4));\n",
    "    FPV32 = BandedMatrix{Float32}(Zeros(2n, 2n), (2, 4));\n",
    "#\n",
    "# Build the initial iterate\n",
    "#\n",
    "    tv = bdata.tv;\n",
    "    sv = -.1 * tv .* tv;\n",
    "    view(U0,1:2:2n-1) .= exp.(-.1 .* tv .* tv);\n",
    "    view(U0,2:2:2n).= -.2 .* view(U0,1:2:2n-1) .* tv;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will begin by comparing the default ```sham=5``` with Newton's method. As we did with the H-equation, we will write a test function that uses the data we allocated above. We only use an analytic Jacobian for this and other examples with sparse Jacobians. The reader might want to look at the project in this chapter on [sparse differencing](#Sparse Differencing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bvptest (generic function with 1 method)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function bvptest(U0, FS, FPS, bdata; bsham=5, bfact=qr!)\n",
    "    FS .*= 0.0\n",
    "    FPS .*= 0.\n",
    "        bvpout = nsol(Fbvp!, U0, FS, FPS, Jbvp!; atol=1.e-8, rtol = 1.e-8, sham=bsham,\n",
    "             pdata = bdata, jfact=bfact)\n",
    "    return bvpout\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will look at the convergence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "Figure(PyObject <Figure size 640x480 with 1 Axes>)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bvpoutn64=bvptest(U0, FV, FPV, bdata;  bsham=1);\n",
    "bvpouts64=bvptest(U0, FV, FPV, bdata; bsham=5);\n",
    "bvpoutn32=bvptest(U0, FV, FPV32, bdata;  bsham=1);\n",
    "bvpouts32=bvptest(U0, FV, FPV32, bdata; bsham=5);\n",
    "newtonhist=bvpoutn64.history./bvpoutn64.history[1];\n",
    "shamhist=bvpouts64.history./bvpoutn64.history[1];\n",
    "newtonhist32=bvpoutn32.history./bvpoutn64.history[1];\n",
    "shamhist32=bvpouts32.history./bvpoutn64.history[1];\n",
    "nn=length(newtonhist);\n",
    "ns=length(shamhist);\n",
    "nn32=length(newtonhist32);\n",
    "ns32=length(shamhist32);\n",
    "semilogy(0:nn-1,newtonhist,\"k-\",0:ns-1,shamhist,\"k--\",0:nn32-1,newtonhist32,\"k.\",0:ns32-1,shamhist32,\"k-.\")\n",
    "legend([\"newton\",\"sham=5\",\"newton, single\",\"sham=5, single\"]);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1×10 Adjoint{Int64,Array{Int64,1}}:\n",
       " 0  0  0  2  2  1  0  0  0  0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bvpoutn64.stats.iarm'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Updating the Jacobian less frequently has a smaller impact than in the case of the H-equation. You should expect this because the linear solver costs $O(N)$ work rather than $O(N^3)$ and the Jacobian is updated for the early iterations because the residual is not decreasing fast enough.\n",
    "\n",
    "The other interesting feature of this computation is that the single precision results differ in a meaningful way from the double precision results. The reason for this is that the Jacobian is ill-conditioned enough to affect the quality of the single precision solver. The condition number of the Jacobian is $O(1/N)$, so for $N=10^5$ one should expect poor results from the linear solve in single precision, and that's what you get.\n",
    "\n",
    "More over, the benchmark results say that there is no benefit from doing the linear algebra in single precision. This is also no surprise since the factorization is $O(N)$ work.\n",
    "\n",
    "The reader should try this problem with $N=10^6$ and watch the line search fail for the single precision linear algebra computations. That is also no surprise with an inaccurate Newton direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analytic, double, Newton\n",
      "  150.244 ms (151 allocations: 47.31 MiB)\n",
      "analytic, double, sham=5\n",
      "  125.686 ms (137 allocations: 44.26 MiB)\n",
      "analytic, single, Newton\n",
      "  142.089 ms (169 allocations: 33.58 MiB)\n",
      "analytic, single, sham=5\n",
      "  117.675 ms (163 allocations: 34.34 MiB)\n"
     ]
    }
   ],
   "source": [
    "println(\"analytic, double, Newton\"); @btime bvptest($U0, $FV, $FPV, $bdata;  bsham=1);\n",
    "println(\"analytic, double, sham=5\"); @btime bvptest($U0, $FV, $FPV, $bdata;  bsham=5);\n",
    "println(\"analytic, single, Newton\"); @btime bvptest($U0, $FV, $FPV32, $bdata;  bsham=1);\n",
    "println(\"analytic, single, sham=5\"); @btime bvptest($U0, $FV, $FPV32, $bdata;  bsham=5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ptcsol.jl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ptcsol.jl__ is our $\\ptc$ solve. As usual, we begin with the docstrings ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search: \u001b[0m\u001b[1mp\u001b[22m\u001b[0m\u001b[1mt\u001b[22m\u001b[0m\u001b[1mc\u001b[22m\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1ml\u001b[22m \u001b[0m\u001b[1mp\u001b[22m\u001b[0m\u001b[1mt\u001b[22m\u001b[0m\u001b[1mc\u001b[22m\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1ml\u001b[22msc \u001b[0m\u001b[1mP\u001b[22mar\u001b[0m\u001b[1mt\u001b[22mialQui\u001b[0m\u001b[1mc\u001b[22mk\u001b[0m\u001b[1mS\u001b[22m\u001b[0m\u001b[1mo\u001b[22mrt\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "ptcsol(F!, x0, FS, FPS, J! = diffjac!; rtol=1.e-6, atol=1.e-12,                maxit=20, dt0=1.e-6, dx=1.e-7, pdata = nothing, jfact = klfact,                printerr = true, keepsolhist = false)\n",
       "\n",
       "C. T. Kelley, 2020\n",
       "\n",
       "Julia versions of the nonlinear solvers from my SIAM books.  Herewith: some new stuff ==> ptcsol\n",
       "\n",
       "You must allocate storage for the function and Jacobian in advance –> in the calling program <– ie. in FS and FPS\n",
       "\n",
       "Inputs:\n",
       "\n",
       "\\begin{itemize}\n",
       "\\item F!: function evaluation, the ! indicates that F! overwrites FS, your   preallocated storage for the function.\n",
       "\n",
       "So, FV=F!(FV,x) or FV=F!(FV,x,pdata) returns FV=F(x)\n",
       "\n",
       "\n",
       "\\item x0: initial iterate\n",
       "\n",
       "\\end{itemize}\n",
       "\\begin{itemize}\n",
       "\\item FS: Preallocated storage for function. It is an N x 1 column vector.\n",
       "\n",
       "\\end{itemize}\n",
       "You may dimension it as (n,) or (n,1). (n,) is best, but the solvers can deal with it either way.\n",
       "\n",
       "\\begin{itemize}\n",
       "\\item FPS: preallocated storage for Jacobian. It is an N x N matrix\n",
       "\n",
       "\\end{itemize}\n",
       "\\begin{itemize}\n",
       "\\item J!: Jacobian evaluation, the ! indicates that J! overwrites FPS, your   preallocated storage for the Jacobian. If you leave this out the   default is a finite difference Jacobian.\n",
       "\n",
       "So, FP=J!(FP,FV,x) or FP=J!(FP,FV,x,pdata) returns FP=F'(x);   (FP,FV, x) must be the argument list, even if FP does not need FV.   One reason for this is that the finite-difference Jacobian   does and that is the default in the solver.\n",
       "\n",
       "\n",
       "\\item Precision: Lemme tell ya 'bout precision. I designed this code for    full precision   functions and linear algebra in any precision you want. You can declare   FPS as Float64, Float32, or Float16 and ptcsol will do the right thing if   YOU do not destroy the declaration in your J! function. I'm amazed   that this works so easily. If the Jacobian is reasonably well   conditioned, you can cut the cost of Jacobian factorization and   storage in half with no loss. For large dense Jacobians and inexpensive   functions, this is a good deal.\n",
       "\n",
       "BUT ... There is very limited support for direct sparse solvers in   anything other than Float64. I recommend that you only use Float64   with direct sparse solvers unless you really know what you're doing. I   have a couple examples in the notebook, but watch out.\n",
       "\n",
       "\\end{itemize}\n",
       "\\rule{\\textwidth}{1pt}\n",
       "Keyword Arguments (kwargs):\n",
       "\n",
       "rtol and atol: relative and absolute error tolerances\n",
       "\n",
       "dt0: initial time step. The default value of 1.e-3 is a bit conservative and is one option you really should play with. Look at the example where I set it to 1.0!\n",
       "\n",
       "maxit: limit on nonlinear iterations, default=100. \n",
       "\n",
       "This is coupled to dt0. If your choice of dt0 is too small (conservative) then you'll need many iterations to converge and will need a larger value of maxit\n",
       "\n",
       "For PTC you'll need more iterations than for a straight-up nonlinear solve. This is part of the price for finding the  stable solution.\n",
       "\n",
       "dx: default = 1.e-7\n",
       "\n",
       "difference increment in finite-difference derivatives       h=dx*norm(x)+1.e-6\n",
       "\n",
       "pdata:\n",
       "\n",
       "precomputed data for the function/Jacobian.  Things will go better if you use this rather than hide the data  in global variables within the module for your function/Jacobian\n",
       "\n",
       "jfact: default = klfact (tries to figure out best choice) \n",
       "\n",
       "If your Jacobian has any special structure, please set jfact to the correct choice for a factorization.\n",
       "\n",
       "I use jfact when I call PTCUpdate to evaluate the Jacobian (using your J!) and factor it. The default is to use klfact (an internal function) to do something reasonable. For general matrices, klfact picks lu! to compute an LU factorization and share storage with the Jacobian.  You may change LU to something else by, for example, setting jfact = cholseky! if your Jacobian is spd.\n",
       "\n",
       "klfact knows about banded matrices and picks qr. You should, however RTFM, allocate the extra two upper bands, and use jfact=qr! to override klfact.\n",
       "\n",
       "If you give me something that klfact does not know how to dispatch on, then nothing happens. I just return the original Jacobian matrix and  ptcsol will use backslash to compute the Newton step.\n",
       "\n",
       "I know that this is probably not optimal in your situation, so it is  good to pick something else, like jfact = lu.\n",
       "\n",
       "printerr: default = true\n",
       "\n",
       "I print a helpful message when the solver fails. To suppress that message set printerr to false.\n",
       "\n",
       "keepsolhist: default = false\n",
       "\n",
       "Set this to true to get the history of the iteration in the output tuple. This is on by default for scalar equations and off for systems. Only turn it on if you have use for the data, which can get REALLY LARGE.\n",
       "\n",
       "Output:\n",
       "\n",
       "A named tuple (solution, functionval, history, stats, idid,                errcode, solhist) where\n",
       "\n",
       "solution = converged result functionval = F(solution) history = the vector of residual norms (||F(x)||) for the iteration stats = named tuple of the history of (ifun, ijac, iarm), the number of functions/derivatives/steplength reductions at each iteration.\n",
       "\n",
       "I do not count the function values for a finite-difference derivative because they count toward a Jacobian evaluation. \n",
       "\n",
       "idid=true if the iteration succeeded and false if not.\n",
       "\n",
       "errcode = 0 if if the iteration succeeded         = -1 if the initial iterate satisfies the termination criteria         = 10 if no convergence after maxit iterations         = 1  if the line search failed\n",
       "\n",
       "solhist:\n",
       "\n",
       "This is the entire history of the iteration if you've set keepsolhist=true\n",
       "\n",
       "solhist is an N x K array where N is the length of x and K is the number of iteration + 1. So, for scalar equations, it's a row vector.\n",
       "\n"
      ],
      "text/markdown": [
       "ptcsol(F!, x0, FS, FPS, J! = diffjac!; rtol=1.e-6, atol=1.e-12,                maxit=20, dt0=1.e-6, dx=1.e-7, pdata = nothing, jfact = klfact,                printerr = true, keepsolhist = false)\n",
       "\n",
       "C. T. Kelley, 2020\n",
       "\n",
       "Julia versions of the nonlinear solvers from my SIAM books.  Herewith: some new stuff ==> ptcsol\n",
       "\n",
       "You must allocate storage for the function and Jacobian in advance –> in the calling program <– ie. in FS and FPS\n",
       "\n",
       "Inputs:\n",
       "\n",
       "  * F!: function evaluation, the ! indicates that F! overwrites FS, your   preallocated storage for the function.\n",
       "\n",
       "    So, FV=F!(FV,x) or FV=F!(FV,x,pdata) returns FV=F(x)\n",
       "  * x0: initial iterate\n",
       "\n",
       "  * FS: Preallocated storage for function. It is an N x 1 column vector.\n",
       "\n",
       "You may dimension it as (n,) or (n,1). (n,) is best, but the solvers can deal with it either way.\n",
       "\n",
       "  * FPS: preallocated storage for Jacobian. It is an N x N matrix\n",
       "\n",
       "  * J!: Jacobian evaluation, the ! indicates that J! overwrites FPS, your   preallocated storage for the Jacobian. If you leave this out the   default is a finite difference Jacobian.\n",
       "\n",
       "    So, FP=J!(FP,FV,x) or FP=J!(FP,FV,x,pdata) returns FP=F'(x);   (FP,FV, x) must be the argument list, even if FP does not need FV.   One reason for this is that the finite-difference Jacobian   does and that is the default in the solver.\n",
       "  * Precision: Lemme tell ya 'bout precision. I designed this code for    full precision   functions and linear algebra in any precision you want. You can declare   FPS as Float64, Float32, or Float16 and ptcsol will do the right thing if   YOU do not destroy the declaration in your J! function. I'm amazed   that this works so easily. If the Jacobian is reasonably well   conditioned, you can cut the cost of Jacobian factorization and   storage in half with no loss. For large dense Jacobians and inexpensive   functions, this is a good deal.\n",
       "\n",
       "    BUT ... There is very limited support for direct sparse solvers in   anything other than Float64. I recommend that you only use Float64   with direct sparse solvers unless you really know what you're doing. I   have a couple examples in the notebook, but watch out.\n",
       "\n",
       "---\n",
       "\n",
       "Keyword Arguments (kwargs):\n",
       "\n",
       "rtol and atol: relative and absolute error tolerances\n",
       "\n",
       "dt0: initial time step. The default value of 1.e-3 is a bit conservative and is one option you really should play with. Look at the example where I set it to 1.0!\n",
       "\n",
       "maxit: limit on nonlinear iterations, default=100. \n",
       "\n",
       "This is coupled to dt0. If your choice of dt0 is too small (conservative) then you'll need many iterations to converge and will need a larger value of maxit\n",
       "\n",
       "For PTC you'll need more iterations than for a straight-up nonlinear solve. This is part of the price for finding the  stable solution.\n",
       "\n",
       "dx: default = 1.e-7\n",
       "\n",
       "difference increment in finite-difference derivatives       h=dx*norm(x)+1.e-6\n",
       "\n",
       "pdata:\n",
       "\n",
       "precomputed data for the function/Jacobian.  Things will go better if you use this rather than hide the data  in global variables within the module for your function/Jacobian\n",
       "\n",
       "jfact: default = klfact (tries to figure out best choice) \n",
       "\n",
       "If your Jacobian has any special structure, please set jfact to the correct choice for a factorization.\n",
       "\n",
       "I use jfact when I call PTCUpdate to evaluate the Jacobian (using your J!) and factor it. The default is to use klfact (an internal function) to do something reasonable. For general matrices, klfact picks lu! to compute an LU factorization and share storage with the Jacobian.  You may change LU to something else by, for example, setting jfact = cholseky! if your Jacobian is spd.\n",
       "\n",
       "klfact knows about banded matrices and picks qr. You should, however RTFM, allocate the extra two upper bands, and use jfact=qr! to override klfact.\n",
       "\n",
       "If you give me something that klfact does not know how to dispatch on, then nothing happens. I just return the original Jacobian matrix and  ptcsol will use backslash to compute the Newton step.\n",
       "\n",
       "I know that this is probably not optimal in your situation, so it is  good to pick something else, like jfact = lu.\n",
       "\n",
       "printerr: default = true\n",
       "\n",
       "I print a helpful message when the solver fails. To suppress that message set printerr to false.\n",
       "\n",
       "keepsolhist: default = false\n",
       "\n",
       "Set this to true to get the history of the iteration in the output tuple. This is on by default for scalar equations and off for systems. Only turn it on if you have use for the data, which can get REALLY LARGE.\n",
       "\n",
       "Output:\n",
       "\n",
       "A named tuple (solution, functionval, history, stats, idid,                errcode, solhist) where\n",
       "\n",
       "solution = converged result functionval = F(solution) history = the vector of residual norms (||F(x)||) for the iteration stats = named tuple of the history of (ifun, ijac, iarm), the number of functions/derivatives/steplength reductions at each iteration.\n",
       "\n",
       "I do not count the function values for a finite-difference derivative because they count toward a Jacobian evaluation. \n",
       "\n",
       "idid=true if the iteration succeeded and false if not.\n",
       "\n",
       "errcode = 0 if if the iteration succeeded         = -1 if the initial iterate satisfies the termination criteria         = 10 if no convergence after maxit iterations         = 1  if the line search failed\n",
       "\n",
       "solhist:\n",
       "\n",
       "This is the entire history of the iteration if you've set keepsolhist=true\n",
       "\n",
       "solhist is an N x K array where N is the length of x and K is the number of iteration + 1. So, for scalar equations, it's a row vector.\n"
      ],
      "text/plain": [
       "  ptcsol(F!, x0, FS, FPS, J! = diffjac!; rtol=1.e-6, atol=1.e-12, maxit=20,\n",
       "  dt0=1.e-6, dx=1.e-7, pdata = nothing, jfact = klfact, printerr = true,\n",
       "  keepsolhist = false)\n",
       "\n",
       "  C. T. Kelley, 2020\n",
       "\n",
       "  Julia versions of the nonlinear solvers from my SIAM books. Herewith: some\n",
       "  new stuff ==> ptcsol\n",
       "\n",
       "  You must allocate storage for the function and Jacobian in advance –> in the\n",
       "  calling program <– ie. in FS and FPS\n",
       "\n",
       "  Inputs:\n",
       "\n",
       "    •    F!: function evaluation, the ! indicates that F! overwrites FS,\n",
       "        your preallocated storage for the function.\n",
       "      \n",
       "        So, FV=F!(FV,x) or FV=F!(FV,x,pdata) returns FV=F(x)\n",
       "\n",
       "    •    x0: initial iterate\n",
       "\n",
       "    •    FS: Preallocated storage for function. It is an N x 1 column\n",
       "        vector.\n",
       "\n",
       "  You may dimension it as (n,) or (n,1). (n,) is best, but the solvers can\n",
       "  deal with it either way.\n",
       "\n",
       "    •    FPS: preallocated storage for Jacobian. It is an N x N matrix\n",
       "\n",
       "    •    J!: Jacobian evaluation, the ! indicates that J! overwrites FPS,\n",
       "        your preallocated storage for the Jacobian. If you leave this out\n",
       "        the default is a finite difference Jacobian.\n",
       "      \n",
       "        So, FP=J!(FP,FV,x) or FP=J!(FP,FV,x,pdata) returns FP=F'(x);\n",
       "        (FP,FV, x) must be the argument list, even if FP does not need FV.\n",
       "        One reason for this is that the finite-difference Jacobian does\n",
       "        and that is the default in the solver.\n",
       "\n",
       "    •    Precision: Lemme tell ya 'bout precision. I designed this code for\n",
       "        full precision functions and linear algebra in any precision you\n",
       "        want. You can declare FPS as Float64, Float32, or Float16 and\n",
       "        ptcsol will do the right thing if YOU do not destroy the\n",
       "        declaration in your J! function. I'm amazed that this works so\n",
       "        easily. If the Jacobian is reasonably well conditioned, you can\n",
       "        cut the cost of Jacobian factorization and storage in half with no\n",
       "        loss. For large dense Jacobians and inexpensive functions, this is\n",
       "        a good deal.\n",
       "      \n",
       "        BUT ... There is very limited support for direct sparse solvers in\n",
       "        anything other than Float64. I recommend that you only use Float64\n",
       "        with direct sparse solvers unless you really know what you're\n",
       "        doing. I have a couple examples in the notebook, but watch out.\n",
       "\n",
       "  ────────────────────────────────────────────────────────────────────────────\n",
       "\n",
       "  Keyword Arguments (kwargs):\n",
       "\n",
       "  rtol and atol: relative and absolute error tolerances\n",
       "\n",
       "  dt0: initial time step. The default value of 1.e-3 is a bit conservative and\n",
       "  is one option you really should play with. Look at the example where I set\n",
       "  it to 1.0!\n",
       "\n",
       "  maxit: limit on nonlinear iterations, default=100. \n",
       "\n",
       "  This is coupled to dt0. If your choice of dt0 is too small (conservative)\n",
       "  then you'll need many iterations to converge and will need a larger value of\n",
       "  maxit\n",
       "\n",
       "  For PTC you'll need more iterations than for a straight-up nonlinear solve.\n",
       "  This is part of the price for finding the stable solution.\n",
       "\n",
       "  dx: default = 1.e-7\n",
       "\n",
       "  difference increment in finite-difference derivatives h=dx*norm(x)+1.e-6\n",
       "\n",
       "  pdata:\n",
       "\n",
       "  precomputed data for the function/Jacobian. Things will go better if you use\n",
       "  this rather than hide the data in global variables within the module for\n",
       "  your function/Jacobian\n",
       "\n",
       "  jfact: default = klfact (tries to figure out best choice) \n",
       "\n",
       "  If your Jacobian has any special structure, please set jfact to the correct\n",
       "  choice for a factorization.\n",
       "\n",
       "  I use jfact when I call PTCUpdate to evaluate the Jacobian (using your J!)\n",
       "  and factor it. The default is to use klfact (an internal function) to do\n",
       "  something reasonable. For general matrices, klfact picks lu! to compute an\n",
       "  LU factorization and share storage with the Jacobian. You may change LU to\n",
       "  something else by, for example, setting jfact = cholseky! if your Jacobian\n",
       "  is spd.\n",
       "\n",
       "  klfact knows about banded matrices and picks qr. You should, however RTFM,\n",
       "  allocate the extra two upper bands, and use jfact=qr! to override klfact.\n",
       "\n",
       "  If you give me something that klfact does not know how to dispatch on, then\n",
       "  nothing happens. I just return the original Jacobian matrix and ptcsol will\n",
       "  use backslash to compute the Newton step.\n",
       "\n",
       "  I know that this is probably not optimal in your situation, so it is good to\n",
       "  pick something else, like jfact = lu.\n",
       "\n",
       "  printerr: default = true\n",
       "\n",
       "  I print a helpful message when the solver fails. To suppress that message\n",
       "  set printerr to false.\n",
       "\n",
       "  keepsolhist: default = false\n",
       "\n",
       "  Set this to true to get the history of the iteration in the output tuple.\n",
       "  This is on by default for scalar equations and off for systems. Only turn it\n",
       "  on if you have use for the data, which can get REALLY LARGE.\n",
       "\n",
       "  Output:\n",
       "\n",
       "  A named tuple (solution, functionval, history, stats, idid, errcode,\n",
       "  solhist) where\n",
       "\n",
       "  solution = converged result functionval = F(solution) history = the vector\n",
       "  of residual norms (||F(x)||) for the iteration stats = named tuple of the\n",
       "  history of (ifun, ijac, iarm), the number of\n",
       "  functions/derivatives/steplength reductions at each iteration.\n",
       "\n",
       "  I do not count the function values for a finite-difference derivative\n",
       "  because they count toward a Jacobian evaluation. \n",
       "\n",
       "  idid=true if the iteration succeeded and false if not.\n",
       "\n",
       "  errcode = 0 if if the iteration succeeded = -1 if the initial iterate\n",
       "  satisfies the termination criteria = 10 if no convergence after maxit\n",
       "  iterations = 1 if the line search failed\n",
       "\n",
       "  solhist:\n",
       "\n",
       "  This is the entire history of the iteration if you've set keepsolhist=true\n",
       "\n",
       "  solhist is an N x K array where N is the length of x and K is the number of\n",
       "  iteration + 1. So, for scalar equations, it's a row vector."
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?ptcsol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### More on the Buckling Beam\n",
    "\n",
    "The residual function for the beam uses precomputed data to store the discrete second derivative and the bifurcation parameter $\\lambda$. These data are smaller and far less complex that the data for the H-equation and it's worthwhile too look at the functions. These are part of the __TestProblems__ submodule for __SIAMFANLEquations.jl__. We will not conver all the details here, but will give you enough to see how precomputed data is useful.\n",
    "\n",
    "First the function itself\n",
    "```Julia\n",
    "\"\"\"\n",
    "FBeam!(FV, U, bdata)\n",
    "Function evaluation for PTC example.\n",
    "F(u) = -u'' - lambda sin(u)\n",
    "\"\"\"\n",
    "function FBeam!(FV, U, bdata)\n",
    "    D2 = bdata.D2\n",
    "    lambda = bdata.lambda\n",
    "    su = lambda * sin.(U)\n",
    "    FV .= (D2 * U - su)\n",
    "end\n",
    "```\n",
    "Note how the function ```FBeam``` harvests $\\lambda$ and the discrete second derivative ```D2``` from the precomputed data ```bdata```. In this case ```bdata``` is a __named tuple__ which we create with the\n",
    "__beaminit__ function.\n",
    "\n",
    "```Julia\n",
    "\"\"\"\n",
    "beaminit(n,dt,lambda=20.0)\n",
    "\n",
    "Set up the beam problem with n interior grid points.\n",
    "\"\"\"\n",
    "\n",
    "function beaminit(n, dt, lambda = 20.0)\n",
    "    D2 = Lap1d(n)\n",
    "    dx = 1.0 / (n + 1)\n",
    "    x = collect(dx:dx:1.0-dx)\n",
    "    UN = zeros(size(x))\n",
    "    bdata = (D2 = D2, x = x, dx = dx, dt = dt, lambda = lambda, UN = UN)\n",
    "end\n",
    "```\n",
    "The bdata structure has more that just ```D2``` and $\\lambda$. I data for the solver and the construction of the Jacobian.\n",
    "\n",
    "Finally, the function __Lap1D__ computes ```D2``` as a tridiagonal matrix. This is part of the LinearAlgebra package that is part of Julia.Base.\n",
    "\n",
    "```Julia\n",
    "\"\"\"\n",
    "Lap1d(n)\n",
    "\n",
    "returns -d^2/dx^2 on [0,1] zero BC\n",
    "\"\"\"\n",
    "function Lap1d(n)\n",
    "    dx = 1 / (n + 1)\n",
    "    d = 2.0 * ones(n)\n",
    "    sup = -ones(n - 1)\n",
    "    slo = -ones(n - 1)\n",
    "    D2 = Tridiagonal(slo, d, sup)\n",
    "    D2 = D2 / (dx * dx)\n",
    "    return D2\n",
    "end\n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2.9 Projects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More on the H-equation\n",
    "\n",
    "- Run the H-equation tests with $c=.99$ and larger dimensions ($2^k$ for $k=12, 13, 14$). Are the values of ```sham``` and ```resdec``` optimal for this case? How, for example, is ```resdec = Inf``` (your author's favorite value). To see why I like ```sham=Inf``` have a look at\n",
    "<cite data-cite=\"brent\"><a href=\"siamfa.html#brent\">(Bre73)</cite>.\n",
    "\n",
    "- What happens with $c=1$? Is the chord method a good idea? \n",
    "Don't look at\n",
    "<cite data-cite=\"ctk:sirev20\"><a href=\"siamfa.html#ctk:sirev20\">(Kel20a)</cite>,\n",
    "<cite data-cite=\"ctk:n1\"><a href=\"siamfa.html#ctk:n1\">(DK80)</cite>, or\n",
    "<cite data-cite=\"ctk:chord\"><a href=\"siamfa.html#ctk:chord\">(DK83)</cite> before trying to figure things out on your own.\n",
    "\n",
    "- Increase the dimension as much as you can and compare single and double precision linear algebra.\n",
    "    \n",
    "- Duplicate the table on page 125 of <cite data-cite=\"chand\"><a href=\"siamfa.html#chand\">(Cha60)</cite>.    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic Differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(not trivial!)__ Try using __automatic differention__ to compute the Jacobian for the examples in this chapter and use the results to write a Jacobian evaluation function to pass to __nsol.jl__.\n",
    "You will first have to decide how you want to do that. Two of mature packages are [ForwardDiff.jl](#https://github.com/JuliaDiff/ForwardDiff.jl)\n",
    "<cite data-cite=\"forwarddiff\"><a href=\"siamfa.html#forwarddiff\">(RPL16)</cite>\n",
    "and [Zygote.jl](#https://github.com/FluxML/Zygote.jl) \n",
    "<cite data-cite=\"zygote\"><a href=\"siamfa.html#zygote\">(CoRR)</cite>.\n",
    "Books like\n",
    "<cite data-cite=\"grautodiff\"><a href=\"siamfa.html#grautodiff\">(Gri00)</cite> explain the\n",
    "algorithmic differences between these packages.\n",
    "    \n",
    "How does the performance compare to an analytic or forward difference Jacobian? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nested Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solving a differential or integral equation\n",
    "by __nested iteration__ or __grid sequencing__ means resolving\n",
    "the rough features of the solution of a differential or\n",
    "integral equation on a coarse mesh, interpolating the\n",
    "solution to a finer mesh, resolving on the finer mesh, and\n",
    "then repeating the process until you have a solution on a target\n",
    "mesh.\n",
    "\n",
    "Apply this idea to some of the examples in the text, using piecewise\n",
    "linear interpolation to move from coarse to fine meshes. If the\n",
    "discretization is second-order accurate and you halve the mesh\n",
    "width at each level, how should you terminate the solver at\n",
    "each level? What kind of iteration statistics would tell you that\n",
    "you've done a satisfactory job?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse Differencing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write Jacobian evaluation functions using sparse differencing for the boundary value problem and the buckling beam examples. You can use the literature, for example the methods from \n",
    "<cite data-cite=\"curtispr\"><a href=\"siamfa.html#curtisor\">(CPR74)</cite> or\n",
    "<cite data-cite=\"colmore\"><a href=\"siamfa.html#colmore\">(CM83)</cite>.  <cite data-cite=\"ctk:newton\"><a href=\"siamfa.html#ctk:newton\">(Kel03)</cite> has a Matlab code for banded matrices that, while convertible to Julia, needs some work to explictly store the Jacobian as a BandedMatrix so\n",
    "[BandedMatrices.jl](https://github.com/JuliaMatrices/BandedMatrices.jl) will solve the linear system efficiently.\n",
    "Another alternative is to use __sparsefdiff__ \n",
    "<cite data-cite=\"sparsediff\"><a href=\"siamfa.html#sparsediff\">(RMGH20)</cite> from\n",
    "[SparseDiffTools.jl](#https://github.com/JuliaDiff/SparseDiffTools.jl). \n",
    "    \n",
    "No matter what you do, you'll need to think about fill-in, symbolic factorization in the general case, and storage. \n",
    "    \n",
    "Have fun!    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.5.2",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
