{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\newcommand{\\calf}{{\\cal F}}\n",
    "\\newcommand{\\dnu}{d \\nu}\n",
    "\\newcommand{\\mf}{{\\bf F}}\n",
    "\\newcommand{\\vu}{{\\bf u}}\n",
    "\\newcommand{\\vx}{{\\bf x}}\n",
    "\\newcommand{\\vy}{{\\bf y}}\n",
    "\\newcommand{\\vs}{{\\bf s}}\n",
    "\\newcommand{\\ve}{{\\bf e}}\n",
    "\\newcommand{\\vd}{{\\bf d}}\n",
    "\\newcommand{\\mg}{{\\bf G}}\n",
    "\\newcommand{\\ml}{{\\bf L}}\n",
    "\\newcommand{\\mg}{{\\bf G}}\n",
    "\\newcommand{\\mv}{{\\bf V}}\n",
    "\\newcommand{\\ma}{{\\bf A}}\n",
    "\\newcommand{\\mi}{{\\bf I}}\n",
    "\\newcommand{\\mm}{{\\bf M}}\n",
    "\\newcommand{\\ball}{{\\cal B}}\n",
    "\\newcommand{\\ptc}{{\\Psi TC}}\n",
    "\\newcommand{\\diag}{\\mbox{diag}}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Precompiling SIAMFANLEquations [084e46ad-d928-497d-ad5e-07fa361a48c4]\n",
      "└ @ Base loading.jl:1278\n",
      "┌ Warning: Package SIAMFANLEquations does not have BandedMatrices in its dependencies:\n",
      "│ - If you have SIAMFANLEquations checked out for development and have\n",
      "│   added BandedMatrices as a dependency but haven't updated your primary\n",
      "│   environment's manifest file, try `Pkg.resolve()`.\n",
      "│ - Otherwise you may need to report an issue with SIAMFANLEquations\n",
      "└ Loading BandedMatrices into SIAMFANLEquations from project dependency, future warnings for SIAMFANLEquations are suppressed.\n",
      "┌ Info: Precompiling NotebookSIAMFANL [ec7fb596-f0f3-5de5-8d97-3d08ac0c8f9e]\n",
      "└ @ Base loading.jl:1278\n"
     ]
    }
   ],
   "source": [
    "using SIAMFANLEquations\n",
    "using SIAMFANLEquations.TestProblems\n",
    "using LinearAlgebra\n",
    "using PyPlot\n",
    "using NotebookSIAMFANL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2.7 Solvers for Chapter 2\n",
    "\n",
    "Contents for Section 1.10\n",
    "\n",
    "[Overview](#Overview)\n",
    "\n",
    "[nsolsc.jl](#nsold.jl)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will follow the patter of Chapter 1 and present two sovlers, a Newton code and a $\\ptc$ code. Both codes are for systems of equations and use direct methods to compute the step. We returned the solution history for the simple two dimensional example in Section 2.6, but will not do that again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nsold.jl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__nsold.jl__ solves systems of nonlinear equations and computes the Newton step with direct linear solvers. Let's look at the docstrings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search: \u001b[0m\u001b[1mn\u001b[22m\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1ml\u001b[22m\u001b[0m\u001b[1md\u001b[22m \u001b[0m\u001b[1mn\u001b[22m\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1ml\u001b[22msc\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "nsold(F!, x0, FS, FPS, J!=diffjac!; rtol=1.e-6, atol=1.e-12,\n",
       "           maxit=20, solver=\"newton\", sham=1, armmax=10, resdec=.1,\n",
       "           dx = 1.e-7, armfix=false, \n",
       "           pdata = nothing, jfact = lu!,\n",
       "           printerr = true, keepsolhist = false, stagnationok=false)\n",
       "\\end{verbatim}\n",
       ")\n",
       "\n",
       "C. T. Kelley, 2020\n",
       "\n",
       "Julia versions of the nonlinear solvers from my SIAM books.  Herewith: nsold\n",
       "\n",
       "Inputs:\n",
       "\n",
       "\\begin{itemize}\n",
       "\\item F!: function evaluation, the ! indicates that F! overwrites FS, your   preallocated storage for the function.\n",
       "\n",
       "\\end{itemize}\n",
       "\\begin{itemize}\n",
       "\\item x: initial iterate\n",
       "\n",
       "\\end{itemize}\n",
       "\\begin{itemize}\n",
       "\\item FS: Preallcoated storage for function. It is an N x 1 column vector\n",
       "\n",
       "\\end{itemize}\n",
       "\\begin{itemize}\n",
       "\\item FPS: preallcoated storage for Jacobian. It is an N x N matrix\n",
       "\n",
       "\\end{itemize}\n",
       "\\begin{itemize}\n",
       "\\item J!: Jacobian evaluation, the ! indicates that J! overwrites FPS, your   preallocated storage for the Jacobian. If you leave this out the   default is a finite difference Jacobian.\n",
       "\n",
       "\\end{itemize}\n",
       "\\rule{\\textwidth}{1pt}\n",
       "Keyword Arguments (kwargs):\n",
       "\n",
       "\\begin{itemize}\n",
       "\\item rtol and atol: relative and absolute error tolerances\n",
       "\n",
       "\\end{itemize}\n",
       "\\begin{itemize}\n",
       "\\item maxit: limit on nonlinear iterations\n",
       "\n",
       "\\end{itemize}\n",
       "solver:\n",
       "\n",
       "Your choices are \"newton\"(default) or \"chord\". However, you have sham at your disposal only if you chose newton. \"chord\" will keep using the initial derivative until the iterate converges, uses the iteration budget, or the line search fails. It is not the same as sham=Inf, which is smarter.\n",
       "\n",
       "sham:\n",
       "\n",
       "This is the Shamanskii method. If sham=1, you have Newton. The iteration updates the derivative every sham iterations. The convergence rate has local q-order sham+1 if you only count iterations where you update the derivative. You need not provide your own derivative function to use this option. sham=Inf is chord only if chord is converging well.\n",
       "\n",
       "armmax: upper bound on stepsize reductions in linesearch\n",
       "\n",
       "resdec:\n",
       "\n",
       "target value for residual reduction. The default value is .1. In the old MATLAB codes it was .5. I only turn Shamanskii on if the residuals are decreasing rapidly, at least a factor of resdec, and the line search is quiescent. If you want to eliminate resdec from the method ( you don't ) then set resdec = 1.0 and you will never hear from it again.\n",
       "\n",
       "dx:\n",
       "\n",
       "difference increment in finite-difference derivatives       h=dx*norm(x)+1.e-6\n",
       "\n",
       "armfix:\n",
       "\n",
       "The default is a parabolic line search (ie false). Set to true and the stepsize will be fixed at .5. Don't do this unless you are doing experiments for research.\n",
       "\n",
       "pdata:\n",
       "\n",
       "precomputed data for the function/Jacobian.  Things will go better if you use this rather than hide the data  in global variables within the module for your function/Jacobian\n",
       "\n",
       "jfact:\n",
       "\n",
       "If you have a dense Jacobian I call PrepareJac! to evaluate the Jacobian (using your J!) and factor it. The default is to use lu! to compute an LU factorization and share storage with the Jacobian. You may change LU to something else by, for example, setting jfact = cholseky! if your Jacobian is spd. \n",
       "\n",
       "Please do not mess with the line that calls PrepareJac!.          FPF=PrepareJac!(FS, FPS, x, F!, J!, dx, pdata; fact = jfact) FPF is not the same as FPS (the storage you allocate for the Jacobian) for a reason. FPF and FPS do not have the same type, even though they share storage. So, FPS=PrepareJac!(FS, FPS, ...) will break things.\n",
       "\n",
       "printerr:\n",
       "\n",
       "I print a helpful message when the solver fails. To supress that message set printerr to false.\n",
       "\n",
       "keepsolhist:\n",
       "\n",
       "Set this to true to get the history of the iteration in the output tuple. This is on by default for scalar equations and off for systems. Only turn it on if you have use for the data, which can get REALLY LARGE.\n",
       "\n",
       "stagnationok:\n",
       "\n",
       "Set this to true if you want to disable the line search and either observe divergence or stagnation. This is only useful for research or writing a book.\n",
       "\n",
       "\\rule{\\textwidth}{1pt}\n",
       "\\section{Using nsold.jl}\n",
       "Here are the rules as of August 11, 2020.\n",
       "\n",
       "F! is the nonlinear residual.  J! is the Jacobian evaluation.\n",
       "\n",
       "I like to put all my function/Jacobian/initialization stuff in a Module and only export the things I actually use.\n",
       "\n",
       "A) You allocate storage for the function and Jacobian in advance     –> in the calling program <– NOT in FS and FPS\n",
       "\n",
       "FV=F!(FV,x) or FV=F!(FV,x,pdata) returns FV=F(x)\n",
       "\n",
       "FP=J!(FV,FP,x) or FP=J!(FV,FP,x,pdata) returns FP=F'(x);      (FV,FP, x) must be the argument list, even if FP does not need FV.     One reason for this is that the finite-difference Jacobian     does and that is the default in the solver.\n",
       "\n",
       "In the future J! will also be a matrix-vector product and FPS will be the PREALLOCATED (!!) storage for the GMRES(m) Krylov vectors.\n",
       "\n",
       "Lemme tell ya 'bout precision. I designed this code for full precision functions and linear algebra in any precision you want. You can declare FPS as Float64, Float32, or Float16 and nsold will do the right thing if  YOU do not destroy the declaration in your J! function. I'm amazed  that this works so easily. \n",
       "\n",
       "If the Jacobian is reasonably well conditioned, I can see no reason to do linear algebra in double precision\n",
       "\n",
       "Don't try to evaluate function and Jacobian all at once because  that will cost you a extra function evaluation every time the line search kicks in.\n",
       "\n",
       "B) Any precomputed data for functions, Jacobians, matrix-vector products    or preallocted storage may live in global variables within a module     containing F! and J!.  Don't do that if you can avoid it.     Use pdata instead.\n",
       "\n",
       "\\section{Examples}\n",
       "\\begin{verbatim}\n",
       " julia> function f!(fv,x)\n",
       "       fv[1]=x[1] + sin(x[2])\n",
       "       fv[2]=cos(x[1]+x[2])\n",
       "       end\n",
       "f (generic function with 1 method)\n",
       "\n",
       "julia> x=ones(2,); fv=zeros(2,); jv=zeros(2,2);\n",
       "julia> nout=nsold(f!,x,fv,jv);\n",
       "julia> nout.history\n",
       "5-element Array{Float64,1}:\n",
       " 1.88791e+00\n",
       " 2.43119e-01\n",
       " 1.19231e-02\n",
       " 1.03266e-05\n",
       " 1.46416e-11\n",
       "\n",
       "julia> nout.solution\n",
       "2-element Array{Float64,1}:\n",
       " -7.39085e-01\n",
       "  2.30988e+00\n",
       "\n",
       "\\end{verbatim}\n",
       "\\subsection{H-equation example}\n",
       "\\begin{verbatim}\n",
       "julia> n=16; x0=ones(n,); FV=ones(n,); JV=ones(n,n);\n",
       "help?> heqinit\n",
       "search: heqinit\n",
       "\n",
       "  heqinit(x0::Array{T,1}, c, TJ=Float64) where T<:Real\n",
       "\n",
       "  Initialize H-equation precomputed data.\n",
       "\n",
       "julia> hdata=heqinit(x0, .5);\n",
       "julia> hout=nsold(heqf!,x0,FV,JV;pdata=hdata);\n",
       "julia> hout.history\n",
       "3-element Array{Float64,1}:\n",
       " 6.17376e-01\n",
       " 3.17810e-03\n",
       " 6.22034e-08\n",
       "\\end{verbatim}\n"
      ],
      "text/markdown": [
       "```\n",
       "nsold(F!, x0, FS, FPS, J!=diffjac!; rtol=1.e-6, atol=1.e-12,\n",
       "           maxit=20, solver=\"newton\", sham=1, armmax=10, resdec=.1,\n",
       "           dx = 1.e-7, armfix=false, \n",
       "           pdata = nothing, jfact = lu!,\n",
       "           printerr = true, keepsolhist = false, stagnationok=false)\n",
       "```\n",
       "\n",
       ")\n",
       "\n",
       "C. T. Kelley, 2020\n",
       "\n",
       "Julia versions of the nonlinear solvers from my SIAM books.  Herewith: nsold\n",
       "\n",
       "Inputs:\n",
       "\n",
       "  * F!: function evaluation, the ! indicates that F! overwrites FS, your   preallocated storage for the function.\n",
       "\n",
       "  * x: initial iterate\n",
       "\n",
       "  * FS: Preallcoated storage for function. It is an N x 1 column vector\n",
       "\n",
       "  * FPS: preallcoated storage for Jacobian. It is an N x N matrix\n",
       "\n",
       "  * J!: Jacobian evaluation, the ! indicates that J! overwrites FPS, your   preallocated storage for the Jacobian. If you leave this out the   default is a finite difference Jacobian.\n",
       "\n",
       "---\n",
       "\n",
       "Keyword Arguments (kwargs):\n",
       "\n",
       "  * rtol and atol: relative and absolute error tolerances\n",
       "\n",
       "  * maxit: limit on nonlinear iterations\n",
       "\n",
       "solver:\n",
       "\n",
       "Your choices are \"newton\"(default) or \"chord\". However, you have sham at your disposal only if you chose newton. \"chord\" will keep using the initial derivative until the iterate converges, uses the iteration budget, or the line search fails. It is not the same as sham=Inf, which is smarter.\n",
       "\n",
       "sham:\n",
       "\n",
       "This is the Shamanskii method. If sham=1, you have Newton. The iteration updates the derivative every sham iterations. The convergence rate has local q-order sham+1 if you only count iterations where you update the derivative. You need not provide your own derivative function to use this option. sham=Inf is chord only if chord is converging well.\n",
       "\n",
       "armmax: upper bound on stepsize reductions in linesearch\n",
       "\n",
       "resdec:\n",
       "\n",
       "target value for residual reduction. The default value is .1. In the old MATLAB codes it was .5. I only turn Shamanskii on if the residuals are decreasing rapidly, at least a factor of resdec, and the line search is quiescent. If you want to eliminate resdec from the method ( you don't ) then set resdec = 1.0 and you will never hear from it again.\n",
       "\n",
       "dx:\n",
       "\n",
       "difference increment in finite-difference derivatives       h=dx*norm(x)+1.e-6\n",
       "\n",
       "armfix:\n",
       "\n",
       "The default is a parabolic line search (ie false). Set to true and the stepsize will be fixed at .5. Don't do this unless you are doing experiments for research.\n",
       "\n",
       "pdata:\n",
       "\n",
       "precomputed data for the function/Jacobian.  Things will go better if you use this rather than hide the data  in global variables within the module for your function/Jacobian\n",
       "\n",
       "jfact:\n",
       "\n",
       "If you have a dense Jacobian I call PrepareJac! to evaluate the Jacobian (using your J!) and factor it. The default is to use lu! to compute an LU factorization and share storage with the Jacobian. You may change LU to something else by, for example, setting jfact = cholseky! if your Jacobian is spd. \n",
       "\n",
       "Please do not mess with the line that calls PrepareJac!.          FPF=PrepareJac!(FS, FPS, x, F!, J!, dx, pdata; fact = jfact) FPF is not the same as FPS (the storage you allocate for the Jacobian) for a reason. FPF and FPS do not have the same type, even though they share storage. So, FPS=PrepareJac!(FS, FPS, ...) will break things.\n",
       "\n",
       "printerr:\n",
       "\n",
       "I print a helpful message when the solver fails. To supress that message set printerr to false.\n",
       "\n",
       "keepsolhist:\n",
       "\n",
       "Set this to true to get the history of the iteration in the output tuple. This is on by default for scalar equations and off for systems. Only turn it on if you have use for the data, which can get REALLY LARGE.\n",
       "\n",
       "stagnationok:\n",
       "\n",
       "Set this to true if you want to disable the line search and either observe divergence or stagnation. This is only useful for research or writing a book.\n",
       "\n",
       "---\n",
       "\n",
       "# Using nsold.jl\n",
       "\n",
       "Here are the rules as of August 11, 2020.\n",
       "\n",
       "F! is the nonlinear residual.  J! is the Jacobian evaluation.\n",
       "\n",
       "I like to put all my function/Jacobian/initialization stuff in a Module and only export the things I actually use.\n",
       "\n",
       "A) You allocate storage for the function and Jacobian in advance     –> in the calling program <– NOT in FS and FPS\n",
       "\n",
       "FV=F!(FV,x) or FV=F!(FV,x,pdata) returns FV=F(x)\n",
       "\n",
       "FP=J!(FV,FP,x) or FP=J!(FV,FP,x,pdata) returns FP=F'(x);      (FV,FP, x) must be the argument list, even if FP does not need FV.     One reason for this is that the finite-difference Jacobian     does and that is the default in the solver.\n",
       "\n",
       "In the future J! will also be a matrix-vector product and FPS will be the PREALLOCATED (!!) storage for the GMRES(m) Krylov vectors.\n",
       "\n",
       "Lemme tell ya 'bout precision. I designed this code for full precision functions and linear algebra in any precision you want. You can declare FPS as Float64, Float32, or Float16 and nsold will do the right thing if  YOU do not destroy the declaration in your J! function. I'm amazed  that this works so easily. \n",
       "\n",
       "If the Jacobian is reasonably well conditioned, I can see no reason to do linear algebra in double precision\n",
       "\n",
       "Don't try to evaluate function and Jacobian all at once because  that will cost you a extra function evaluation every time the line search kicks in.\n",
       "\n",
       "B) Any precomputed data for functions, Jacobians, matrix-vector products    or preallocted storage may live in global variables within a module     containing F! and J!.  Don't do that if you can avoid it.     Use pdata instead.\n",
       "\n",
       "# Examples\n",
       "\n",
       "```jldoctest\n",
       " julia> function f!(fv,x)\n",
       "       fv[1]=x[1] + sin(x[2])\n",
       "       fv[2]=cos(x[1]+x[2])\n",
       "       end\n",
       "f (generic function with 1 method)\n",
       "\n",
       "julia> x=ones(2,); fv=zeros(2,); jv=zeros(2,2);\n",
       "julia> nout=nsold(f!,x,fv,jv);\n",
       "julia> nout.history\n",
       "5-element Array{Float64,1}:\n",
       " 1.88791e+00\n",
       " 2.43119e-01\n",
       " 1.19231e-02\n",
       " 1.03266e-05\n",
       " 1.46416e-11\n",
       "\n",
       "julia> nout.solution\n",
       "2-element Array{Float64,1}:\n",
       " -7.39085e-01\n",
       "  2.30988e+00\n",
       "\n",
       "```\n",
       "\n",
       "## H-equation example\n",
       "\n",
       "```jldoctest\n",
       "julia> n=16; x0=ones(n,); FV=ones(n,); JV=ones(n,n);\n",
       "help?> heqinit\n",
       "search: heqinit\n",
       "\n",
       "  heqinit(x0::Array{T,1}, c, TJ=Float64) where T<:Real\n",
       "\n",
       "  Initialize H-equation precomputed data.\n",
       "\n",
       "julia> hdata=heqinit(x0, .5);\n",
       "julia> hout=nsold(heqf!,x0,FV,JV;pdata=hdata);\n",
       "julia> hout.history\n",
       "3-element Array{Float64,1}:\n",
       " 6.17376e-01\n",
       " 3.17810e-03\n",
       " 6.22034e-08\n",
       "```\n"
      ],
      "text/plain": [
       "\u001b[36m  nsold(F!, x0, FS, FPS, J!=diffjac!; rtol=1.e-6, atol=1.e-12,\u001b[39m\n",
       "\u001b[36m             maxit=20, solver=\"newton\", sham=1, armmax=10, resdec=.1,\u001b[39m\n",
       "\u001b[36m             dx = 1.e-7, armfix=false, \u001b[39m\n",
       "\u001b[36m             pdata = nothing, jfact = lu!,\u001b[39m\n",
       "\u001b[36m             printerr = true, keepsolhist = false, stagnationok=false)\u001b[39m\n",
       "\n",
       "  )\n",
       "\n",
       "  C. T. Kelley, 2020\n",
       "\n",
       "  Julia versions of the nonlinear solvers from my SIAM books. Herewith: nsold\n",
       "\n",
       "  Inputs:\n",
       "\n",
       "    •    F!: function evaluation, the ! indicates that F! overwrites FS,\n",
       "        your preallocated storage for the function.\n",
       "\n",
       "    •    x: initial iterate\n",
       "\n",
       "    •    FS: Preallcoated storage for function. It is an N x 1 column\n",
       "        vector\n",
       "\n",
       "    •    FPS: preallcoated storage for Jacobian. It is an N x N matrix\n",
       "\n",
       "    •    J!: Jacobian evaluation, the ! indicates that J! overwrites FPS,\n",
       "        your preallocated storage for the Jacobian. If you leave this out\n",
       "        the default is a finite difference Jacobian.\n",
       "\n",
       "  ────────────────────────────────────────────────────────────────────────────\n",
       "\n",
       "  Keyword Arguments (kwargs):\n",
       "\n",
       "    •    rtol and atol: relative and absolute error tolerances\n",
       "\n",
       "    •    maxit: limit on nonlinear iterations\n",
       "\n",
       "  solver:\n",
       "\n",
       "  Your choices are \"newton\"(default) or \"chord\". However, you have sham at\n",
       "  your disposal only if you chose newton. \"chord\" will keep using the initial\n",
       "  derivative until the iterate converges, uses the iteration budget, or the\n",
       "  line search fails. It is not the same as sham=Inf, which is smarter.\n",
       "\n",
       "  sham:\n",
       "\n",
       "  This is the Shamanskii method. If sham=1, you have Newton. The iteration\n",
       "  updates the derivative every sham iterations. The convergence rate has local\n",
       "  q-order sham+1 if you only count iterations where you update the derivative.\n",
       "  You need not provide your own derivative function to use this option.\n",
       "  sham=Inf is chord only if chord is converging well.\n",
       "\n",
       "  armmax: upper bound on stepsize reductions in linesearch\n",
       "\n",
       "  resdec:\n",
       "\n",
       "  target value for residual reduction. The default value is .1. In the old\n",
       "  MATLAB codes it was .5. I only turn Shamanskii on if the residuals are\n",
       "  decreasing rapidly, at least a factor of resdec, and the line search is\n",
       "  quiescent. If you want to eliminate resdec from the method ( you don't )\n",
       "  then set resdec = 1.0 and you will never hear from it again.\n",
       "\n",
       "  dx:\n",
       "\n",
       "  difference increment in finite-difference derivatives h=dx*norm(x)+1.e-6\n",
       "\n",
       "  armfix:\n",
       "\n",
       "  The default is a parabolic line search (ie false). Set to true and the\n",
       "  stepsize will be fixed at .5. Don't do this unless you are doing experiments\n",
       "  for research.\n",
       "\n",
       "  pdata:\n",
       "\n",
       "  precomputed data for the function/Jacobian. Things will go better if you use\n",
       "  this rather than hide the data in global variables within the module for\n",
       "  your function/Jacobian\n",
       "\n",
       "  jfact:\n",
       "\n",
       "  If you have a dense Jacobian I call PrepareJac! to evaluate the Jacobian\n",
       "  (using your J!) and factor it. The default is to use lu! to compute an LU\n",
       "  factorization and share storage with the Jacobian. You may change LU to\n",
       "  something else by, for example, setting jfact = cholseky! if your Jacobian\n",
       "  is spd. \n",
       "\n",
       "  Please do not mess with the line that calls PrepareJac!. FPF=PrepareJac!(FS,\n",
       "  FPS, x, F!, J!, dx, pdata; fact = jfact) FPF is not the same as FPS (the\n",
       "  storage you allocate for the Jacobian) for a reason. FPF and FPS do not have\n",
       "  the same type, even though they share storage. So, FPS=PrepareJac!(FS, FPS,\n",
       "  ...) will break things.\n",
       "\n",
       "  printerr:\n",
       "\n",
       "  I print a helpful message when the solver fails. To supress that message set\n",
       "  printerr to false.\n",
       "\n",
       "  keepsolhist:\n",
       "\n",
       "  Set this to true to get the history of the iteration in the output tuple.\n",
       "  This is on by default for scalar equations and off for systems. Only turn it\n",
       "  on if you have use for the data, which can get REALLY LARGE.\n",
       "\n",
       "  stagnationok:\n",
       "\n",
       "  Set this to true if you want to disable the line search and either observe\n",
       "  divergence or stagnation. This is only useful for research or writing a\n",
       "  book.\n",
       "\n",
       "  ────────────────────────────────────────────────────────────────────────────\n",
       "\n",
       "\u001b[1m  Using nsold.jl\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡\u001b[22m\n",
       "\n",
       "  Here are the rules as of August 11, 2020.\n",
       "\n",
       "  F! is the nonlinear residual. J! is the Jacobian evaluation.\n",
       "\n",
       "  I like to put all my function/Jacobian/initialization stuff in a Module and\n",
       "  only export the things I actually use.\n",
       "\n",
       "  A) You allocate storage for the function and Jacobian in advance –> in the\n",
       "  calling program <– NOT in FS and FPS\n",
       "\n",
       "  FV=F!(FV,x) or FV=F!(FV,x,pdata) returns FV=F(x)\n",
       "\n",
       "  FP=J!(FV,FP,x) or FP=J!(FV,FP,x,pdata) returns FP=F'(x); (FV,FP, x) must be\n",
       "  the argument list, even if FP does not need FV. One reason for this is that\n",
       "  the finite-difference Jacobian does and that is the default in the solver.\n",
       "\n",
       "  In the future J! will also be a matrix-vector product and FPS will be the\n",
       "  PREALLOCATED (!!) storage for the GMRES(m) Krylov vectors.\n",
       "\n",
       "  Lemme tell ya 'bout precision. I designed this code for full precision\n",
       "  functions and linear algebra in any precision you want. You can declare FPS\n",
       "  as Float64, Float32, or Float16 and nsold will do the right thing if YOU do\n",
       "  not destroy the declaration in your J! function. I'm amazed that this works\n",
       "  so easily. \n",
       "\n",
       "  If the Jacobian is reasonably well conditioned, I can see no reason to do\n",
       "  linear algebra in double precision\n",
       "\n",
       "  Don't try to evaluate function and Jacobian all at once because that will\n",
       "  cost you a extra function evaluation every time the line search kicks in.\n",
       "\n",
       "  B) Any precomputed data for functions, Jacobians, matrix-vector products or\n",
       "  preallocted storage may live in global variables within a module containing\n",
       "  F! and J!. Don't do that if you can avoid it. Use pdata instead.\n",
       "\n",
       "\u001b[1m  Examples\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡≡≡≡\u001b[22m\n",
       "\n",
       "\u001b[36m   julia> function f!(fv,x)\u001b[39m\n",
       "\u001b[36m         fv[1]=x[1] + sin(x[2])\u001b[39m\n",
       "\u001b[36m         fv[2]=cos(x[1]+x[2])\u001b[39m\n",
       "\u001b[36m         end\u001b[39m\n",
       "\u001b[36m  f (generic function with 1 method)\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> x=ones(2,); fv=zeros(2,); jv=zeros(2,2);\u001b[39m\n",
       "\u001b[36m  julia> nout=nsold(f!,x,fv,jv);\u001b[39m\n",
       "\u001b[36m  julia> nout.history\u001b[39m\n",
       "\u001b[36m  5-element Array{Float64,1}:\u001b[39m\n",
       "\u001b[36m   1.88791e+00\u001b[39m\n",
       "\u001b[36m   2.43119e-01\u001b[39m\n",
       "\u001b[36m   1.19231e-02\u001b[39m\n",
       "\u001b[36m   1.03266e-05\u001b[39m\n",
       "\u001b[36m   1.46416e-11\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> nout.solution\u001b[39m\n",
       "\u001b[36m  2-element Array{Float64,1}:\u001b[39m\n",
       "\u001b[36m   -7.39085e-01\u001b[39m\n",
       "\u001b[36m    2.30988e+00\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\n",
       "\u001b[1m  H-equation example\u001b[22m\n",
       "\u001b[1m  ====================\u001b[22m\n",
       "\n",
       "\u001b[36m  julia> n=16; x0=ones(n,); FV=ones(n,); JV=ones(n,n);\u001b[39m\n",
       "\u001b[36m  help?> heqinit\u001b[39m\n",
       "\u001b[36m  search: heqinit\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m    heqinit(x0::Array{T,1}, c, TJ=Float64) where T<:Real\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m    Initialize H-equation precomputed data.\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> hdata=heqinit(x0, .5);\u001b[39m\n",
       "\u001b[36m  julia> hout=nsold(heqf!,x0,FV,JV;pdata=hdata);\u001b[39m\n",
       "\u001b[36m  julia> hout.history\u001b[39m\n",
       "\u001b[36m  3-element Array{Float64,1}:\u001b[39m\n",
       "\u001b[36m   6.17376e-01\u001b[39m\n",
       "\u001b[36m   3.17810e-03\u001b[39m\n",
       "\u001b[36m   6.22034e-08\u001b[39m"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?nsold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input\n",
    "\n",
    "The calling sequence for the Newton solvers in this book are similar, differing mostly in the management of the linear solver and memory allocation. The calling sequence for __nsold.jl__ is\n",
    "\n",
    "```julia\n",
    "function nsold(\n",
    "    F!,\n",
    "    x0,\n",
    "    FS,\n",
    "    FPS,\n",
    "    J! = diffjac!;\n",
    "    rtol = 1.e-6,\n",
    "    atol = 1.e-12,\n",
    "    maxit = 20,\n",
    "    solver = \"newton\",\n",
    "    sham = 1,\n",
    "    armmax = 10,\n",
    "    resdec = 0.1,\n",
    "    dx = 1.e-7,\n",
    "    armfix = false,\n",
    "    pdata = nothing,\n",
    "    jfact = lu!,\n",
    "    printerr = true,\n",
    "    keepsolhist = false,\n",
    "    stagnationok = false,\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we said earlier in the chapter, the calling sequence has some new things which are not in __nsolsc.jl__. The most significant are the arrays __FS__ and __FPS__, which preallocate\n",
    "storage for the function and Jacobian. As we have pointed out earlier, the farther upstream one allocates memory the better, so __nsold.jl__ insists that you allocate an vector __FS__ of the same size as the initial iterate and a matrix \n",
    "__FPS__ for the Jacobian. \n",
    "\n",
    "The other major new feature is the keyword argument __pdata__. This is the data structure for you to store any precomuted or preallocated data your function evaluation needs. You will almost surely need __pdata__ for any but the most trivial problems. The H-equation example from section 2.6 uses __pdata__ in a serious manner.\n",
    "\n",
    "You may dimension __x0__ either as $(N,1)$ or $(N,)$, but you must be consistent and dimension __FS__ the same way. __nsold.jl__ expects vectors to be in double precision (Float64). \n",
    "\n",
    "The __!__ in the function evaluation __F!__ is to indicate that __nsold__ expects __F__  to overwrite its imput. So,\n",
    "the way to call __F!__ is to preallocate the storage for the function value in an array __FS__ and then call the function as\n",
    "```Julia\n",
    "F!(FS,x)\n",
    "```\n",
    "or\n",
    "```Julia\n",
    "F!(FS,x,pdata)\n",
    "```\n",
    "__nsold.jl__ will figure out if you have populated __pdata__ or left it alone as the default value of __nothing__.\n",
    "\n",
    "\n",
    "And now for the Jacobian. __nsold.jl__ uses direct methods for linear algebra. If your matrix is dense, the default is to use Julia's __lu!__ function to do an LU factorization. If your matrix is symmetric or symmetric positive definite you can use the __factorization__ keyword to change __lu!__ to __ldlt!__ or __cholesky!__ for example. __nsold.jl__ assumes that the factorization you ask for will overwrite the matrix. Hence, the __factorize__ function in Julia is not what you want for this application.\n",
    "\n",
    "You will also need to preallocate storage for the Jacobian in the array __FPS__. You may use any legal real precision for __FPS__. Float64 is the default. If you use Float32 you cut the storage for the matrix and the time for the factorization in half. We recommend that you do this if your Jacobian is dense. If you are using the __Sparsesuite__ sparse solvers, then you must store the Jacobian in double precision. __Sparsesuite__ does not support lower precision.\n",
    "\n",
    "Your Jacobian computation __J!__ must also overwrite it's input. The call looks like\n",
    "```julia\n",
    "J!(FV,FP,x)\n",
    "```\n",
    "or \n",
    "```julia\n",
    "J!(FV,FP,x,pdata)\n",
    "\n",
    "```\n",
    "returns FP=F'(x). The input FP=F(x), which __nsold.jl__ has already compouted, has to be there.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.5.0",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
